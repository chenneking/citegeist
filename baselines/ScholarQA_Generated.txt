Large Language Models (LLMs) for code understanding have progressed significantly in their approaches to representing source code. Initially, code was represented as either a set of explicit metrics or as plain text, allowing researchers to adapt techniques from Natural Language Processing (Utkin et al., 2022). While code can be viewed as text, it possesses a richer underlying structure compared to natural language (Utkin et al., 2022)(Ernst, 2017). This recognition has led to increased research that takes code structure into account.

Abstract Syntax Trees (ASTs) have become a fundamental structured representation in code understanding models. By parsing raw source code according to language-specific syntax rules, ASTs capture the hierarchical structure of code (Utkin et al., 2022). This structural approach has proven beneficial across various software engineering tasks, including code summarization, type inference, bug detection, and malware detection (Utkin et al., 2022)(Allamanis et al., 2020)(Rusak et al., 2018). Path-based representations that traverse ASTs have also emerged as effective methods for learning from programs (Alon et al., 2018).

Recent research has introduced multiple approaches to incorporating structural information. Some state-of-the-art models parse code into tree or graph forms, as seen in models like BASTS and Structure-Induced Transformer (SIT) (Gu et al., 2022)(Wu et al., 2020). These models explicitly encode structural clues rather than treating code merely as a token sequence. The Structure-Induced Transformer, for example, incorporates multi-view structural clues through a specially designed self-attention mechanism (Wu et al., 2020).

Alternative approaches have demonstrated that jointly learning from both source code (context) and its parsed AST (structure) can be effective. Zügner et al. showed that using language-agnostic features computed directly from the AST, alongside source code, achieved state-of-the-art results in code summarization across multiple programming languages (Zugner et al., 2021). This combined approach particularly benefits low-resource languages in multilingual settings.

Despite these advances in structural representations, some researchers have pursued token-based neural models that process code as a sequence. This approach offers advantages in handling corrupted or partial code where complete parsing may not be possible (Gu et al., 2022). However, a significant limitation of models that focus solely on static code text and structures is their inability to capture dynamic program semantics that only become apparent during execution. Without understanding program execution, these statically pretrained models cannot fully comprehend dynamic code properties such as branch coverage and runtime variable values (Ding et al., 2023).

Recent industrial advances in code understanding have been driven by large-scale models and specialized fine-tuning. Large language models like GPT-4, Codex, and LLaMA have demonstrated capabilities in explaining and summarizing source code (Su et al., 2023). These LLMs, trained on vast datasets comprising both natural language text and source code, have transformed programming assistance and shown potential to improve programmer productivity, code correctness, and automated code generation (Li et al., 2024).

The evaluation of Large Language Models (LLMs) for code generation has evolved significantly, with numerous benchmarks and metrics developed to assess their capabilities. Benchmarks like HumanEval have become standard for evaluating code generation from natural language descriptions, enabling researchers to identify "significant and reproducible advancements in LLM capabilities" (Fakhoury et al., 2023). However, these benchmarks typically focus narrowly on code synthesis from natural language instructions, neglecting other aspects of code understanding (Prenner et al., 2025). Additionally, concerns about data contamination have emerged, as benchmark problems may appear in training data, potentially biasing evaluation results (Prenner et al., 2025).

Evaluation metrics for code generation span from traditional natural language processing metrics to task-specific measures. Many studies rely on generic metrics like accuracy, precision, BLEU, and Pass@k (She et al., 2023)(Wang et al., 2023). However, these metrics often provide an incomplete assessment of model capabilities. For instance, empirical studies have shown that popular metrics like BLEU and ROUGE are "poor reliable indicators of human judgment" regarding code quality (She et al., 2023)(Roy et al., 2021). BLEU, which calculates n-gram precision between generated and reference code, is particularly problematic for programming languages due to "trivially shared n-grams" that make it difficult to distinguish genuinely similar code from coincidental similarities (She et al., 2023)(Eghbali et al., 2022). This limitation is evident in cases where models achieve high BLEU scores but demonstrate poor execution correctness, as seen with CodeT5, which generated code with "only 6.4% compilation rate" despite impressive BLEU scores (She et al., 2023).

Recent efforts have expanded evaluation frameworks to better capture the complexity of code generation tasks. Manh et al. developed a benchmark with multiple-choice programming tasks that assess fundamental cognitive processes in programmer comprehension, including "code completion, code repair, defect detection, and fill in the blank" (Manh et al., 2024). These tasks aim to evaluate core capabilities that cognitive models of programmer behavior must address: composition, comprehension, debugging, and modification (Manh et al., 2024).

The emergence of more comprehensive benchmarks reflects the recognition that code generation is inherently an iterative process. Current evaluations that request single responses to prompts represent a "simplistic way to measure code writing ability," as human programmers typically alternate between writing, testing, and debugging code (Song et al., 2024). To address this limitation, researchers have implemented search procedures with test execution information and model self-reflections to improve generation quality. Approaches like Reflexion, self-refine, and Monte-Carlo tree search leverage model self-reflection to correct buggy implementations (Song et al., 2024).

Beyond basic programming tasks, researchers have developed larger-scale benchmarks that capture real-world use cases. SWE-Bench challenges LLMs to solve GitHub issues, while DevBench evaluates LLMs on various software development tasks, including project design and unit testing (Song et al., 2024). These comprehensive benchmarks often incorporate retrieval-augmented generation (RAG) as an essential component for accomplishing complex tasks (Song et al., 2024).

The evaluation of LLMs for code generation continues to evolve, with researchers examining aspects such as code correctness, efficiency, readability, reliability, security, and interpretability (Chen et al., 2024). Liu et al. conducted a systematic empirical assessment of ChatGPT's code generation capabilities, analyzing correctness, complexity, and security across multiple programming languages and problem types (Chen et al., 2024)(Liu et al., 2023). Their findings revealed variations in performance based on problem age and language, with ChatGPT showing "better at generating functionally correct code for problems before 2021" but demonstrating relatively weak ability to fix erroneous code through multi-round interactions (Chen et al., 2024)(Liu et al., 2023).

While Large Language Models (LLMs) have demonstrated impressive capabilities in understanding source code and descriptive texts, they exhibit significant weaknesses when it comes to predicting dynamic program behavior (Le et al., 2024). This limitation stems primarily from their reliance on static code representations, which fail to capture the dynamic program behavior and state changes that occur during runtime. The ability to mentally simulate code execution is a fundamental skill among human developers, exemplified by practices like rubber duck debugging, where programmers verbalize code execution in natural language to identify and fix issues (Ni et al., 2024). However, LLMs are typically trained on the surface textual form of programs, potentially lacking a semantic understanding of how programs execute at runtime.

The disconnect between code generation ability and execution understanding has been empirically demonstrated through comprehensive evaluation frameworks. Chen et al. proposed REval, which specifically assesses code reasoning capability through Runtime Behavior Reasoning and Incremental Consistency Evaluation. Their large-scale empirical study revealed that the majority of evaluated LLMs showed unsatisfactory performance in both evaluation components, highlighting a critical gap in their code reasoning capabilities (Chen et al._1, 2024). Similarly, Liu et al. identified that state-of-the-art LLMs struggle with tasks requiring dynamic program understanding, such as code coverage prediction and runtime error detection (Le et al., 2024)(Liu et al._1, 2023).

This superficial understanding of code execution manifests in multiple ways. LLMs struggle to accurately simulate loops, conditional branches, and the cumulative effects between statements. They lack the context necessary to track variable states and control flow across multiple iterations, and fail to comprehend dynamic dependencies and interactions between various statements (Le et al., 2024). These limitations become particularly evident in what Grishina et al. term the "near-miss syndrome," where LLM-generated code closely resembles a correct solution but fails unit tests due to minor errors in execution logic (Grishina et al., 2025). To address these limitations, researchers have proposed multi-agent frameworks like Synthesize, Execute, Instruct, Debug, and Repair (SEIDR), which incorporate execution feedback to improve the debugging and repair of unsuccessful programs (Grishina et al., 2025).

Recent research has focused on integrating execution feedback and runtime behavior to enhance LLMs' code understanding abilities. A prominent approach is self-debugging, which enables LLMs to identify and correct errors in their generated code without human intervention. Chen et al. introduced a framework that teaches LLMs to perform "rubber duck debugging" — investigating execution results and explaining code in natural language to identify mistakes (Chen et al., 2023). This self-debugging approach significantly improved performance across various code generation benchmarks, with up to 12% accuracy gains and 9% improvement on the most challenging problems.

Similar execution-guided frameworks have been proposed, typically following a multi-stage workflow. Olausson et al. describe a process where a program is first generated, then executed against test cases, with error messages and the faulty program fed to a feedback generation model, which explains the failure (Olausson et al., 2023). This feedback is then used by a repair model to fix the code. This approach resembles the trial-and-error process human programmers employ and has been shown to overcome limitations in one-shot code generation (Olausson et al., 2023).

Several research efforts have refined this execution-feedback loop. For example, Le et al. proposed "CodeRL," which incorporates deep reinforcement learning with pretrained language models for program synthesis (Le et al., 2022). Their approach uses an actor-critic architecture where a critic network predicts the functional correctness of generated programs and provides feedback to the actor, resulting in state-of-the-art performance on challenging benchmarks (Olausson et al., 2023). Similarly, Gupta et al. developed SED, a neural program generation framework integrating synthesis, execution, and debugging stages to iteratively repair generated programs, mimicking human programmers' workflow (Gupta et al., 2020).

More sophisticated strategies for integrating execution information have emerged recently. Li et al. explored self-reflection techniques to summarize experiences from previous code attempts and incorporate these insights into subsequent explorations (Li et al._1, 2024). Another approach involves tree-based search strategies with pruning to improve efficiency, as demonstrated by Yao et al. in their Tree of Thoughts framework, which enables deliberate decision-making by considering multiple reasoning paths and evaluating choices (Yao et al., 2023).

The connection between code execution and optimization has also been investigated. Menna et al. note that while execution-aware strategies have enhanced language models' effectiveness in various software engineering tasks, their specific impact on code optimization remains largely unexplored (Menna et al., 2025). This gap is particularly significant given the pressing need for efficient code generation, as emphasized by Waghjale et al., who highlight that while LLMs can generate functionally correct code, creating solutions that are both correct and efficient remains challenging (Waghjale et al., 2024)(Li et al., 2022)(Madaan et al., 2023).

In practical applications, LLMs have been evaluated on real-world software maintenance tasks, including code question answering, fault localization, and code editing. Hu et al. found that different models excel in specific areas, with proprietary models not consistently outperforming open-source alternatives (Hu et al., 2024). Their research underscores the importance of task-specific model selection and the need for continued improvement in how LLMs integrate execution understanding into software engineering tasks.

Beyond general code understanding and execution, researchers have investigated specialized code structures that present unique challenges for LLMs. Parallel programming in particular has received attention due to its complexity and importance for performance-critical applications. Several approaches for automatic parallelization of sequential code have been developed, employing various techniques to identify and exploit opportunities for concurrent execution. Stuglik et al. review methods that use dependency extraction for Java source code analysis, where translators generate highly parallel versions of sequential programs by interpreting Abstract Syntax Tree nodes to identify dependencies between executable tasks. These approaches have demonstrated significant performance improvements, achieving up to 10.97x speedup for recursive algorithms like Fibonacci on multi-core machines (Stuglik et al., 2023).

Trace-based parallelization represents another approach, where execution information collected during runtime guides the parallelization process. Some systems collect online trace information while a program executes and dynamically recompile methods that can be executed in parallel, while others leverage traces specifically for data-parallel applications. More specialized approaches focus on parallelizing programs that use pointer-based dynamic data structures, creating asynchronous threads for method invocations (Stuglik et al., 2023).

The evaluation of LLMs' capabilities for generating parallel code has recently become an active research area. Nichols et al. created ParEval, a benchmark consisting of 420 different coding tasks related to scientific and parallel computing. This benchmark evaluates how effectively state-of-the-art language models can generate parallel code across six different parallel programming models and twelve computational problem types, introducing novel metrics for assessing the performance of generated code (Nichols et al., 2024). Other frameworks like Laminar have been developed to showcase and evaluate parallel programming capabilities, providing structured approaches to assess different aspects of code parallelization (Zahra et al., 2023).

These specialized code structures present significant challenges for LLMs that have primarily been trained on and evaluated with sequential code. The ability to understand and generate code that leverages parallel execution, recursive algorithms, and object-oriented programming principles represents an important frontier in advancing LLMs' code understanding capabilities beyond the syntax-focused evaluation common in existing benchmarks.

----------------------------------------------------------------------------------

Machine learning privacy concerns span multiple components and stages of the ML pipeline. Privacy-preserving machine learning (PPML) operates across three main phases: model generation, model training, and model serving, with privacy protection mechanisms implemented at each stage (Islam et al., 2025). These protections can be categorized as either object-oriented guarantees (protecting specific components like training data or model parameters) or pipeline-oriented guarantees (providing end-to-end privacy across the entire ML workflow) (Islam et al., 2025).

The research community has extensively investigated various privacy threats in machine learning. Inference attacks represent a significant concern, allowing adversaries to extract sensitive information from ML models (Liu et al., 2021). These attacks can be categorized into four representative types: membership inference (determining if a specific data sample was in the training dataset) (Shokri et al., 2016), model inversion (recovering training data) (Fredrikson et al., 2015), attribute inference (predicting properties unrelated to the model's original task) (Song et al., 2019), and model stealing (reconstructing non-public model parameters) (Tramer et al., 2016). These attacks can lead to severe consequences, including violating individuals' privacy and compromising intellectual property (Liu et al., 2021).

Existing privacy-enhancing techniques for ML have primarily focused on allowing multiple parties to collaboratively train models without revealing their private data (Al-Rubaie, 2018). These techniques generally employ cryptographic approaches or differential privacy (Al-Rubaie, 2018). Differential privacy has proven particularly effective against membership inference attacks and can be applied at different stages of the ML lifecycle - during data collection and processing, model training, or at inference time (Roy et al., 2023)(Zhu et al., 2020).

The privacy landscape in ML continues to evolve rapidly. Recent surveys have contributed significantly to our understanding of vulnerabilities, potential attacks, and privacy concerns in ML deployment (Luqman et al., 2024). However, as ML advances, the threat landscape evolves, demanding continuous exploration of novel challenges and innovative solutions (Luqman et al., 2024). This dynamic environment creates opportunities for fresh perspectives on emerging threats and privacy-preserving techniques to address them.

Privacy-preserving techniques in machine learning have historically focused more on the training phase than on inference. Compared to the extensive body of research on privacy-preserving training, significantly less work has been dedicated to addressing privacy concerns during inference (Zheng et al., 2019). This imbalance creates opportunities for novel research in protecting privacy during model deployment and usage.

Privacy-preserving inference approaches primarily assume that ML models have been previously trained using public plaintext data, and they aim to protect the privacy of test data vectors while maintaining inference accuracy (Zheng et al., 2019). Simple techniques like additive perturbation are generally not advisable for deep models, as even small input perturbations can significantly degrade inference accuracy (Zheng et al., 2019)(Zheng et al., 2016). To achieve stronger privacy guarantees during inference against honest-but-curious coordinators, more sophisticated approaches like CryptoNets and Multi-party Computation (MPC) have been proposed (Zheng et al., 2019)(Barni et al., 2006).

The privacy objectives in ML inference can be multi-faceted. Solutions often aim to either train models without allowing servers to see training data in the clear or to obliviously classify input data without leaking model details (Bayerl et al., 2020). These approaches typically rely on cryptographic techniques or Trusted Execution Environments (TEEs). For protecting only the intellectual property of ML models without considering client input privacy, orthogonal approaches like model watermarking (Rouhani et al., 2019) and fingerprinting (Chen et al., 2019) have been developed.

In the broader privacy-preserving machine learning (PPML) framework, both training and inference are critical phases that require distinct privacy protection mechanisms. While model training incorporates privacy-preserving techniques throughout the learning process, model serving (inference) focuses on ensuring privacy during deployment, protecting both user queries and model outputs (Islam et al., 2025). This distinction reflects the different privacy concerns and stakeholders involved in each phase of the machine learning lifecycle.

Privacy-preserving machine learning inference has received less research attention compared to privacy-preserving training, yet several important approaches have emerged to address this gap (Zheng et al., 2019). These approaches typically assume that ML models have been previously trained on public plaintext data and focus on protecting the privacy of test data while maintaining inference accuracy (Zheng et al., 2019). Simple techniques like additive perturbation have proven ineffective for deep models, as even minor input perturbations can significantly degrade inference accuracy (Zheng et al., 2019)(Zheng et al., 2016).

Cryptographic solutions represent a major category of privacy-preserving ML inference approaches. Homomorphic encryption enables computation on encrypted data, allowing models to process user inputs without decrypting them (Chong et al., 2024)(Papernot et al., 2018). CryptoNets is a notable implementation of this approach, though it introduces significant performance overhead and imposes constraints on model architecture due to limitations in the arithmetic operations supported by homomorphic encryption (Papernot et al., 2018). Multi-party computation (MPC) provides another cryptographic alternative for secure inference against honest-but-curious coordinators (Zheng et al., 2019)(Barni et al., 2006).

Trusted Execution Environments (TEEs) have become increasingly popular for securing ML inference. Systems like S3ML leverage Intel SGX to protect the confidentiality and integrity of user data while providing high-availability and scalable ML inference services through secure key management (Ma et al., 2020). TEEs have been extensively explored for ML inference, often incorporating optimizations like model partitioning to overcome memory constraints (Duddu et al., 2024)(Mo et al., 2020). Some approaches combine TEEs with GPUs for efficient execution, adding cryptographic checks to ensure data integrity between the TEE and GPU (Duddu et al., 2024)(Tramer et al., 2018).

Privacy concerns in ML inference are particularly pronounced when dealing with sparse data and sparse models. While sparse implementation patterns (SIP) improve performance on sparse data, they inherit privacy issues from plaintext computations (Xu et al., 2022). In typical ML inference platforms, clients must provide plaintext queries while servers expose sparsity details of model parameters, creating privacy breaches for both parties (Xu et al., 2022)(Zheng et al._1, 2019). Client queries often contain sensitive information such as personal physiological data, financial information, or medical history (Xu et al., 2022)(Sav et al., 2020), while model parameters represent valuable intellectual property that service providers need to protect (Xu et al., 2022).

Despite advances in privacy-preserving techniques, significant challenges remain. For large language models (LLMs), current approaches like homomorphic encryption are not yet applicable to inference, and federated learning approaches for LLMs require substantial modifications to the model and training process (Chong et al., 2024)(Zhang et al., 2023). As ML models continue to grow in size and complexity, developing efficient privacy-preserving inference methods remains an active and important area of research.

Edge computing has emerged as a promising approach for deploying machine learning models closer to data sources, addressing both environmental sustainability concerns and privacy issues associated with traditional cloud-based deployments. The shift toward on-device inference and edge ML applications is driven by financial and environmental considerations, as server-side scaling for model deployment becomes increasingly unsustainable (Sreeram, 2023). This transition is evidenced by commercial applications like iOS 15's offline Siri functionality and the development of mobile-friendly model architectures such as MobileNet, MobileViT, and MobileBERT (Sreeram, 2023).

Privacy concerns are particularly significant in distributed and cooperative ML models operating at the edge. While these approaches are crucial for achieving higher accuracy and faster learning, they introduce privacy and trust challenges that require careful consideration (Gur, 2020). In edge computing environments, collaborative learning must enable model training from shared data sources such as spectrum occupancy and network configuration information without compromising the privacy of participating entities (Gur, 2020).

Distributed learning approaches typically involve exchanging model parameters rather than raw data, providing a first layer of privacy protection. However, research has shown that exposed model parameters could be reversely traced, meaning privacy is only partially preserved (Park et al., 2020)(Fredrikson et al., 2015). To enhance privacy protection, approaches such as adopting extra coding, introducing noise to shared parameters, and exchanging redundant information have been proposed, yet each introduces additional challenges including increased processing delays, reduced inference accuracy, or additional communication overhead (Park et al., 2020).

Federated learning (FL) has emerged as a particularly promising approach for addressing edge computing privacy challenges. FL offers a privacy-preserving, scalable, and efficient solution that addresses many shortcomings of both traditional centralized and decentralized ML (Driss et al., 2023). This approach is especially well-suited for applications involving sensitive data, edge computing, and distributed environments (Driss et al., 2023).

To address trust issues in distributed edge environments, blockchain technology has been explored as a promising solution. The integration of blockchain with AI in wireless networks can facilitate flexible and secure resource sharing while mitigating various privacy and trust concerns (Gur, 2020)(Dai et al., 2019). This combined approach enables the establishment of a secure and decentralized resource sharing environment, enhancing the performance and privacy guarantees of distributed ML systems operating at the edge (Dai et al., 2019).

Machine learning models are increasingly vulnerable to a range of privacy threats that target both the data used to train them and the models themselves. Inference attacks represent a significant security and privacy risk, allowing adversaries to extract sensitive information from ML models (Liu et al., 2021). These attacks can be categorized into four representative types: membership inference (determining if a specific data sample was used in training), model inversion (recovering training data), attribute inference (predicting properties unrelated to the model's original task), and model stealing (reconstructing non-public model parameters) (Liu et al., 2021)(Shokri et al., 2016)(Song et al., 2019)(Fredrikson et al., 2015). The consequences of these attacks can be severe, potentially violating individuals' privacy and compromising valuable intellectual property (Liu et al., 2021).

Membership inference attacks, which reveal whether specific data was used to train a model, have been demonstrated across various commercial ML-as-a-service platforms (Shokri et al., 2016). These attacks are particularly concerning in settings involving sensitive data, such as medical information (Shokri et al., 2016). Research has shown that such attacks can be executed with fewer assumptions than previously thought, making them broadly applicable at low cost (Salem et al., 2018). This increases the severity of the privacy risk posed by deployed ML models.

Model inversion attacks aim to reconstruct the training data used to build ML models. While early work demonstrated such attacks against simple models like linear and logistic regression (Fredrikson et al., 2015), recent advances have enabled successful attacks against deep neural networks. For instance, generative model-inversion attacks leverage partial public information and generative adversarial networks to guide the inversion process (Zhang et al., 2019). These techniques have achieved approximately 75% improvement in identification accuracy for reconstructing face images from state-of-the-art face recognition classifiers (Zhang et al., 2019).

Model stealing attacks target the intellectual property of ML models by attempting to duplicate their functionality. These attacks are particularly relevant in ML-as-a-service contexts, where models may be accessed through public query interfaces (Tramer et al., 2016). Research has shown that even without prior knowledge of a model's parameters or training data, adversaries can extract target ML models with near-perfect fidelity for various model classes including logistic regression, neural networks, and decision trees (Tramer et al., 2016). Moreover, model functionality stealing can be achieved by querying random images from a different distribution than the blackbox training data, even when using a different architecture for the "knockoff" model (Orekondy et al., 2018).

Collaborative learning environments, such as federated learning, introduce additional privacy vulnerabilities. Adversarial participants can infer the presence of specific data points in others' training data or deduce properties that are independent of the joint model's intended purpose (Melis et al., 2018). Even in online learning scenarios, where models are updated with newly collected data, the changes in model outputs before and after updates can leak information about the updating dataset (Salem et al., 2019). This creates a new attack surface against black-box ML models that may compromise both intellectual property and data privacy (Salem et al., 2019).

Differential privacy has emerged as a promising defense mechanism against inference attacks, particularly membership inference (Zaman et al., 2024)(Shokri et al., 2016). In federated learning contexts, differential privacy can be applied either centrally (requiring a trusted party) or locally (where users perturb updates before sending them to an untrusted aggregator) (Hu et al., 2021)(Truex et al., 2018). However, these approaches face challenges in balancing privacy protection with model utility (Hu et al., 2021). Recent work has explored novel designs of local differential privacy mechanisms that adapt to varying ranges at different layers of deep neural networks, offering improved performance while maintaining strong privacy guarantees (Sun et al., 2020).

The intersection of security, privacy, and machine learning remains an active research area with many open problems (Papernot et al., 2016). As ML models continue to evolve in complexity and deployment scope, addressing privacy threats will require ongoing development of robust defense mechanisms that balance protection against various attack vectors with the maintenance of model utility.

Cryptographic technologies have emerged as foundational solutions for privacy-preserving machine learning (PPML), addressing concerns regarding both user data privacy and model intellectual property protection. Homomorphic encryption (HE) has garnered significant attention for its ability to enable computation on encrypted data, allowing models to process user inputs without decryption (Chong et al., 2024)(Gentry, 2009). This capability makes HE particularly suitable for distributed ML scenarios, potentially overcoming privacy and security concerns while facilitating confidential secure computing (Marcolla et al., 2022). However, these benefits come with substantial performance trade-offs, as homomorphic encryption introduces significant computational overhead and imposes constraints on supported model architectures.

For large language models (LLMs), current homomorphic encryption approaches face limitations that prevent direct application to inference tasks (Chong et al., 2024). These limitations highlight the ongoing challenge of balancing privacy protection with computational efficiency, particularly as model size and complexity increase. Similarly, cloud-hosted ML services employing homomorphic encryption-based protocols have been shown vulnerable to model extraction attacks, undermining their privacy guarantees (Qayyum et al., 2020)(Reith et al., 2019).

Secure Multi-Party Computation (SMC) represents another promising cryptographic approach for privacy-preserving ML. By enabling multiple parties to jointly compute functions without revealing their inputs, SMC can facilitate collaborative model training and inference while maintaining data confidentiality. When combined with differential privacy in federated learning frameworks, SMC helps protect against extraction attacks and collusion threats, providing end-to-end privacy guarantees (Truex et al., 2018). However, like homomorphic encryption, SMC implementations typically incur higher computation and communication costs compared to plaintext evaluations.

Zero-knowledge proofs (ZKPs), particularly zero-knowledge Succinct Non-interactive Arguments of Knowledge (zkSNARKs), have recently gained traction for verifiable machine learning. These cryptographic primitives allow one party to prove to another that a statement is true without revealing any additional information, making them suitable for verifying properties like fairness and privacy in ML systems (Duddu et al., 2024)(Kilbertus et al., 2018). Recent advances in this space include zero-knowledge proofs of training (zkPoT), which formalize security guarantees for privacy-preserving proof of model training (Garg et al., 2023). While theoretically powerful, zkSNARK-based approaches often face practical limitations regarding proof generation times for complex ML operations.

Trusted Execution Environments (TEEs) have emerged as a pragmatic alternative to pure cryptographic solutions, offering hardware-based isolation for sensitive computations. TEEs have been extensively explored for both ML training and inference (Duddu et al., 2024)(Segal et al., 2020)(Mo et al., 2021)(Mo et al., 2020). Systems leveraging TEEs often employ model partitioning to overcome memory constraints and can be combined with GPUs for efficient execution, adding cryptographic checks to ensure data integrity between the TEE and GPU (Tramer et al., 2018). While primarily focused on data confidentiality, TEEs can also provide integrity guarantees necessary for ML verifiability (Duddu et al., 2024).

Hybrid approaches combining multiple privacy-preserving techniques show particular promise. For instance, some systems integrate homomorphic encryption with hardware-based security reinforcement through Software Guard Extensions (SGX), demonstrating practical hybrid cryptographic solutions (Qayyum et al., 2020)(Jiang et al., 2019). Other research explores combining techniques from MPC-in-the-head and zkSNARKs to achieve practical trade-offs between proof size and computation time (Garg et al., 2023). Such hybrid approaches may offer the most viable path forward, addressing the limitations of individual techniques while leveraging their complementary strengths.

The development of privacy-preserving ML for cloud computing platforms remains an active research area with many open challenges (Qayyum et al., 2020). As ML models continue to grow in complexity and deployment scope, finding efficient cryptographic and hardware-based solutions that effectively balance privacy protection with performance represents a critical frontier for the field.

----------------------------------------------------------------------------------

Large Language Models (LLMs) have demonstrated extraordinary capabilities in natural language processing tasks, exhibiting strong textual understanding, generation, and reasoning through large-scale pre-training (Li et al., 2023)(Ning et al., 2023). This success has inspired the development of Multimodal Large Language Models (MLLMs), which extend language models to process visual information alongside text input (Huang et al., 2024).

In the image domain, researchers have adopted several approaches to develop MLLMs. Some methods incorporate additional parameters inside LLMs, such as gated cross-attention or adapter layers, while others employ projection layers or Q-Formers to connect visual encoders to LLMs (Huang et al., 2024). Notable image-based MLLMs include Flamingo, which bridges vision-only and language-only models through cross-attention (Li et al., 2023)(Alayrac et al., 2022), BLIP-2 with its Query Transformer (Q-Former), and LLaVA, which uses a simple linear layer to connect the visual encoder with the LLM (Li et al., 2023)(Liu et al., 2023).

Building on the success with images, the research community has expanded MLLMs to video understanding (Yuan et al., 2024). Video MLLMs typically follow similar architectural patterns as their image counterparts, using pre-trained vision models to extract sequence-based information from videos, which is then integrated with textual embeddings for the LLM to process (Yuan et al., 2024). Examples include Video-LLaMA, which aligns features from both visual and audio encoders with the LLM's embedding space using specialized Q-formers (Li et al., 2023), and Video-ChatGPT, which merges a video-adapted visual encoder with an LLM (Maaz et al., 2023).

Despite their promising results, current video MLLMs still face challenges in regional and temporal understanding (Yuan et al., 2024). These models can serve as critical components for a broad range of downstream applications, such as Video Question Answering and Natural Language Queries, both of which require comprehensive understanding and reasoning of text and video (Wang et al., 2023).

With the success of Multimodal Large Language Models (MLLMs) in image understanding, researchers have explored adapting these models to video tasks. Two primary approaches have emerged: zero-shot inference with frozen image-based MLLMs and fine-tuning with video data.

The zero-shot approach leverages pre-trained image-language models directly for video understanding without additional training. Madasu et al. investigated the generalization abilities of image-text models across various video understanding tasks in a zero-shot setting, aiming to avoid costly pre-training steps while effectively adapting foundational image-text models to videos (Madasu et al., 2023). This approach is attractive as it avoids the expense of collecting and annotating large video datasets, which is significantly more challenging than for images (Li et al., 2024)(Bain et al., 2021).

However, zero-shot applications face important limitations in temporal understanding. Wang et al. noted that MLLMs, while impressive for many vision-language tasks, struggle with comprehensive understanding and reasoning of text and videos in long-form contexts (Wang et al., 2023). This temporal understanding gap is particularly evident in challenging benchmarks like EgoSchema, which requires intrinsic temporal understanding capabilities (Li et al., 2024)(Mangalam et al., 2023).

The alternative approach involves fine-tuning image-based MLLMs with video data. Video-LLaMA aligns features from both visual and audio encoders with an LLM's embedding space using specialized video and audio Q-formers, training on massive video/image-caption pairs and visual-instruction-tuning datasets (Li et al., 2023). Other models like VideoChat utilize learnable modules to combine video foundation models with LLMs (Li et al., 2023). Some approaches simply extend image LLMs to video LLMs by projecting visual tokens to LLMs' input space using projection layers or Q-Formers (Huang et al., 2024).

Despite the promising results of both approaches, researchers continue to face challenges in efficient video-language representation learning. Video adaptation models still struggle with temporal localization capabilities even after fine-tuning (Huang et al., 2024). These limitations have motivated innovative solutions such as synthesizing video-like samples to enrich instruction diversity in training data, as proposed in the user's research.

Early approaches to video-language representation learning built upon the success of transformer-based architectures. UniViLM proposed a unified video and language pre-training model adopting Transformer as a backbone with four components: two single-modal encoders, a cross encoder, and a decoder (Luo et al., 2020)(Vaswani et al., 2017). This architecture performed four pre-training tasks: masked language modeling, masked frame modeling, video-text alignment, and language reconstruction.

The collection and curation of large-scale video-language datasets has been critical for advancing the field. Datasets like HowTo100M, containing 136 million video clips with automatically transcribed narrations, have enabled the training of text-video embeddings (Miech et al., 2019). Similarly, WebVid-2M provides over two million videos with weak captions scraped from the internet (Bain et al., 2021). These datasets have facilitated the development of models that can handle tasks such as video retrieval, video captioning, and video question answering across conventional benchmarks (Li et al., 2024).

Video understanding presents unique challenges compared to image understanding, particularly in capturing temporal relationships. Recent research has focused on developing more sophisticated approaches for aligning video and language representations. Models like ALBEF introduced a contrastive loss to align image and text representations before fusing them through cross-modal attention (Li et al., 2021). LAVENDER unified video-language tasks with Masked Language Modeling as a common interface (Li et al., 2022), while other approaches have explored the use of self-attention mechanisms to model temporal relationships in videos.

The quality of text descriptions paired with videos is crucial for building effective multimodal large language models. Detailed and accurate captions that capture both spatial features within individual frames and audio-visual events across time enable better multimodal understanding and reasoning (Tang et al., 2024). However, generating such captions for videos remains challenging due to the complexity of video content.

Recent work has begun leveraging large language models (LLMs) to enhance video-language representation learning. Approaches like LAVILA use pre-trained LLMs conditioned on visual input to create automatic video narrators (Zhao et al., 2022), while others employ LLMs to rewrite noisy ASR text or generate classification labels (Gupta et al., 2024). These methods have shown promise in improving zero-shot and few-shot video recognition transfer.

Despite progress, challenges remain in video-language representation learning, particularly in capturing fine-grained temporal understanding. Models struggle with a challenging long-form video understanding benchmark, EgoSchema, which requires intrinsic temporal understanding capabilities (Li et al., 2024)(Mangalam et al., 2023). Addressing these temporal understanding capabilities continues to be an active area of research.

Temporal understanding represents one of the most significant challenges in extending Multimodal Large Language Models (MLLMs) from image to video domains. While these models have shown impressive results across various tasks, they struggle with the fundamental temporal aspects of video content. As Chen et al. note, many video MLLMs "fail to understand... and are deficient in temporal perception" (Chen et al., 2023), highlighting a critical limitation in their capability to comprehend events that unfold over time.

The challenge of temporal understanding manifests in several forms. For video question answering and retrieval tasks, models must not only recognize objects and scenes but also understand how they interact and change over time. This requires processing "both high-level (scene) and low-level (object) visual clues and how they relate to the text query" (Wang et al._1, 2023). Similarly, for tasks like video temporal grounding, models must "accurately identify event timestamps within a given video" (Wang et al., 2024), a capability that traditional task-specific models have addressed but remains challenging for generalist video MLLMs.

Recent research has approached temporal understanding through various architectural innovations. Some models employ hierarchical structures to capture both short-term and long-term temporal associations. HierVL, for instance, introduces "a novel hierarchical video-language embedding that simultaneously accounts for both long-term and short-term associations" (Ashutosh et al., 2023). Other approaches focus on progressive understanding, where "high-level knowledge from local temporal information" (Wang et al._1, 2024).

Despite these efforts, temporal reasoning remains a persistent challenge. Recent studies reveal that "video-LLMs still struggle with tasks demanding sophisticated temporal comprehension" (Wang et al., 2025). Rather than effectively capturing complex dependencies between frames, these models often "exploit data biases, such as pre-training priors or simplistic visual-textual co-occurrence patterns" (Wang et al., 2025). This suggests that simply increasing the number of frames processed does not inherently improve temporal understanding.

The challenge extends to fine-grained temporal localization as well. Current video MLLMs face "challenges in fine-grained regional and temporal understanding" (Yuan et al., 2024), limiting their ability to precisely locate and describe events within videos. This is particularly evident in complex tasks that require "intrinsic temporal understanding capabilities" (Wang et al._1, 2024).

The significance of temporal understanding cannot be overstated for comprehensive video comprehension. As Wang et al. emphasize, video understanding encompasses several key areas including "action recognition, temporal action localization, video question answering (VQA), and video retrieval" (Wang et al._2, 2024). Each of these tasks requires sophisticated temporal reasoning to identify actions, locate their occurrences in time, and establish logical connections between visual content and textual context.

To address these challenges, researchers continue to explore various approaches for enhancing the temporal understanding capabilities of video MLLMs. These include specialized adapter structures to reduce token generation from video inputs (Wang et al., 2025), architectural innovations for aligning "spatiotemporal coordinates of linguistic and visual representations" (Zhou et al., 2025), and transformer-based architectures that directly link textual event descriptions with corresponding temporal segments in videos (Zhou et al., 2025).

The development of effective video MLLMs is hindered by challenges in data collection and curation that are more pronounced than for image MLLMs. While image-language understanding has advanced through pre-training on large-scale web-collected image-text data (Radford et al., 2021)(Jia et al., 2021)(Schuhmann et al., 2022), replicating this success for video-language understanding faces significant obstacles. As noted by Li et al., the question remains: "Can we mirror this success in video-language understanding?" (Li et al., 2024).

Research has attempted to address this challenge by exploring the pre-training of video-language models on millions of web video-text pairs (Miech et al., 2019)(Bain et al., 2021)(Wang et al._2, 2023). However, these datasets often contain inherent limitations in quality and temporal comprehension. Studies reveal that models trained on such data struggle with challenging long-form video understanding benchmarks that require intrinsic temporal understanding capabilities (Mangalam et al., 2023), highlighting a critical gap between web-collected data and comprehensive video understanding requirements.

The quality of text descriptions paired with videos is particularly crucial for building effective multimodal LLMs. Detailed and accurate captions that capture both spatial features within individual frames and audio-visual events across time enable better multimodal understanding and reasoning (Tang et al., 2024). However, generating such captions for videos remains challenging due to the complexity of video content.

To address these issues, researchers have explored various approaches to enhance data quality and diversity. Large Language Models (LLMs) have been leveraged to improve video-language datasets by rewriting noisy ASR text (Gupta et al., 2023) or generating classification labels (Gupta et al., 2024). Some approaches use LLMs conditioned on visual input to create automatic video narrators (Zhao et al., 2022), demonstrating positive results for zero-shot and few-shot video recognition transfer.

An emerging strategy to improve data efficiency involves synthetic data generation. Fan et al. introduced Language augmented CLIP (LaCLIP), which enhances training through language rewrites using LLMs (Fan et al., 2023). This approach introduces diversity in sentence structure and vocabulary while preserving original key concepts, improving transfer performance without computational overhead.

More recent multimodal approaches have focused on aligning different modality encoders with LLMs. ImageBind-based methods (Girdhar et al., 2023) create a joint embedding space for multiple modalities, which enables cross-modal reasoning capabilities (Su et al., 2023). Other models introduce specialized architectures for aggregating question-related audio-visual clues to improve response clarity (Ye et al., 2024).

For fine-tuning approaches, selective training can be more effective than using all available video data. Recent work has found that using a smaller, more diverse subset of video samples can achieve comparable or even superior performance to training on full datasets (LLM Memory). This suggests that instruction diversity may be more important than dataset size for effective video MLLM training.

Video-LLMs focused on temporal understanding face unique challenges in modeling long videos. Time-sensitive multimodal models have been developed with specialized architectures such as timestamp-aware frame encoders (Ren et al., 2023) and boundary-aware training strategies (Li et al._1, 2023)(Huang et al., 2023). These models have demonstrated improved performance on tasks requiring temporal localization, including dense video captioning (Krishna et al., 2017) and temporal video grounding (Hendricks et al., 2017)(Gao et al., 2017).

The challenge of data efficiency in video MLLMs remains an active area of research, with promising directions including synthetic data generation, selective training strategies, and specialized architectures for temporal understanding. As these approaches continue to evolve, they offer potential solutions to the limitations of web-collected video data and the unique challenges of video-language representation learning.

Long video understanding presents unique challenges that differentiate it from the comprehension of short video clips. Unlike short videos where a single key frame often captures most semantic information, long-form videos such as movies, tutorials, and documentaries contain multiple sequential events that require sophisticated temporal reasoning capabilities (Wang et al._3, 2024). This complexity makes understanding long-form videos particularly difficult for current video MLLMs.

Traditional approaches to video understanding have typically focused on processing all visual content uniformly, which becomes computationally expensive and less effective for longer videos. To address this issue, researchers have explored hierarchical video representation methods. HierVL proposed a novel hierarchical video-language embedding that captures associations at both clip and video levels, enabling models to understand both immediate actions ("what is happening") and broader context ("why it is happening") (Wang et al._1, 2024)(Ashutosh et al., 2023). Similarly, VideoReCap introduced a progressive video captioning approach that first generates clip-level captions and then summarizes them into longer segments (Wang et al._1, 2024).

More recent approaches have adopted a top-down strategy for processing long videos. VIDEOTREE proposed a tree structure that enables efficient long video understanding by dynamically extracting query-relevant keyframes for LLM reasoning (Wang et al._1, 2024)(Wang et al._1, 2023). This coarse-to-fine approach contrasts with traditional bottom-up methods that first capture all low-level details before aggregating, resulting in significant computational advantages.

Several video LLMs have been extended specifically to handle long videos. Video-ChatGPT encodes frames independently and employs average pooling to derive temporal and spatial features, which are then concatenated to create video-level features for the LLM (Li et al._1, 2024)(Maaz et al., 2023). Video-LLaMA processes video frames and audio signals separately using Vision-Language and Audio-Language models, recognizing that videos are inherently multimodal, containing both visual and auditory information (Li et al._1, 2024)(Shu et al., 2023). Video-Chat takes yet another approach, converting videos into textual descriptions in real-time and employing a foundation model to encode videos into embeddings (Li et al._1, 2024).

For effective long video understanding, models must not only comprehend the content but also precisely locate events within the temporal dimension. Time-sensitive MLLMs have emerged as a solution to this challenge. VTimeLLM adopts a boundary-aware training strategy that increases temporal-boundary awareness and improves temporal understanding capabilities (Deng et al., 2024)(Huang et al., 2023). Similarly, TimeChat incorporates a timestamp-aware frame encoder that binds visual content with frame timestamps and a sliding video Q-Former that accommodates videos of various durations (Deng et al., 2024)(Ren et al., 2023).

These time-sensitive models have demonstrated superior performance on tasks requiring detailed temporal understanding, such as dense video captioning (Deng et al., 2024)(Krishna et al., 2017) and temporal video grounding (Deng et al., 2024)(Hendricks et al., 2017)(Gao et al., 2017). Recent research has also introduced specialized benchmarks like MVBench to evaluate temporal understanding in video MLLMs, revealing that existing models often struggle with tasks requiring sophisticated temporal comprehension (Li et al._1, 2023).

Despite progress in model architectures and evaluation frameworks, challenges in long video understanding persist. Current approaches often struggle to align audio-visual modalities effectively, with models like Video-LLaMA potentially limited by their reliance on pre-trained models like Image-Bind for audio signal representation (Shu et al., 2023). Similarly, approaches that use visual and audio signals from different sources may introduce bias and instability in training (Shu et al., 2023). These observations highlight continuing opportunities for improvement in long video understanding, particularly in the joint processing of audiovisual data over extended temporal sequences.

----------------------------------------------------------------------------------

Recent advances in Large Language Models have revealed their impressive yet still developing reasoning capabilities. Despite significant progress, even advanced models like GPT-3 and ChatGPT still struggle with complex reasoning tasks and are prone to generating factual errors, highlighting the need for further development in structural reasoning skills (Wang et al., 2023). As models scale in size, their reasoning abilities improve predictably, enhancing their effectiveness on new tasks (Chen, 2023)(Brown et al., 2020)(Wei et al., 2022). This emergence of reasoning capabilities with increased model size represents a fundamental attribute of LLMs (Gui et al., 2024).

LLMs demonstrate versatility in reasoning across multiple domains. They can perform deductive reasoning over explicitly provided natural language statements and even transfer this ability to different domains in a zero-shot manner (Yu et al., 2023). Additionally, these models can combine memorized implicit taxonomic knowledge with explicitly provided information and perform defeasible reasoning, where inferences may be weakened or overturned with new evidence (Yu et al., 2023)(Rudinger et al., 2020). Recent research has established benchmarks to evaluate various reasoning forms including logical reasoning (inductive, deductive, and abductive), mathematical reasoning (from grade school problems to Olympiad-level challenges), and knowledge-based reasoning such as commonsense reasoning (Gui et al., 2024).

The growing research focus on enhancing reasoning capacities in LLMs aims to achieve human-level or even superhuman reasoning within specialized domains (Peng et al., 2024). However, LLM reasoning occurs in a parameterized, implicit space that heavily depends on embedded knowledge, which can lead to hallucinations that undermine result reliability (Chen, 2023). This contrasts with knowledge graph reasoning, which is based on explicitly acquired symbolic knowledge, making it more interpretable and reliable but often lacking the robustness and generalization capabilities of LLMs (Chen, 2023).

Chain-of-Thought (CoT) prompting has emerged as a groundbreaking approach for enhancing complex reasoning capabilities in language models. Introduced by Wei et al., CoT reveals language models' ability to formulate their own "thinking procedure" for problem solving by generating a series of logical intermediate reasoning steps (Wei et al._1, 2022). This approach has significantly improved performance on complex reasoning tasks across multiple domains, including commonsense reasoning, arithmetic, and symbolic reasoning (He et al., 2023). The effectiveness of CoT has sparked numerous follow-up works that build upon this foundational approach.

Several variants of CoT have been developed to enhance reasoning performance further. Self-consistency, for instance, samples multiple diverse reasoning paths instead of relying solely on greedy decoding, then selects the most consistent answer by marginalizing out the sampled paths (Wang et al., 2022). This approach leverages the intuition that complex reasoning problems typically admit multiple valid reasoning paths leading to a correct answer. Other variants include least-to-most prompting for solving complex tasks by breaking them down into simpler subproblems (Zhou et al., 2022), and approaches that employ external modules to verify and refine intermediate thoughts (Park et al., 2024).

While CoT prompting has proven highly effective, its success is predominantly observed in larger models with billions of parameters (Yao et al., 2022). To address this limitation, researchers have developed knowledge distillation approaches that transfer reasoning capabilities from larger models to smaller ones. Magister et al. demonstrated that reasoning capabilities can be transferred to smaller models by finetuning them on CoT outputs generated by larger teacher models (Magister et al., 2022). Similarly, Ho et al. proposed Fine-tune-CoT, where reasoning samples from teacher models are used to enhance smaller student models, enabling them to outperform even the teacher model on many tasks (Ho et al., 2022).

More specialized distillation approaches have also been developed. Shridhar et al. introduced Socratic CoT, which trains two smaller distilled models—a problem decomposer and a subproblem solver—that work in synchrony to decompose and solve complex problems (Shridhar et al., 2022). Wang et al. proposed an interactive multi-round learning paradigm where student models provide feedback to teacher models during the distillation process, obtaining tailored training data and learning from self-reflection (Wang et al._1, 2023). Hsieh et al. introduced "Distilling step-by-step," which extracts LLM rationales as additional supervision within a multi-task framework, achieving better performance with fewer labeled examples (Hsieh et al., 2023).

An important consideration in knowledge distillation for reasoning is data quality. Zhu et al. observed that synthetic CoT data often contains faulty reasoning, which deteriorates distillation quality. Their Program-aided Distillation (PaD) approach introduces reasoning programs instead of CoT, allowing automated error checking and iterative self-refinement of reasoning (Zhu et al., 2023). Similarly, Wang et al. developed SCOTT, which elicits rationales supporting gold answers through contrastive decoding and employs a counterfactual reasoning objective to ensure faithful distillation (Wang et al._2, 2023).

Recent research has also highlighted that model specialization can significantly improve reasoning performance for smaller models. Fu et al. demonstrated that by concentrating model capacity on specific target tasks like multi-step math reasoning, smaller models can achieve substantially improved performance, although this comes at the cost of decreased general abilities (Fu et al., 2023). Additionally, Wei et al. noted that reasoning decomposes complex problems into multiple simpler problems, making decisions based on context and patterns, which can be more effective than learning from vast training data through traditional optimization objectives (Wei et al., 2025).

Researchers have also identified different reasoning strategies employed by language models. Wang et al. described the Vertical Thinking Strategy (VTS), where models output reasoning results in a single forward pass based on their inherent structure—an efficient approach that demands high intelligence from the model itself (Wang et al., 2024)(Brown et al., 2020). While effective, such strategies often struggle to generalize to unseen, complex scenarios that diverge significantly from few-shot demonstrations (Zhou et al., 2022). To address this limitation, various approaches have been proposed, including using deterministic solvers for intermediate steps in math problems, finetuning language models to ensure logical validity, and decomposing questions into subquestions (Kim et al., 2024).

While forward reasoning (from premises to conclusions) has been extensively studied in LLMs, reverse reasoning approaches offer complementary capabilities by working backward from observations or conclusions to find explanations or premises. Abductive reasoning represents a primary form of reverse reasoning, defined as "a reverse reasoning process that finds the most likely explanation for a particular observation" (He et al., 2024). This approach has been categorized into abductive commonsense reasoning, which explains abnormal observations in daily events (Bhagavatula et al., 2019)(Zhang et al., 2020)(Rudinger et al., 2020)(Wang et al., 2019), and abductive logical reasoning, which identifies missing causes in logical reasoning paths without requiring external knowledge (He et al., 2024).

Bidirectional reasoning, which combines forward and backward reasoning processes, has shown particular promise in mathematical domains. Prior research demonstrates that LLMs benefit from forward-backward reasoning in mathematics, largely due to the structured nature of math problems and the clear inverse relationship between forward and backward reasoning (Chen et al., 2024). These approaches have typically been employed at test time for verification purposes—checking whether forward reasoning is correct by examining backward reasoning from the solution (Chen et al., 2024). However, recent work like REVTHINK proposes moving beyond test-time verification to train models that inherently incorporate backward thinking, thereby enhancing their forward reasoning capabilities (Chen et al., 2024).

A key research question emerging from this field is whether reverse thinking approaches can be effectively applied beyond highly structured domains like mathematics to areas with less rigid formulations (Chen et al., 2024). Defeasible inference represents one such extension, where inferences may be weakened or overturned in light of new evidence (Rudinger et al., 2020). This form of reasoning allows for more nuanced and adaptable inference processes that better reflect human reasoning in ambiguous or evolving contexts. As research progresses, the integration of both forward and backward reasoning capabilities appears crucial for developing models with more robust, human-like reasoning abilities across diverse domains.

Knowledge distillation has emerged as a critical technique for transferring the reasoning capabilities of large language models (LLMs) to smaller, more deployable models (Bucila et al., 2006). This approach becomes particularly important as reasoning abilities typically emerge only in models with tens of billions of parameters, making their direct deployment challenging in resource-constrained environments (Magister et al., 2022). Distillation techniques enable these capabilities to be compressed into smaller models while maintaining reasonable performance across arithmetic, commonsense, and symbolic reasoning tasks (Magister et al., 2022).

Several pioneering approaches focus on distilling chain-of-thought (CoT) reasoning from teacher models to student models. Ho et al. demonstrated that reasoning capabilities can be transferred to smaller models by fine-tuning them on diverse CoT outputs generated by larger teacher models, sometimes enabling student models to outperform even their teachers (Ho et al., 2022). Similarly, Magister et al. showed that reasoning capability transfer depends on both model and dataset size, with student models achieving significant improvements on benchmark tasks after being fine-tuned on CoT outputs from larger models (Magister et al., 2022).

More specialized distillation frameworks have evolved to address specific challenges in reasoning transfer. Hsieh et al. introduced "Distilling step-by-step," a multi-task learning approach that extracts LLM rationales as additional supervision for smaller models (Hsieh et al., 2023). This method significantly reduces the amount of training data needed compared to traditional fine-tuning while enabling smaller models to outperform few-shot prompted LLMs (Hsieh et al., 2023). Shridhar et al. proposed Socratic CoT, which decomposes the distillation process by training two specialized student models—a problem decomposer and a subproblem solver—that work together to tackle complex reasoning tasks (Shridhar et al., 2022). This approach boosts smaller models' performance by over 70% compared to baselines and even enables cases where much smaller models can outperform those ten times larger (Shridhar et al., 2022).

A significant challenge in distillation for reasoning is ensuring the quality of the synthetic training data. Zhu et al. observed that synthetic CoT data often contains faulty reasoning, which deteriorates distillation quality (Zhu et al., 2023). Their Program-aided Distillation (PaD) approach addresses this by using reasoning programs instead of CoT, enabling automated error checking and iterative self-refinement (Zhu et al., 2023). Similarly, Wang et al. developed SCOTT, which elicits rationales supporting gold answers through contrastive decoding and employs a counterfactual reasoning objective to ensure faithful distillation (Wang et al._2, 2023).

The interaction between teacher and student models has also been reimagined in newer approaches. Wang et al. established a multi-round interactive learning paradigm where student models provide feedback to teacher models during the distillation process, obtaining customized training data tailored to their specific learning needs (Wang et al._1, 2023). This approach incorporates a self-reflection learning mechanism that allows student models to learn from their mistakes and enhance their reasoning abilities (Wang et al._1, 2023).

An important consideration in knowledge distillation for reasoning is the potential trade-off between specialized and general capabilities. Fu et al. demonstrated that increasing task-specific capabilities through distillation may inadvertently lead to reduced performance in solving generalized problems (Fu et al., 2023). However, by concentrating model capacity on specific target tasks like multi-step math reasoning, smaller models can achieve substantial improvements in those domains (Fu et al., 2023).

Beyond traditional reasoning tasks, knowledge distillation has been applied to other domains requiring reasoning capabilities. West et al. developed NovaCOMET, a large-scale, open commonsense model that handles both explicit knowledge generation and tasks requiring commonsense reasoning through symbolic knowledge distillation (West et al., 2023). Additionally, researchers have begun exploring distillation methods that combine LLM-generated data with self-generated data to optimize strategy choice, striking a balance between leveraging external knowledge and self-improvement (Adarsh et al., 2024)(Hahn et al., 2019).

Research into LLM reasoning has expanded beyond general capabilities to focus on specialized domains that require distinct types of reasoning skills. Mathematical reasoning has emerged as a prominent area of focus, with researchers developing benchmarks ranging from elementary math to competition-level problems (Hendrycks et al., 2021). These benchmarks have enabled rigorous evaluation of LLMs' quantitative reasoning abilities, with recent models demonstrating impressive performance on fundamental math problems while still struggling with more complex mathematical tasks that require specific theorem application (Chen et al., 2023). The development of TheoremQA has further extended this evaluation to problems requiring domain-specific knowledge across mathematics, physics, computer science, and finance, revealing significant limitations in open-source models compared to proprietary ones like GPT-4 (Chen et al., 2023).

Causal reasoning represents another specialized domain that challenges LLMs, encompassing abilities like identifying cause-effect relationships, abductive reasoning, and counterfactual reasoning (Liu et al., 2024). Researchers have explored abductive reasoning—inference to the most plausible explanation for observations—finding it central to natural language understanding but still challenging for current models (Bhagavatula et al., 2019). Similarly, counterfactual reasoning requires predicting how alternative events might have led to different outcomes, a capability crucial for AI systems but not yet fully mastered by LLMs (Qin et al., 2019). Interestingly, recent studies suggest that code-trained LLMs may exhibit stronger causal reasoning abilities than text-only models, potentially because programming languages express causal relationships more explicitly through conditional statements (Liu et al., 2023).

Scientific reasoning extends mathematical and causal reasoning to broader domains, with researchers developing multimodal datasets like ScienceQA to evaluate models' abilities to synthesize information across modalities and generate coherent chains of thought (Lu et al., 2022). These benchmarks assess not only factual knowledge but also the ability to apply scientific principles to novel problems, revealing both strengths and limitations in current models' scientific reasoning capabilities (Chen et al., 2023).

Different approaches to enhancing specialized reasoning have emerged, including both pure neural methods and neuro-symbolic hybrids. While some researchers focus on prompt engineering techniques like chain-of-thought and output verification to improve reasoning performance without additional training (Cao, 2023), others advocate for neuro-symbolic approaches that combine LLMs with explicit reasoning systems. Rajasekharan et al. propose separating neural and symbolic components, using LLMs for predicate extraction and symbolic systems for explicit, reliable, and explainable reasoning (Rajasekharan et al., 2023). This approach differs from integrated neuro-symbolic systems like NeurASP that embed neural networks within answer set programs to combine sub-symbolic and symbolic computation (Yang et al., 2020).

The development of specialized reasoning capabilities in LLMs remains an active research area, with studies investigating how these models might capture causal representations and perform domain-specific reasoning tasks (Willig et al., 2022). Recent evaluations of models like ChatGPT suggest that while they may excel as causal explainers, they still struggle with accurate causal reasoning and exhibit hallucination issues that can be exacerbated by techniques like in-context learning and chain-of-thought (Gao et al., 2023). These findings highlight both the progress made in specialized reasoning domains and the substantial challenges that remain in developing models with robust, domain-specific reasoning capabilities.

----------------------------------------------------------------------------------

Scientific discovery and innovation rely on researchers having a rich understanding of prior work, which they achieve through reviewing literature, extracting meaning, and identifying connections across papers with domain-specific information (Palani et al., 2023)(Knopf, 2006)(Zhang et al., 2008). This process has become increasingly challenging due to the exponential growth of scientific publications (Bornmann et al., 2020)(Jinha, 2010) and the increasingly interdisciplinary nature of science (Okamura, 2019).

In scientific papers, the "Related Work" section is crucial for situating research in the field and clarifying the new contribution of the proposed work (Narimatsu et al., 2021). Writing this section can be viewed as a type of multi-document summarization, though it differs from traditional summarization approaches as it aims to highlight specific relations of each input article to the current discovery (Shah et al., 2021). Prior research has identified that reasons for citing papers fall into several argumentative classes such as reliance on a previous result or gap in existing solutions (Shah et al., 2021)(Teufel et al., 2006).

Literature reviews serve multiple purposes: they help scholars identify patterns and gaps in prior research, find opportunities, determine rationale for new investigations, and situate research goals within the literature (Palani et al., 2023). A high-quality literature review comprehensively includes all main themes and sub-themes in a chosen topic, from both classic foundational work and recent studies (Palani et al., 2023)(Denney et al., 2013)(Knopf, 2006).

Researchers explore and learn from the literature by reading scientific papers, which not only provides insights into individual prior work but also allows them to discover connections to additional relevant papers via inline citations (Chang et al., 2023)(King et al., 2009). This process enables researchers to contextualize papers within cited work, become aware of research threads that influenced the current paper, and discover other relevant papers to further their literature reviews (Chang et al., 2023)(Evans, 2008). The time investment scholars make in reading articles—estimated at 132 hours and 240 articles per year—demonstrates their value to faculty's work (King et al., 2009).

The task of automatic related work generation (RWG) aims to assist researchers by automatically producing a draft of the "Related Work" section given a target paper and its references (Li et al., 2022). This area has gained increasing attention in recent years as researchers seek tools to handle the growing volume of scientific literature. The pioneering work of Hoang and Kan framed RWG as a topic-biased, multi-document summarization problem, identifying three main steps: finding relevant documents, identifying salient aspects worth summarizing, and generating a topic-biased summary (Li et al._1, 2022)(Hoang et al., 2010).

Early approaches to RWG primarily used extractive methods, which directly pulled sentences from original papers to form a related work discussion. However, this approach proved inflexible for creating cohesive sections (Chen et al., 2021). More recent work has shifted toward abstractive techniques that generate new sentences rather than simply extracting existing ones. Chen et al. proposed a Relation-aware Related work Generator (RRG) that creates abstractive summaries by modeling content dependencies between papers in a relation graph (Chen et al., 2021). Similarly, Chen et al. later developed a target-aware related work generator (TAG) that captures relationships between reference papers and the target paper through target-centered attention mechanisms (Chen et al., 2022)(Liu et al., 2023).

Various researchers have explored different encoding strategies and knowledge sources to improve RWG systems. Ge et al. encoded citation networks as external knowledge, while Xing et al. utilized citation contexts alongside paper abstracts (Liu et al., 2023). Wang et al. developed a neural data-driven summarizer using the seq2seq paradigm with a joint context-driven attention mechanism that measures contextual relevance within full texts and a heterogeneous bibliography graph simultaneously (Shi et al., 2023)(Wang et al., 2019).

Despite these advances, most existing RWG methods focus primarily on summarizing pre-selected reference papers rather than addressing the initial challenge of retrieving relevant papers from large-scale corpora (Shi et al., 2023). This highlights an underexplored area in the RWG pipeline, as retrieving related papers represents a fundamental first step before generating the summarized related work section.

The study of strategic communication has deep roots in information economics, with seminal contributions from Crawford and Sobel who established the cheap talk model where a sender with private information communicates with a receiver who takes an action affecting both parties' utilities (LLM Memory). Unlike cheap talk models where messages are costless, Spence's signaling model introduced costly actions that can credibly convey information when costs correlate with the sender's type (LLM Memory).

Bayesian persuasion, formalized by Kamenica and Gentzkow, has become a cornerstone framework for analyzing communication with commitment power, where a sender commits to an information disclosure policy before observing private information (LLM Memory). This framework has been extended by Lipnowski and Ravid to explore cases where senders have state-independent preferences, showing that commitment power allows senders to persuade receivers through strategic information disclosure even when their interests diverge substantially (LLM Memory).

Recent literature has expanded beyond pure information design by incorporating monetary transfers and costly signaling. Deb, Pai, and Said examined communication mechanisms that combine information revelation with monetary transfers, demonstrating how payment schemes can enhance information transmission (LLM Memory). Similarly, Frankel and Kartik analyzed settings where senders use both cheap talk and costly signaling, finding that these combined approaches can achieve outcomes superior to either method alone (LLM Memory).

Particularly relevant to the current paper, Karamychev and Visser examined "money burning" as a commitment device in communication, where the sender incurs observable costs to enhance credibility (LLM Memory). This approach differs from traditional signaling models because the cost is not naturally correlated with the sender's type but is strategically chosen to support information transmission. Kolotilin extended this analysis by characterizing optimal communication mechanisms with monetary transfers, showing how they can strictly improve upon pure information design solutions (LLM Memory).

The geometric interpretation of optimal persuasion strategies, as pioneered by Kamenica and Gentzkow and further developed by Dworczak and Martini, provides powerful tools for analyzing complex communication problems (LLM Memory). These approaches rely on concavification methods that visualize the sender's highest achievable payoff as a function of the receiver's beliefs, which bears similarity to the geometric characterizations used in the current paper (LLM Memory).

While prior work has examined either pure information design or costly signaling separately, the current paper bridges these approaches by analyzing how money-burning tactics can enhance the commitment power in mediated communication, with direct applications to modern Web 3.0 communities where such commitment devices naturally arise through blockchain technology (LLM Memory).

The emergence of Web 3.0 communities represents a significant shift in how online interactions are structured, with blockchain technology enabling new forms of commitment devices that parallel the theoretical money-burning mechanisms explored in this paper. Web 3.0 ecosystems fundamentally differ from traditional online communities through their incorporation of decentralized governance, tokenized incentives, and cryptographic verification that creates credible commitment mechanisms (LLM Memory).

Token-based governance systems in decentralized autonomous organizations (DAOs) have been extensively studied as novel commitment mechanisms. Buterin et al. analyzed how token staking serves as a costly signaling device that enables participants to demonstrate their commitment to community outcomes (LLM Memory). This aligns with our paper's characterization of money-burning tactics as the sender incurs observable costs (through token staking) to enhance message credibility.

Smart contracts provide another form of commitment device in Web 3.0 communities by enforcing agreements without requiring third-party mediation. As demonstrated by Cong and He, these contracts enable credible commitments by making the cost of defection prohibitively high through automated execution of penalties (LLM Memory). Our paper's analysis of optimal communication mechanisms with monetary transfers provides a theoretical foundation for understanding how these smart contract designs can maximize participant welfare.

The concept of "skin in the game" through financial commitment has been identified as crucial for maintaining community alignment in decentralized systems. Davidson et al. observed that successful Web 3.0 communities typically implement mechanisms requiring participants to burn or lock tokens as proof of commitment before gaining influence in governance decisions (LLM Memory). This directly corresponds to our model where the sender uses money-burning tactics to enhance commitment power in communication.

Reputation systems in Web 3.0 communities often incorporate economic costs that function similarly to the money-burning mechanisms we analyze. Reputation tokens that must be staked before making proposals or participating in governance create a direct financial incentive that enhances message credibility (LLM Memory). Our paper extends this understanding by providing a geometric interpretation of when such costly mechanisms optimally enhance communication.

While existing research has examined commitment mechanisms in Web 3.0 communities from a practical implementation perspective, our paper contributes a formal theoretical framework that characterizes the optimal design of such mechanisms. By linking the commitment value in these communities to robust Bayesian persuasion, we provide a rigorous foundation for understanding when and how money-burning tactics enhance communication efficiency in decentralized systems (LLM Memory).

----------------------------------------------------------------------------------

The Condorcet principle, proposed in the eighteenth century by the Marquis de Condorcet, represents one of the most significant normative principles in voting theory. This principle states that a candidate who would win against every other candidate in pairwise majority comparisons (known as a Condorcet winner) should be elected whenever such a candidate exists (Duddy, 2013). Many electoral theorists consider Condorcet consistency essential to fair voting, though interestingly, in May's review of 18 prominent voting systems, only 8 satisfied this property (Darlington, 2017)(May, 1952).

A key challenge for Condorcet-based methods is that a Condorcet winner does not always exist. This phenomenon, known as the Voting Paradox or Condorcet's paradox, occurs when collective preferences become cyclic, even when individual voter preferences are not (Zhang, 2020)(Yang, 2009). This paradoxical situation means that majority wishes can conflict with each other, creating fundamental tensions in democratic voting systems.

Various voting rules have been designed as Condorcet extensions, meaning they select the Condorcet winner when one exists but provide alternative methods for selecting winners in cycles or other problematic cases. These extensions have been studied extensively in the context of a broader evaluation framework that includes numerous other criteria. As Goel et al. discuss, following Arrow's Impossibility Theorem, which proved that no deterministic voting rule could satisfy three desirable criteria simultaneously, research has focused on the tradeoffs between different voting properties (Goel et al., 2012)(Pattanaik et al., 1986)(Gibbard, 1973).

The Condorcet principle's application can vary depending on the number of candidates. For instance, with only two candidates, the Condorcet rule is equivalent to the strict majority rule, allowing some theoretical results to be adapted between these frameworks (Kellerhals et al., 2017). More broadly, researchers have analyzed how various social choice paradoxes, including the Condorcet paradox, can be formalized within frameworks such as binary aggregation with integrity constraints (Grandi, 2014).

Voting theory is plagued by numerous paradoxes, which represent counterintuitive outcomes resulting from the application of specific voting rules. These paradoxes often highlight tensions between desirable properties in voting systems (Nurmi, 2020). A significant class of such paradoxical phenomena involves variable-electorate situations, where changes to the voter population produce unexpected effects on election outcomes.

Among the most studied variable-electorate paradoxes affecting Condorcet extensions are the reinforcement paradox and the no-show paradox. The reinforcement paradox, introduced by Young and Levenglick (1978), occurs when combining two electorates that would separately elect the same candidate results in a different winner (Brandt et al., 2024). This paradox reveals a fundamental limitation of Condorcet extensions, demonstrating that consistency across merged electorates cannot always be maintained.

The no-show paradox, formalized by Moulin (1988), represents another counterintuitive situation where voters might achieve a more favorable outcome by not participating in the election at all (Brandt et al., 2024). McCune provides a concrete example of this paradox, describing a scenario where voters "could have gotten their second-favorite candidate as the winner instead of their least-favorite, if they had abstained from the election" (McCune, 2023). This paradox has been observed in real elections using alternative voting systems like Instant Runoff Voting (IRV), alongside the monotonicity paradox (Clelland, 2023).

The prevalence of these paradoxes varies among different Condorcet extensions. Empirical studies using Monte Carlo simulations by Courtin et al. (2014) found that while all Condorcet extensions exhibit the reinforcement paradox, maximin suffers from it less frequently than other methods (Brandt et al., 2024). Further theoretical work by Heilmaier (2020) demonstrated that as the number of voters approaches infinity, maximin experiences the reinforcement paradox in only 0.37% of profile pairs where winners coincide—a lower rate than for Black's rule and plurality with runoff (Brandt et al., 2024).

Nurmi suggests a broader classification of voting paradoxes into three main categories: Condorcet incompatibility (where a Condorcet winner exists but is not elected), monotonicity-related paradoxes (including variable-electorate paradoxes), and subset choice paradoxes (Nurmi, 2020). This classification helps situate variable-electorate paradoxes within the broader landscape of voting theory anomalies, which can be formalized using frameworks such as binary aggregation with integrity constraints (Grandi, 2014).

A particularly significant insight from this body of research is that no voting rule is immune to all paradoxes (Nurmi, 2020). This finding echoes the fundamental tensions identified in the Voting Paradox itself, which reveals how majority preferences can conflict with each other in ways that question fundamental assumptions of democratic systems (Zhang, 2020)(Yang, 2009).

The reinforcement paradox, introduced by Young and Levenglick (1978), represents a significant challenge for Condorcet extensions. This paradox occurs when two separate electorates that would elect the same candidate produce a different winner when combined (Brandt et al., 2024). The paradox demonstrates a fundamental tension between the Condorcet principle and the desire for consistency across merged electorates.

Empirical studies have provided insights into the frequency of the reinforcement paradox across different voting rules. Using Monte Carlo simulations, Courtin et al. (2014) found that while all Condorcet extensions exhibit the reinforcement paradox, it occurs less frequently with maximin than with other methods (Brandt et al., 2024). This finding has been further strengthened by theoretical work from Heilmaier (2020), who proved that as the number of voters approaches infinity, maximin experiences the reinforcement paradox in only 0.37% of profile pairs where winners coincide—a notably lower rate than for other methods like Black's rule and plurality with runoff (Brandt et al., 2024).

Research methodologies in studying the reinforcement paradox have varied significantly. McCune et al. highlight an important distinction in approaches: while some studies examine two separate elections with the same winner and check if combining them produces a different winner, others analyze a single election to determine if it can be partitioned into sub-elections that share a common winner different from the original election's outcome (McCune et al., 2025).

The reinforcement paradox belongs to a broader category of monotonicity-related paradoxes, specifically those focused on augmenting or diminishing the electorate (Nurmi, 2020). Nurmi emphasizes that these paradoxes are rule-specific, with different voting methods vulnerable to different paradoxical situations. Critically, no voting rule has been found to be immune to all paradoxes (Nurmi, 2020). The standard method for demonstrating a rule's vulnerability to a particular paradox is to present a specific preference profile where applying the rule leads to a paradoxical outcome (Nurmi, 2020).

Beyond the reinforcement paradox, Condorcet extensions have also been extensively studied regarding their susceptibility to other variable-electorate paradoxes, such as the no-show paradox (Brandt et al., 2024). The literature on Condorcet-consistent methods includes significant work examining their vulnerability to various types of no-show paradoxes (McCune et al., 2025)(Duddy, 2014).

The no-show paradox represents a profound challenge for voting systems, particularly Condorcet extensions. This paradox occurs when "the addition of a ballot that ranks candidate x above candidate y may take victory away from x and give it to y," creating situations where voters might achieve better outcomes by abstaining rather than voting sincerely (Duddy, 2013). This counterintuitive phenomenon directly challenges basic democratic assumptions that participation should never harm a voter's interests.

Moulin's seminal work established that the conflict between the Condorcet principle and participation is unavoidable in many contexts. Specifically, Moulin proved that "a voting rule cannot satisfy both Condorcet's principle and the participation principle when there are four or more candidates" (Duddy, 2013). This impossibility result demonstrates a fundamental tension between these two desirable properties, forcing designers of voting systems to make difficult tradeoffs.

Further research has identified more nuanced variations of the no-show paradox. Duddy examined two specific manifestations: one where "the casting of a ballot that ranks a candidate in first place causes that candidate to lose the election, superseded by a lower-ranked candidate," and another where "a ballot that ranks a candidate in last place causes that candidate to win, superseding a higher-ranked candidate" (Duddy, 2014). This work extended Moulin's findings by proving that when there are at least four candidates and voters may express indifference, every Condorcet-consistent voting rule must exhibit both of these paradoxical behaviors.

The no-show paradox has been observed in real-world electoral contexts beyond theoretical analyses. Some documented cases have emerged in elections using Instant Runoff Voting (IRV), where the paradox appeared alongside other anomalies such as the monotonicity paradox (Clelland, 2023). In these situations, practical examples demonstrate how "voters could have gotten their second-favorite candidate as the winner instead of their least-favorite, if they had abstained from the election" (McCune, 2023).

Methodological approaches to studying the no-show paradox differ significantly across the literature. While some researchers generate theoretical examples to demonstrate vulnerability to the paradox, others analyze real election data to identify instances where the paradox manifested (McCune et al., 2025). This growing body of research has produced "a much broader literature concerning paradoxes and voting methods" that examines "the susceptibility of Condorcet consistent methods to various types of no-show paradox" (McCune et al., 2025)(Duddy, 2014).

Condorcet extensions comprise a diverse family of voting rules that select the Condorcet winner when one exists. Among these, maximin and Copeland's method are especially notable for their consistent adherence to the Condorcet principle (Dey et al., 2015)(Bhattacharyya et al., 2015). Maximin in particular has received substantial attention for its comparative resistance to variable-electorate paradoxes. Research has demonstrated that while all Condorcet extensions exhibit the reinforcement paradox, maximin experiences it less frequently than alternatives, with theoretical work establishing that as voter numbers increase, maximin suffers from this paradox in only 0.37% of relevant profile pairs—notably lower than other methods like Black's rule (Brandt et al., 2024).

The family of Condorcet extensions is remarkably diverse, encompassing numerous well-studied voting rules. These include the Kemény-Young method, Nanson's method, Baldwin's method, ranked pairs, and maximin, among others—many of which can be classified as generalized scoring rules or hyperplane rules (Mossel et al., 2012). Each of these methods represents a different approach to resolving situations where no Condorcet winner exists, while preserving the selection of such a winner when one does exist.

Some Condorcet extensions can be understood through the lens of distance rationalization, which involves measuring the proximity of an election to achieving consensus. Young's rule, for instance, can be interpreted as selecting candidates with the lowest "deletion score"—those requiring the removal of the fewest voters to become Condorcet winners. Similar conceptualizations exist for other metrics, such as "replacement score" and "insertion score," providing a unified framework for understanding various Condorcet extensions (Elkind et al., 2010).

Researchers have also examined Condorcet extensions from a strategic perspective, analyzing how they perform under various game-theoretic models. Some studies have identified conditions under which certain voting rules yield unique outcomes in iteratively undominated strategies, with particular attention to whether these outcomes align with Condorcet winners. Notable findings show that when games using plurality rule are dominance-solvable, a Condorcet winner exists, and under sufficient conditions, this winner becomes the unique outcome (Basteck, 2021)(Courtin et al., 2017).

Empirical analyses comparing different Condorcet extensions have provided valuable insights into their practical performance. Using data generated through spatial modeling—argued to represent real-world preference profiles for three candidates accurately—researchers have found that "the Black rule and the Nanson rule encounter most paradoxes and ties less frequently than the other rules do, especially in elections with few voters" (Brandt et al., 2024). Such findings highlight the importance of considering specific electoral contexts when selecting among Condorcet extensions.

----------------------------------------------------------------------------------

The study of chaotic systems has evolved significantly since the pioneering work of Lorenz, who demonstrated that deterministic systems could exhibit unpredictable, aperiodic behavior (LLM Memory). Within this field, coupled map lattices (CMLs) have emerged as important mathematical frameworks for studying spatiotemporal chaos and pattern formation in extended systems. First systematically studied by Kaneko, these models consist of chaotic maps placed on a lattice with various coupling schemes between neighboring sites (LLM Memory). They have proven invaluable for exploring fundamental questions in statistical physics, particularly regarding how local chaotic dynamics give rise to macroscopic transport properties.

Arnold's cat map, a paradigmatic example of a hyperbolic chaotic system on a torus, has been extensively studied for its strong mixing properties and complete analytical tractability (LLM Memory). This tractability makes it an ideal candidate for constructing coupled systems where exact results can be obtained. While previous research has explored various coupling schemes for chaotic maps, the specific lattice model of coupled cat maps presented in the current work offers a unique contribution due to its analytical solvability, particularly with respect to Lyapunov exponents, which characterize the rate of divergence of nearby trajectories and serve as key quantifiers of chaos strength (LLM Memory).

A distinguishing feature of the present research is its focus on the relationship between microscopic chaos and macroscopic diffusion in a lattice of coupled cat maps with a local perturbation. While other studies have examined diffusion in chaotic systems, the current work provides a rare example where diffusive transport in phase space can be directly connected to the underlying chaotic dynamics with analytical precision (LLM Memory). This bridge between microscopic chaos and macroscopic transport properties contributes to the broader understanding of how deterministic chaos underpins statistical behavior in complex systems.

Arnold's cat map stands as a paradigmatic example of a fully chaotic system, exhibiting uniform chaos across its entire domain with positive Lyapunov exponents for practically all trajectories (Branicki et al., 2009). This uniformity in the chaotic behavior distinguishes cat maps from other dynamical systems, as they lack the spatial heterogeneity typically needed to reveal structural features through techniques like finite-time Lyapunov exponent (FTLE) fields (Branicki et al., 2009). Cat maps belong to a special class of dynamical systems known as Anosov systems, which are structurally stable—meaning that small perturbations to the system preserve the qualitative nature of the dynamics through homeomorphic conjugation (Bullo et al., 2012). This stability property makes cat maps particularly valuable for theoretical investigations of chaos.

In the broader context of dynamical systems theory, chaotic behavior exists on a spectrum, with cat maps representing "strong chaos" characterized by positive Lyapunov exponents, exponential sensitivity to initial conditions, ergodicity, and exponential relaxation to equilibrium states (Pessoa et al., 2011). These properties make strongly chaotic systems especially relevant for traditional statistical mechanics. Between ordered systems (with negative Lyapunov exponents) and strongly chaotic ones, there exists an intermediate regime of "weak chaos," where systems exhibit zero maximal Lyapunov exponents and power-law sensitivity to initial conditions rather than exponential sensitivity (Pessoa et al., 2011)(Ananos et al., 2004). Such weakly chaotic systems often display broken ergodicity, non-exponential relaxation, and non-Gaussian distributions, making them candidates for description via nonextensive statistical mechanics (Pessoa et al., 2011). The research on coupled cat maps occupies an important position within this theoretical landscape, as it leverages the strong chaotic properties of individual cat maps while introducing spatial coupling that can potentially reveal new phenomena at the interface between microscopic chaos and macroscopic transport.

The concept of Lyapunov exponents is central to characterizing chaos in dynamical systems, providing a quantitative measure of the rate at which nearby trajectories diverge or converge over time. A positive Lyapunov exponent indicates sensitive dependence on initial conditions—the hallmark of chaos—while negative exponents suggest stability and convergence (Shukla et al., 2024)(Lorenz, 1963). Systems with zero Lyapunov exponents, such as limit-cycle oscillations, exhibit initial-condition dependence without being chaotic, highlighting that sensitivity to initial conditions is necessary but not sufficient for identifying chaos (Taniguchi, 2022). This distinction is crucial for properly classifying the behavior observed in coupled cat maps.

The dimensionality of chaos, determined by the number of positive Lyapunov exponents, provides important insights into system complexity. Low-dimensional chaos, characterized by a single positive Lyapunov exponent (as in the classic Lorenz and Rössler attractors), differs fundamentally from high-dimensional chaos, which features multiple positive exponents (Du et al., 2018). This classification is particularly relevant to our study of coupled cat maps, where the lattice structure potentially enables high-dimensional chaotic behavior. The relationship between Lyapunov exponents and global stable and unstable manifolds further establishes an equivalence between spectral properties and the topological structures that organize state space dynamics (Albers et al., 2005).

The theoretical significance of Lyapunov exponents extends beyond mere classification, as they connect different mathematical definitions of chaos through important theorems. Brudno's theorem relates algorithmic complexity to Lyapunov exponents, while Pesin's theorem connects the classical ergodic hierarchy to these exponents, creating a unifying framework for understanding chaotic phenomena (Gomez et al., 2017). These connections are especially relevant for our study of coupled cat maps, where the exact determination of Lyapunov exponents allows for precise characterization of the system's ergodic properties.

In spatially extended systems like coupled map lattices, the scaling behavior of Lyapunov exponents with system size reveals important distinctions between different types of chaos. Intensive chaos, where the number of positive Lyapunov exponents remains independent of system size, contrasts with extensive chaos, where this number scales linearly with the number of oscillatory units (Hohlein et al., 2019). Many realistic systems, particularly those in two or three spatial dimensions with global coupling schemes, exhibit mixed characteristics with both collective macroscopic modes and microscopic chaotic degrees of freedom (Hohlein et al., 2019). The coupled cat map lattice in our research provides an excellent framework for studying these scaling behaviors analytically.

The transition to chaos, characterized by changes in Lyapunov exponents, has been extensively studied in deterministic systems through various routes including period-doubling, intermittency, crisis, and quasiperiodicity (Du et al., 2018). However, the corresponding transitions in random dynamical systems are less understood (Goverse et al., 2025). Research on the transition from synchronization to chaos in such systems has revealed interesting phenomena at the critical point where the Lyapunov exponent crosses zero, including the existence of infinite ergodic invariant measures and intermittent dynamics (Goverse et al., 2025). Our study of coupled cat maps with local perturbations contributes to this body of knowledge by providing an analytically tractable model where the relationship between microscopic chaos (quantified through Lyapunov exponents) and macroscopic diffusion can be precisely established.

The connection between microscopic chaos and macroscopic transport phenomena like diffusion represents one of the central questions in modern statistical mechanics. Researchers have extensively investigated whether deterministic chaos at the microscopic level is necessary for the emergence of macroscopic diffusion, characterized by mean square displacement growing proportionally with time. Dettmann et al. conducted pioneering work using models of single particles moving in two dimensions and colliding with fixed scatterers, demonstrating that several microscopically non-chaotic systems can still exhibit diffusive behavior (Dettmann et al., 2000). This finding challenges the intuitive assumption that chaos is a prerequisite for diffusion and suggests that the relationship between microscopic dynamics and macroscopic transport is more nuanced than previously thought.

The theoretical foundation for connecting dynamical systems theory with non-equilibrium statistical mechanics was significantly advanced by the work of Gallavotti and Cohen, who conjectured that many-particle systems studied in statistical mechanics are generally strongly chaotic (Wijn, 2006)(Gallavotti et al., 1995). Their insights sparked considerable interest in establishing connections between chaos and transport coefficients (Wijn, 2006)(Gaspard et al., 2000). For hyperbolic systems exhibiting strong chaos, research has revealed close relationships between transport coefficients (including diffusivity) and chaos indicators such as Lyapunov exponents and Kolmogorov-Sinai entropy (Cencini et al., 2008). These relationships seemingly support the view that chaos is fundamental to statistical mechanics.

However, numerous counterexamples have demonstrated that chaos is not a necessary condition for robust statistical behaviors. Studies have shown that phenomena such as diffusion and heat conduction can occur in non-chaotic systems as well (Cencini et al., 2008). This evidence suggests that microscopic chaos is not the only possible mechanism underlying macroscopic transport in dynamical systems, prompting researchers to consider alternative frameworks for understanding these phenomena.

Recent numerical analyses have further illuminated the relationship between chaos and diffusion by examining the probability of chaos in strongly nonlinear Hamiltonian systems. Mulansky found different scaling properties depending on the nonlinear structure of the model, arguing that these scaling laws of chaos have definite consequences for macroscopic diffusive behavior, as chaos serves as the microscopic mechanism of diffusion (Mulansky, 2013). This research establishes a relationship between microscopic chaos and macroscopic diffusion that helps bridge the gap between dynamical systems theory and statistical physics.

Our study of a lattice of coupled cat maps with local perturbation contributes significantly to this discourse by providing an analytically tractable model where the connection between microscopic chaos (characterized by exactly determined Lyapunov exponents) and diffusive transport in phase space can be precisely established. While previous work has highlighted both correlations and counterexamples between chaotic dynamics and diffusion, our model offers a rare example where the relationship between these phenomena can be analyzed with mathematical rigor, potentially advancing our understanding of how deterministic chaos underpins emergent statistical behaviors in complex systems.

The dynamics of coupled chaotic systems extend beyond the behavior of individual chaotic elements to reveal rich spatiotemporal patterns and collective phenomena. In systems composed of many coupled oscillatory units, chaos can emerge from either microscopic interactions involving only a few oscillators or from complex patterns of interaction across macroscopic scales, leading to significantly different dynamical characteristics (Hohlein et al., 2019). Traditional categorizations of chaos in these systems have focused on two limiting cases: intensive chaos, where the number of positive Lyapunov exponents remains independent of system size, and extensive chaos, where this number scales linearly with the number of oscillatory units (Hohlein et al., 2019). However, recent research has demonstrated that many realistic systems, particularly those with global coupling schemes or multiple spatial dimensions, exhibit dynamics that cannot be classified within this binary framework. Instead, these systems display a hybrid form characterized by both collective macroscopic modes and numerous microscopic chaotic degrees of freedom (Hohlein et al., 2019).

A distinctive feature of spatially distributed chaotic systems is the dual nature of information flow, which occurs simultaneously in both state space and physical space. While the flow in state space is typically quantified through the maximal Lyapunov exponent (λ), measuring the growth rate of infinitesimal disturbances, the spatial propagation of information can be characterized by the maximal velocity of disturbance propagation (Vp) (Cencini et al., 2000). This dual characterization becomes particularly important when considering finite perturbations, which are governed by the complete nonlinear dynamics rather than being confined to the tangent space. Such finite disturbances can lead to intriguing phenomena like stable chaos, where even linearly stable systems (with negative Lyapunov exponents) can exhibit erratic behavior with positive propagation velocities (Cencini et al., 2000). This indicates that the Lyapunov spectrum alone may not fully capture the complex behaviors emerging in high-dimensional systems.

One of the most remarkable aspects of extensively chaotic dynamical systems is their capacity to exhibit non-trivial collective behavior (NTCB), characterized by long-range order emerging from local chaos (Lemaitre et al., 1999). This phenomenon, wherein coherent structures emerge despite underlying chaotic dynamics, is particularly evident in coupled map lattices (CMLs) where chaotic nonlinear maps are diffusively coupled with specific coupling strengths (Lemaitre et al., 1999). Such systems serve as simplified models for reaction-diffusion processes, demonstrating how microscopic chaos can coexist with macroscopic order. The lattice of coupled cat maps in our current research contributes to this theoretical framework by providing an analytically tractable model where the interplay between local chaotic dynamics and global transport properties can be precisely characterized. Unlike many other studies that rely primarily on numerical approaches, our work allows for exact determination of important dynamical quantities like Lyapunov exponents, offering deeper insights into how microscopic chaos translates to macroscopic diffusion in spatially extended systems.

The connection between microscopic dynamics and macroscopic transport properties forms a cornerstone of statistical mechanics, with significant implications for our understanding of coupled chaotic systems. In hyperbolic systems characterized by strong chaos, research has revealed robust relationships between transport coefficients (including diffusivity, viscosity, and thermal conductivity) and dynamical indicators of chaos such as Lyapunov exponents and Kolmogorov-Sinai entropy (Cencini et al., 2008). These connections are particularly significant for fractal structures of diffusive modes, which control both long-time relaxation toward equilibrium and thermodynamic entropy production (Gaspard, 2014). The theoretical foundation for these chaos-transport relationships was substantially advanced by Gallavotti and Cohen, who conjectured that many-particle systems studied in statistical mechanics generally exhibit strong chaotic behavior (Wijn, 2006)(Gallavotti et al., 1994)(Gallavotti et al., 1995). This conjecture sparked considerable research interest in establishing precise connections between chaos indicators and transport properties (Wijn, 2006)(Gaspard et al., 2000).

The classification of chaotic regimes has important implications for statistical mechanics. Strongly chaotic systems exhibit well-defined characteristics essential for traditional statistical mechanics approaches, including exponential sensitivity to initial conditions, ergodicity, exponential relaxation to equilibrium, and Gaussian distributions (Pessoa et al., 2011). In contrast, weakly chaotic systems characterized by zero maximal Lyapunov exponents and power-law sensitivity to initial conditions often display broken ergodicity, non-exponential relaxation, and non-Gaussian distributions, making them candidates for description via nonextensive statistical mechanics (Pessoa et al., 2011). This distinction highlights how the nature of microscopic dynamics fundamentally shapes macroscopic statistical behavior.

Phase transitions provide particularly illuminating examples of the relationship between microscopic dynamics and macroscopic phenomena. Molecular dynamics simulations have demonstrated that Lyapunov spectra can effectively characterize phase transitions in various systems, from simple models like the two-dimensional system of coupled rotators to more complex many-body interactions (Cockrell, 2020)(Butera et al., 1987). In second-order phase transitions, the first derivative of the maximal Lyapunov exponent with respect to temperature often shows a peak at the critical temperature (Cockrell, 2020)(Kwon et al., 1997), while first-order transitions frequently manifest as discontinuities in the maximal Lyapunov exponent itself (Cockrell, 2020). These observations extend beyond equilibrium phase transitions to nonequilibrium processes like glass transitions, where changes in chaotic behavior measured by the largest Lyapunov exponent provide microscopic insights into macroscopic phase changes (Cockrell, 2020)(Nayak et al., 1998).

The study of coupled cat maps contributes significantly to this theoretical framework by providing an analytically tractable model where the relationship between microscopic chaos and macroscopic diffusion can be precisely established. While previous work has often relied heavily on numerical simulations to explore these connections, our model offers exact determinations of Lyapunov exponents and their relationship to diffusive transport. This analytical approach enhances our understanding of how the microscopic properties of chaos translate into macroscopic transport phenomena, potentially bridging conceptual gaps between dynamical systems theory and statistical mechanics.

----------------------------------------------------------------------------------
Data assimilation (DA) refers to a set of statistical methods designed to estimate the state of a temporally-evolving system by combining observational data with model predictions (Chen et al., 2021). This approach is particularly crucial for systems where accurate initial conditions significantly impact forecast quality, especially in chaotic systems where small errors in initial conditions can lead to drastically different predictions (Chattopadhyay et al., 2022)(Morzfeld et al., 2017). DA has become indispensable across numerous fields including weather prediction, environmental and geophysical flows, combustion systems, aeronautics, hydrology, acoustics, and fluid mechanics (Chattopadhyay et al., 2022)(Liu et al., 2007).

The integration of machine learning with data assimilation has emerged as a powerful approach to address the limitations of traditional DA methods. While machine learning algorithms aim to provide forecasts for various phenomena, their effectiveness is often challenged by the chaotic nature of real systems (Luca et al., 2020). From a Bayesian perspective, both DA and machine learning can be unified under probabilistic frameworks, with recent research highlighting their algorithmic similarities and complementary strengths (Filoche et al., 2022)(Abarbanel et al., 2017). This unification creates opportunities for hybrid approaches that leverage DA's ability to incorporate physical constraints and handle sparse, noisy observations while utilizing machine learning's capability to extract complex relationships from data (Bocquet et al., 2020)(Geer et al., 2021).

Data assimilation methods broadly fall into two main categories: ensemble-based methods (such as the Ensemble Kalman Filter) and variational methods (like 3D-Var and 4D-Var) (Chattopadhyay et al., 2022)(Bannister, 2017). Ensemble methods track the evolution of multiple system states to estimate uncertainty, while variational approaches optimize cost functions to find the most probable state given observations and model constraints (Dimet et al., 1986). Recent advances have developed methods that combine these approaches to leverage their respective strengths (Bannister, 2017)(Farchi et al., 2020).

The increasing complexity of environmental models and the high-dimensional nature of many geophysical systems present significant challenges for traditional DA approaches (Carrassi et al., 2017). Innovative approaches including iterative ensemble methods (Gu et al., 2007), physically-constrained ensemble techniques (Gleiter et al., 2022), and feature-based assimilation (Morzfeld et al., 2017) have been developed to address these challenges. These advancements, coupled with the integration of machine learning techniques, are expanding the applicability of DA to increasingly complex systems with unknown dynamics (Nguyen et al., 2020), opening new avenues for improving model accuracy and prediction capabilities in domains where safety-critical applications are paramount (Brunton et al., 2020).

Data assimilation algorithms can be broadly categorized into two main families: ensemble-based methods and variational methods (Chattopadhyay et al., 2022)(Carrassi et al., 2017). Ensemble-based approaches, such as the Ensemble Kalman Filter (EnKF), estimate system states by tracking the evolution of multiple realizations of the system to capture uncertainty (Chattopadhyay et al., 2022)(Evensen, 1994). These methods are particularly advantageous as they do not require the derivation of model adjoints, but they typically operate under Gaussian approximations of error statistics (Chattopadhyay et al., 2022). Variational methods, including 3D-Var and 4D-Var, formulate data assimilation as an optimization problem where a cost function representing the mismatch between model outputs and observations is minimized (Chattopadhyay et al., 2022)(Dimet et al., 1986). However, 4D-Var requires obtaining the adjoint of the system's dynamical model, which can be computationally complex and challenging to implement (Chattopadhyay et al., 2022).

In recent years, there has been significant progress in developing hybrid methods that combine the strengths of both ensemble and variational approaches (Bannister, 2017). These hybrid ensemble-variational (EnVar) methods aim to leverage the flow-dependent background error covariances from ensemble methods while maintaining the optimization framework of variational approaches (Bannister, 2017). For systems with highly nonlinear dynamics, such as chaotic systems, specialized techniques are being developed to address the limitations of traditional methods. For instance, the quadratic programming ensemble (QPEns) algorithm extends the standard EnKF with constraints on ensemble member updates to preserve physical properties like energy conservation (Gleiter et al., 2022).

One of the persistent challenges in data assimilation is handling measurement uncertainty and noise in the observations. Classical approaches typically require prior knowledge of measurement noise statistics (Antil et al., 2021). Nudging, a simpler data assimilation method, has become increasingly popular when integrated with machine learning due to its computational efficiency (Antil et al., 2021). Additionally, iterative ensemble methods have been developed to better handle nonlinearities in multiphase flow and other complex systems (Gu et al., 2007).

An emerging approach in data assimilation involves feature-based techniques, where likelihoods are defined based on selected features of the data rather than the direct mismatch between model outputs and observations (Morzfeld et al., 2017). This approach is particularly valuable when dealing with redundant data, chaotic systems, or when assimilating data into lower-dimensional models (Morzfeld et al., 2017). The parameterized-background data-weak (PBDW) formulation offers another innovative framework that incorporates experimentally observable spaces and provides both a priori and a posteriori error estimates (Maday et al., 2015).

The growing integration of data assimilation with machine learning techniques presents both opportunities and challenges. These methods share many algorithmic similarities and can be unified under a Bayesian framework (Filoche et al., 2022)(Geer et al., 2021). Hybrid approaches leverage data assimilation's ability to incorporate physics-based knowledge with machine learning's capacity to extract complex relationships from data (Filoche et al., 2022)(Abarbanel et al., 2017). These combined methods have been applied to correct model errors, jointly estimate parameters and system states, and accelerate the assimilation process (Filoche et al., 2022)(Farchi et al., 2020)(Bocquet et al., 2020).

The effectiveness of various data assimilation methods depends significantly on the degree of nonlinearity in the system. For systems with medium nonlinearity, studies suggest that smoothers and variational methods tend to outperform EnKF, and smoothers can achieve accuracy comparable to particle filters but with smaller ensemble sizes (Morzfeld et al., 2019). However, systems with strong nonlinearity present substantial challenges for Gaussian approximations, particularly when posterior distributions become multi-modal (Morzfeld et al., 2019). These considerations are especially important in safety-critical applications where reliable uncertainty quantification is essential (Brunton et al., 2020).

The integration of machine learning with data assimilation represents a growing area of research that leverages the complementary strengths of both methodologies. While data assimilation provides a structured Bayesian framework to combine sparse, noisy observations with physics-based knowledge, machine learning excels at extracting complex relationships from large datasets (Filoche et al., 2022)(Bocquet et al., 2020)(Geer et al., 2021). This synergy is particularly valuable for modeling chaotic systems, where traditional forecasting methods often struggle due to the systems' sensitivity to initial conditions.

Hybrid approaches combining these techniques have been developed for various purposes. Some methods focus on correcting model errors by using machine learning to identify and address systematic biases in physics-based models (Filoche et al., 2022)(Farchi et al., 2020). Others aim to jointly estimate parameters and system states, effectively learning the underlying dynamics while assimilating observational data (Filoche et al., 2022)(Nguyen et al., 2020). Additionally, some approaches leverage machine learning to accelerate the computationally intensive assimilation process (Filoche et al., 2022).

The nudging data assimilation algorithm has gained popularity when combined with machine learning due to its computational efficiency and forecasting capabilities (Antil et al., 2021). Neural networks, in particular, have proven effective in constructing prediction models that can replace or supplement traditional physics-based models. Yanan et al. demonstrated this approach using the Lorenz-96 system, where neural networks generated predictions that were subsequently refined through data assimilation, with the updated states then feeding back into the neural network for the next forecast cycle (Yanan et al., 2020).

From a theoretical perspective, machine learning and variational data assimilation share many algorithmic aspects (Filoche et al., 2022)(Dimet et al., 1986). Abarbanel et al. established a strong equivalence between these methods, noting that layer number in artificial networks corresponds to time in data assimilation frameworks. This insight suggests that adding more layers (making networks deeper) is analogous to increasing temporal resolution in data assimilation (Filoche et al., 2022)(Abarbanel et al., 2017).

For chaotic systems specifically, learning the governing equations from real-life data remains challenging due to noise and incomplete observations. Nguyen et al. proposed a framework combining auto-encoders and the Ensemble Kalman Smoother to address these issues by treating learning as a Bayesian estimation problem (Nguyen et al., 2020). This approach demonstrated effectiveness even with noisy and partial observations of the chaotic Lorenz-63 system.

The unification of data assimilation and machine learning under a Bayesian framework offers promising avenues for improvement in both fields (Geer et al., 2021). Bayesian networks provide a graphical representation that can organize modeling components from both physics-based equations and data-driven learning, establishing equivalences between techniques such as four-dimensional variational (4D-Var) data assimilation and recurrent neural networks (RNNs). While the full Bayesian solution may not be computationally feasible, approximate methods from both disciplines could enable practical implementation of this unified framework (Geer et al., 2021).

As environmental models continue to increase in complexity, the interdisciplinary nature of data assimilation across statistics, dynamical systems, and numerical optimization becomes increasingly valuable (Carrassi et al., 2017). The integration with machine learning extends this interdisciplinary approach, creating new possibilities for improved state estimation in high-dimensional chaotic systems where traditional methods alone may be insufficient.

Chaotic dynamical systems, first introduced by Edward Lorenz in 1963, are characterized by their extreme sensitivity to initial conditions, where slight differences in starting states can lead to dramatically divergent trajectories over time (Schenk et al., 2022)(Lorenz, 1963). This fundamental property of chaos presents significant challenges for prediction and state estimation, making chaotic systems ideal candidates for developing and testing data assimilation methodologies in well-understood and controllable environments (Schenk et al., 2022). Since their introduction, the Lorenz models developed in 1963, 1984, and 1996 have become standard test cases in atmospheric and oceanic sciences, serving as simplified representations that capture essential turbulent and chaotic behaviors without the computational complexity of full physical models (Chen et al., 2018).

The inherent unpredictability of chaotic systems necessitates innovative approaches for forecasting and state estimation. Hybrid methods combining machine learning with data assimilation have shown promising results in addressing these challenges. For instance, Yanan et al. demonstrated the effectiveness of neural networks coupled with data assimilation for predicting the states of the Lorenz-96 system, a higher-dimensional chaotic model commonly used in meteorological studies (Yanan et al., 2020). Their approach utilized neural networks to construct prediction models that replace traditional physics-based models, with data assimilation providing refined inputs for subsequent neural network forecasts. This iterative process produced predictions that closely matched true values over meaningful time scales.

A critical aspect of modeling chaotic systems is their robustness to observational noise, which poses additional challenges beyond the inherent sensitivity to initial conditions. Jiahao et al. investigated this issue using the Lorenz system with Gaussian noise added to observations (Jiahao et al., 2020). Their results demonstrated that while trained models could only accurately predict about one Lyapunov time (a measure of predictability in chaotic systems) before trajectories diverged due to noise, the predicted states still exhibited behavior consistent with the true system dynamics. This finding highlights both the fundamental limitations imposed by chaos—where even small perturbations cause exponential divergence—and the potential for machine learning and data assimilation techniques to capture underlying system behaviors despite these constraints.

The Lorenz models' mathematical simplicity, coupled with their complex dynamics, makes them particularly valuable for developing and benchmarking new data assimilation algorithms. Their well-understood properties allow researchers to evaluate the performance of novel techniques in a controlled setting before applying them to more complex real-world systems. As data assimilation methods continue to evolve, particularly through integration with machine learning approaches, these chaotic models remain essential tools for advancing our understanding of state estimation in nonlinear dynamical systems.

A fundamental challenge in data assimilation is effectively handling measurement uncertainty and noise while maintaining accurate state estimation. Classical data assimilation algorithms, including ensemble-based methods like the Ensemble Kalman Filter and variational approaches such as 3D-Var and 4D-Var, typically assume Gaussian observation noise with known statistical properties (Chattopadhyay et al., 2022)(Evensen, 1994). This requirement presents a significant limitation in practical applications where the true noise characteristics are unknown or cannot be accurately estimated.

The presence of measurement noise is particularly problematic for chaotic systems due to their extreme sensitivity to initial conditions. As demonstrated by Jiahao et al. in their experiments with the Lorenz system, even small amounts of Gaussian noise (with variance as low as 0.01) can cause predicted trajectories to diverge from true values after approximately one Lyapunov time (Jiahao et al., 2020). While this divergence is an inherent characteristic of chaotic systems where small perturbations grow exponentially, their study showed that despite this limitation, the predicted states still exhibited behavior consistent with the underlying system dynamics.

To address the challenges posed by unknown measurement uncertainty, several innovative approaches have emerged. Feature-based data assimilation techniques define likelihoods based on selected features of the data rather than the direct mismatch between model outputs and observations (Morzfeld et al., 2017). This approach is particularly valuable when dealing with redundant data, chaotic systems, or when assimilating data into lower-dimensional models where matching all data components may be impossible or unnecessary.

Another promising development is the integration of physical constraints into the data assimilation process. The quadratic programming ensemble (QPEns) algorithm extends standard ensemble methods by incorporating constraints on ensemble member updates to preserve important physical properties (Gleiter et al., 2022). Such physically-constrained approaches can improve both filtering accuracy and forecast skill by maintaining consistency with the underlying physical system, even in the presence of measurement noise.

For multiphase flow and other highly nonlinear systems, iterative ensemble methods have been developed to better handle the nonlinearities that exacerbate the effects of measurement uncertainty (Gu et al., 2007). These methods address the limitations of standard ensemble filters, which can sometimes fail during the analysis and updating of state variables in strongly nonlinear systems despite handling nonlinear dynamics correctly during the forecast step.

The parameterized-background data-weak (PBDW) formulation offers another framework for addressing measurement uncertainty by incorporating experimentally observable spaces and providing both a priori and a posteriori error estimates (Maday et al., 2015). This approach enables more robust state estimation in practical settings where measurement errors must be accounted for without explicit knowledge of their statistical properties.

Recent advances in hybrid techniques combining variational and ensemble methods (EnVar) aim to leverage the strengths of both approaches while mitigating their respective weaknesses in handling measurement uncertainty (Bannister, 2017). These methods often perform better than pure ensemble or variational approaches in problems with medium nonlinearity (Morzfeld et al., 2019). However, in strongly nonlinear problems where posterior distributions become multi-modal, even these sophisticated approaches may struggle to provide accurate state estimates through Gaussian approximations.

Beyond traditional data assimilation approaches, the integration with machine learning techniques offers new possibilities for handling measurement uncertainty. These combined methods can identify model-form uncertainty alongside measurement uncertainty, providing valuable information for subsequent Bayesian data assimilation to improve predictability in chaotic systems (Sun et al., 2022). Such approaches are particularly important in safety-critical applications where reliable uncertainty quantification is essential (Brunton et al., 2020)(Liu et al., 2007).
----------------------------------------------------------------------------------

Age of Information (AoI) is a relatively recent metric designed to measure the "freshness" of information in systems where timely updates are critical. Formally defined, AoI at time t is δ(t) := t − u(t), where u(t) represents the generation time of the most recent update available at the destination (Feng et al., 2019). This metric differs fundamentally from traditional delay metrics as it focuses on the destination's perspective rather than on individual packet delivery times (Talak et al., 2019)(Talak et al., 2018).

The concept of AoI was introduced to address the growing need for timely information delivery in applications such as environmental sensing and real-time monitoring (Kaul et al., 2012). In these contexts, the freshness of data at the receiver is crucial for accurate decision-making (Wu et al., 2022). AoI captures this requirement by increasing linearly with time until a fresher update arrives at the receiver, at which point it drops to reflect the new information's age (Ayan et al., 2019).

The AoI metric is jointly determined by two factors: the transmission interval between consecutive updates and the transmission delay of each update (Wang et al., 2020). This dual dependency creates interesting optimization challenges, as minimizing one factor might lead to an increase in the other. For example, sending updates too frequently could cause congestion and increase transmission delays, while sending them too infrequently would allow the information to become stale (Kaul et al., 2012).

As research in this area has evolved, AoI has been extended to various network configurations, including single-server and multi-server systems, multi-hop networks, and multiple-access channels (Feng et al., 2019). Additionally, researchers have introduced variations of the basic AoI metric, such as peak age, which characterizes the maximum staleness of information just before receiving a new update (Costa et al., 2015).

The analysis of Age of Information (AoI) in queueing systems forms the foundation for understanding information freshness in various network configurations. Early AoI research focused on single-server queueing models, with Kaul et al.'s seminal work analyzing first-come-first-served (FCFS) M/M/1, M/D/1, and D/M/1 queues (Kaul et al., 2012). Their work revealed a crucial insight—there exists an optimal rate at which a source should generate updates to minimize age, which differs from rates that would maximize throughput or minimize delay.

As research progressed, more complex queuing disciplines were examined. The Last-Come-First-Served (LCFS) policy emerged as particularly age-efficient, with several studies showing its potential advantages over FCFS (Kaul et al._1, 2012). Notably, Costa et al. demonstrated that LCFS with preemption in service could significantly reduce age compared to traditional queuing systems (Costa et al., 2015). This finding was further reinforced by multiple subsequent studies establishing that preemptive LCFS policies are often age-optimal when service times follow an exponential distribution (Tripathi et al., 2019).

The research community has progressively expanded analysis to include multi-server systems. Kam et al. investigated M/M/2 and M/M/∞ queues to demonstrate the advantages of server diversity (Kam et al., 2016)(Kam et al., 2014). These studies addressed a key consideration in dynamic networks—when packets travel through different routes, they may arrive out of order, requiring special treatment in AoI computations.

For analytical tractability, researchers have developed several methodological approaches. The stochastic hybrid systems (SHS) technique, introduced by Yates et al., has emerged as a powerful tool for AoI analysis (Yates et al., 2016). This approach has been widely adopted and extended to various queueing models, including multi-source systems with different priorities (Kaul et al., 2018), systems with packet delivery errors (Farazi et al., 2019), and parallel server configurations (Yates, 2018).

More recent work has explored complex multi-source systems with various packet management policies. Moltafet et al. analyzed source-aware packet management policies, where updates from the same source may replace older updates in the queue (Moltafet et al., 2020)(Moltafet et al._1, 2020). These policies aim to improve both age performance and fairness among sources.

Several researchers have also explored systems with parallel queues and routing strategies. Doncel et al. analyzed a system of parallel networks modeled as queues with probabilistic routing (Doncel et al., 2020), while Yates examined preemptive LCFS queues with updates following independent routes (Yates, 2018). These studies highlight the benefits of route diversity in reducing AoI.

The AoI research landscape has recently expanded to include specialized scenarios such as queues with packet deadlines (Inoue, 2018), energy harvesting constraints (Bacinoglu et al., 2017), and non-linear age penalty functions (Zheng et al., 2019). Additionally, the community has explored age-optimal scheduling policies for multi-source systems, including maximum-age-first (MAF) and other priority-based approaches (Kadota et al., 2018).

The optimization of Age of Information involves multiple approaches addressing different aspects of this complex problem. A key research direction is the development of age-aware scheduling policies. Kadota et al. demonstrated that a Greedy Policy, which transmits packets for clients with the highest current age, is optimal for symmetric networks (Kadota et al., 2016). For more general network configurations, researchers have developed various low-complexity scheduling policies including randomized policies, Max-Weight policies, and Whittle's Index policies, with performance guarantees for each (Kadota et al., 2018).

The computational complexity of AoI optimization poses significant challenges, as He et al. proved that scheduling problems with age minimization goals are NP-hard in multi-user networks (He et al., 2018). This has motivated researchers to explore approximate solutions. Talak et al. proposed virtual queue-based and age-based policies, with the virtual queue-based policy shown to be peak-age optimal and the age-based policy proven to be within a factor of four of optimal values (Talak et al._1, 2018).

More recently, researchers have begun investigating machine learning approaches to address AoI optimization. Beytur et al. applied reinforcement learning methods to achieve scheduling decisions that are resilient to varying network conditions (Beytur et al., 2019). Similarly, Ceran et al. developed online learning algorithms that do not require a priori information about channel states (Ceran et al., 2021). These learning-based approaches demonstrate the ability to find near-optimal policies in complex environments where analytical solutions are intractable.

The optimization of AoI is increasingly considered in conjunction with other system objectives, creating multi-objective optimization problems. Liu et al. formulated sensors' time-averaged transmit power minimization subject to constraints on maximal AoI's tail behavior using Lyapunov stochastic optimization (Liu et al., 2019). Their numerical results revealed several important tradeoffs, including the relationship between transmission rate requirements, blocklength allocation, and energy consumption.

Researchers have also explored the balance between AoI and throughput requirements. Kadota et al. addressed the problem of minimizing expected weighted sum AoI while simultaneously satisfying timely-throughput constraints (Kadota et al., 2019). Similarly, Sun et al. proposed an age-aware policy for a base station supporting both status update traffic and timely throughput traffic (Sun et al., 2021). These approaches demonstrate how AoI optimization can be integrated with traditional performance metrics.

The inherent trade-offs between AoI and other system metrics have been extensively studied. Talak et al. proved a strong age-delay tradeoff, wherein as average AoI approaches its minimum, packet delay and its variance approach infinity (Talak et al._1, 2019). Devassy et al. investigated the probability that delay and peak-age exceed desired thresholds in point-to-point communication systems with short information packets (Devassy et al., 2018).

More sophisticated analysis methods, such as examining joint AoI processes, allow researchers to incorporate second-order statistics (like covariance matrices) in system optimization (Abd-Elmagid et al., 2022). Expanding beyond traditional average AoI metrics, researchers have introduced general functions of AoI that characterize how dissatisfaction depends on data staleness (Kosta et al., 2018)(Sun et al., 2019).

In multi-user mobile edge computing (MEC) systems, the challenge of "how many computational tasks need to be offloaded such that AoI performance is optimized" represents an important area for future research (Tang et al., 2022). This highlights the growing complexity of AoI optimization problems as they extend to heterogeneous networks with varied computational requirements and constraints.

The optimization of Age of Information (AoI) inherently involves managing the trade-off between information freshness and energy consumption, particularly in resource-constrained systems. Modern computing systems employ two primary power management mechanisms: performance scaling and component deactivation (Liu et al., 2014). Performance scaling techniques such as Dynamic Voltage and Frequency Scaling (DVFS) reduce power consumption by adjusting processing speed and voltage in response to system utilization, while component deactivation puts processors into low-power sleep states during idle periods (Liu et al., 2014).

The energy-AoI trade-off has received significant attention in wireless sensor networks and IoT systems where devices often operate with limited energy resources. Bacinoglu et al. examined age minimization under energy harvesting constraints, discovering that age-optimal policies often involve deliberately introducing waiting times between updates—a "lazy" approach that contradicts traditional throughput optimization (Bacinoglu et al., 2017). This finding aligns with other research showing that optimal policies for energy-constrained systems frequently maintain a balance between aggressive updating and energy conservation (Gong et al., 2022).

The research community has developed various approaches to address this energy-AoI trade-off. Some studies formulate the problem within the framework of Markov Decision Processes to derive optimal transmission policies, particularly for systems with varying operational states (e.g., normal versus alarm states) (Sathyavageeswaran et al., 2024)(Stamatakis et al., 2019). Other researchers have leveraged deep reinforcement learning, designing reward functions that explicitly balance AoI and energy consumption through trade-off parameters (Gong et al., 2022).

In multi-core computing systems, analytical models have been developed to estimate performance and energy costs for scientific workloads. These models extend traditional performance laws (such as Amdahl's law) to account for power-aware computations in multi-core environments (Lei et al., 2025)(Ge et al., 2009). Researchers have also decomposed workload execution time into components including computation time, memory access time, storage access time, and communication overhead, modeling each as a function of problem size, parallelism, and hardware frequency (Lei et al., 2025)(Song et al., 2013).

A comprehensive approach to energy-efficient AoI systems requires understanding the complex interactions between update generation rates, transmission scheduling, and power management policies. Yates demonstrated that for an energy harvesting source with a large battery, an optimal lazy policy often leaves the service facility idle even when sufficient energy is available for submitting updates (Sathyavageeswaran et al., 2024)(Yates, 2015). This counter-intuitive finding highlights the fundamental difference between AoI optimization and traditional performance metrics like throughput or delay.

Systems requiring multiple sequential processing steps represent an emerging frontier in Age of Information (AoI) research. Recently, Ramani et al. introduced a framework for analyzing update systems where each status update requires multiple computational steps before becoming useful at the destination (Ramani et al., 2024). This work models two key architectural approaches: parallel setups where multiple processors independently execute all computation steps, and series setups where processors are specialized for specific steps in a processing chain.

The multi-step processing model addresses a critical real-world constraint missing from many AoI analyses—namely, that raw updates often cannot be immediately used but instead require substantial processing. This processing requirement creates additional design considerations beyond simple transmission delays. As highlighted by Liyanaarachchi et al., modern applications ranging from autonomous driving to remote surgery and uncrewed lunar landing missions require not only timely communication but also timely processing of the communicated information (Liyanaarachchi et al., 2024)(Simsek et al., 2016)(Yuan et al., 2024).

When analyzing multi-step systems, a fundamental trade-off emerges between AoI performance and power consumption. Processing updates faster generally improves age metrics but increases power consumption (Ramani et al., 2024). This trade-off is particularly relevant in energy-constrained environments such as IoT networks and space applications where power resources are limited (Abd-Elmagid et al., 2018)(Yuan et al., 2024).

An important phenomenon identified in multi-step systems is "wasted power," which occurs when processing efforts do not result in age reduction. In parallel server configurations, this happens when a fresher update finishes processing before older updates that are still being processed. In series configurations, wasted power occurs when a server preempts its current processing due to receiving a fresher update from a preceding server in the chain (Ramani et al., 2024).

The analysis of path diversity in status update systems provides valuable insights for multi-step processing models. Doncel et al. explored systems where updates can be routed through parallel networks modeled as queues, developing analytical methods using stochastic hybrid systems (SHS) (Doncel et al., 2020)(Yates et al., 2016). Similarly, Yates analyzed systems where updates follow independent routes, showing how server diversity can benefit AoI performance when packets may arrive out of order (Yates, 2018).

Optimization approaches for multi-step systems typically involve determining optimal service rates for each processing stage under power constraints. These optimizations become increasingly complex when considering non-linear age penalty functions that more accurately represent the relationship between information staleness and application performance (Zheng et al., 2019)(Sun et al., 2016)(Kosta et al., 2017).

The scheduling problem becomes particularly challenging in multi-source, multi-step systems. Various scheduling policies have been proposed, including age-aware approaches like Maximum-Age-First (MAF) and Maximum-Age-Difference (MAD), alongside simpler age-agnostic policies such as cyclic and probabilistic scheduling (Kadota et al., 2018)(Beytur et al., 2018)(Liyanaarachchi et al._1, 2024)(Gamgam et al., 2024).

As research in multi-step processing systems evolves, it increasingly connects with broader applications in remote estimation and inference systems. Ari et al. note that many practical implementations require the generation of data "at will" rather than relying solely on exogenous data arrivals, particularly in joint sampling and scheduling policies that optimize end-to-end distortion criteria (Ari et al., 2023). This approach aligns with the "generate-at-will" model explored in various scheduling contexts, where the system has control over when to initiate new updates (Gamgam et al., 2024).

The integration of AoI optimization with sequential processing requirements represents a critical step toward more realistic modeling of practical systems where information must be processed before use. As these models mature, they promise to provide a foundation for designing efficient, power-aware systems that maintain information freshness across complex, multi-stage computing environments.
----------------------------------------------------------------------------------

Rank-metric codes were independently introduced by Delsarte in 1978 and Gabidulin in 1985 for combinatorial purposes (Sheekey, 2019)(Neri et al., 2021). Roth later rediscovered them in 1991, demonstrating their application to crisscross error correction (Augot et al., 2020)(Roth, 1991). These codes are defined in two equivalent ways: as a subspace of matrices over a finite field with the distance between two matrices being the rank of their difference, or as a linear subspace of vectors over an extension field (Pratihar et al., 2019).

The significance of rank-metric codes dramatically increased following Silva, Kötter, and Kschischang's groundbreaking work in 2008, where they demonstrated the application of these codes to random linear network coding (Neri et al., 2021)(Silva et al., 2007). This application catalyzed extensive research in the field (Jurrius et al., 2015). In network coding, messages are transmitted across a network of channels rather than a single channel, making rank-metric codes particularly suitable for error correction in this context (Gorla et al., 2017).

Beyond network coding, rank-metric codes have found diverse applications, including crisscross error correction, distributed storage systems, space-time coding, and post-quantum cryptography (Ravagnani, 2014)(Marino et al., 2022). In cryptography, Gabidulin, Paramonov, and Tretjakov proposed the GPT cryptosystem in 1991, which is a modification of the McEliece cryptosystem using rank-metric codes (Pratihar et al., 2019). For distributed storage, these codes have been employed to construct codes with locality, while in coded caching, they have been used for the placement of coded symbols (Couvreur, 2022)(Bartz et al., 2022).

Maximum Rank Distance (MRD) codes are an important class of rank-metric codes that meet the Singleton bound, which establishes a relationship between the dimension of a code and its minimum rank distance (Gorla et al., 2022). The first known family of MRD codes, known as Gabidulin codes, were constructed independently by Delsarte, Gabidulin, and Roth (Sheekey, 2019). These codes are often described as the rank-metric equivalent of Reed-Solomon codes and share similar favorable properties, including efficient decoding algorithms (Silberstein et al., 2012)(Couvreur, 2022).

From a mathematical perspective, rank-metric codes possess rich algebraic, geometric, and combinatorial properties (Marino et al., 2022). They have established connections with various mathematical objects, including semifields, linear sets in finite geometry, q-analogues in combinatorics, tensorial algebra, skew algebras, and matroid theory (Neri et al., 2021)(Sheekey, 2015)(Polverino et al., 2020)(Byrne et al., 2019)(Gorla et al., 2018). These connections highlight the multidisciplinary nature of rank-metric codes and their theoretical significance beyond practical applications.

Rank-metric codes have gained substantial prominence in network coding applications, where they provide efficient error correction capabilities. The groundbreaking work of Silva, Kötter, and Kschischang in 2007-2008 established rank-metric codes as powerful tools for error control in random linear network coding (Silva et al., 2007)(Koetter et al., 2007). In this context, messages are transmitted across a network of channels rather than a single channel, making rank-metric codes particularly suitable for error correction (Augot et al., 2020). The subspace coding approach, which uses lifted rank-metric codes, preserves the structural and distance properties of the underlying code, enabling efficient error correction in network communications (Morrison, 2013).

Beyond basic network coding, rank-metric codes have been extended to multishot network coding through the sum-rank metric, which hybridizes the Hamming and rank metrics (Bartz et al., 2021). This approach allows for error correction across multiple transmission shots, with linearized Reed-Solomon codes achieving the Singleton bound in the sum-rank metric (Martinez-Penas et al., 2018). These codes have demonstrated superior performance for coherent and non-coherent communication in adversarial settings (Nobrega et al., 2009)(Wachter-Zeh et al., 2011).

In cryptography, rank-metric codes have enabled the development of several systems with smaller key sizes compared to Hamming-metric codes (Franch et al., 2023). The GPT (Gabidulin-Paramonov-Tretjakov) cryptosystem, proposed in 1991, was one of the first applications of rank-metric codes for cryptographic purposes (Augot et al., 2020). More recent rank-based cryptographic schemes include RankSign, identity-based encryption systems, ROLLO, and the Durandal signature scheme (Franch et al., 2023)(Gaborit et al., 2014).

For distributed storage systems, rank-metric codes with locality constraints have been developed to recover from crisscross failures affecting limited numbers of rows and/or columns (Kadhe et al., 2016). Rank-metric codes have also been employed to construct codes with locality for distributed storage, while in coded caching, they have been used for the placement of coded symbols (Rawat et al., 2012). These applications enhance both the security and repair efficiency of distributed storage systems.

Additional applications include crisscross error correction, as initially demonstrated by Roth (Roth, 1991), space-time coding for MIMO systems (Liu et al., 2000), and securing network coding communication systems against eavesdroppers (Ravagnani, 2014)(Kurihara et al., 2013). These diverse applications demonstrate how the unique properties of rank-metric codes enable solutions to problems where traditional error-correcting codes are insufficient or suboptimal.

The sum-rank metric has further expanded the application domain of rank-metric codes to MIMO block-fading channels, streaming codes, robust coded distributed storage systems, and post-quantum secure cryptosystems (Ott et al., 2023). This metric generalizes both the Hamming and rank metrics, providing flexibility for addressing various communication and storage challenges.

Rank-metric codes possess rich geometric interpretations that have enhanced their theoretical understanding and provided alternative frameworks for their analysis. As noted by Marino et al., "rank-metric codes possess interesting algebraic, geometric and combinatorial properties, which make them fascinating also for pure mathematicians" (Marino et al., 2022). The geometric approach to rank-metric codes has been formalized through connections to linear sets in finite geometry and q-systems, which provide powerful tools for analyzing code properties (Marino et al., 2022)(Randrianarisoa, 2019).

Randrianarisoa demonstrated how the geometric perspective through q-systems (similar to linear sets) can be used to redefine and prove properties of generalized rank weights, as well as classify constant rank weight codes, which are analogous to Hadamard codes in the Hamming metric setting (Randrianarisoa, 2019). This geometric approach has revealed parallels between rank-metric codes and classical coding theory, offering new insights into their structure and behavior.

Invariants play a crucial role in characterizing and distinguishing between different rank-metric codes. Gorla et al. describe two basic invariants of rank-metric codes: the minimum distance (the least rank of a nonzero matrix in the code) and the maximum rank (the maximum rank of an element in the code) (Gorla et al., 2022). These invariants are related to fundamental bounds in coding theory - the Singleton bound and the Anticode bound - which help define Maximum Rank Distance (MRD) codes and optimal anticodes, respectively (Gorla et al., 2022)(Meshulam, 1985).

Generalized rank weights have emerged as significant invariants that extend the concept of minimum rank distance and form a strictly increasing sequence of integers (monotonicity) (Marino et al., 2022). These weights have theoretical importance, as they completely determine the generalized rank weights of the dual code (Wei-type duality), and practical significance in applications such as secure network coding (Marino et al., 2022)(Kurihara et al., 2013). Kurihara et al. introduced related concepts of relative dimension/intersection profile (RDIP) and relative generalized rank weight (RGRW), which have applications in analyzing the security performance and error correction capability of secure network coding (Kurihara et al., 2013).

Another important class of invariants comes from tensor representations of rank-metric codes. Byrne et al. demonstrated that "generalized tensor ranks lead to a refinement of the tensor rank bound" and can be used to "distinguish between inequivalent codes and, remarkably, even between MRD codes that otherwise share many invariants" (Byrne et al., 2019). The tensor description also provides insights into coding theoretic parameters, with the minimum rank distance being characterized by the dimensions of slice spaces, analogous to how minimum Hamming distance relates to a code's parity check matrix (Byrne et al., 2019).

A novel geometric invariant for linear rank-metric codes, inspired by the Schur product used in Hamming metric codes, has been introduced by Astore et al. (Astore et al., 2024). This approach examines the sequence of dimensions of Schur powers of the extended Hamming code associated with a linear code and demonstrates its ability to differentiate Gabidulin codes from random ones. From a geometric perspective, this corresponds to investigating the vanishing ideal of a linear set corresponding to the rank-metric code (Astore et al., 2024).

The connection between rank-metric codes and q-polymatroids, the q-analogue of polymatroids, has further expanded the combinatorial understanding of these codes. Gorla et al. showed that several invariants and structural properties of rank-metric codes, such as generalized weights, the property of being MRD or an optimal anticode, and duality, can be captured by the associated q-polymatroid (Gorla et al., 2018). This connection provides a unified framework for studying the combinatorial aspects of rank-metric codes.

Gabidulin codes, despite being first constructed by Delsarte in 1978, are named after Gabidulin who independently studied them in 1985, along with Roth who rediscovered them in 1991 (Pratihar et al., 2019)(Roth, 1991). These codes represent the first known class of Maximum Rank Distance (MRD) codes, which achieve the Singleton bound for rank-metric codes (Pratihar et al., 2019). The construction of Gabidulin codes relies on linearized polynomials (also called q-polynomials), which were introduced and studied by Ore (Pratihar et al., 2019).

Gabidulin codes are frequently characterized as the rank-metric counterpart of Reed-Solomon codes due to their similar optimal distance properties and algebraic structure (Couvreur, 2022). Like Reed-Solomon codes, they can be described using polynomial evaluation, though in the case of Gabidulin codes, linearized polynomials are evaluated on a set of linearly independent points over the base field (Silberstein et al., 2012). Furthermore, Gabidulin codes benefit from efficient decoding algorithms capable of correcting errors up to half the minimum distance (Couvreur, 2022)(Augot et al., 2013). These algorithms are adaptations of efficient deterministic decoding methods originally developed for Reed-Solomon codes (Franch et al., 2023).

In network coding applications, Silva et al. demonstrated that Gabidulin codes can efficiently handle both erasures and deviations during transmission, expanding their error correction capabilities (Silva et al., 2007). If μ erasures and δ deviations occur, errors of rank t can be corrected provided that 2t ≤ d - 1 + μ + δ, where d is the minimum rank distance of the code (Silva et al., 2007).

The strong algebraic structure of Gabidulin codes has made them attractive for various applications, including random network coding (Koetter et al., 2007), crisscross error correction (Roth, 1991), and cryptography (Franch et al., 2023). However, this strong structure also presents challenges in cryptographic applications, as demonstrated by the vulnerability of the GPT cryptosystem and its variants to algebraic attacks (Franch et al., 2023).

Beyond the classical Gabidulin codes, researchers have developed several alternative constructions of MRD codes to expand the family of available rank-metric codes. Sheekey introduced twisted Gabidulin codes, a new class of rank-metric codes that includes the original Gabidulin codes but also contains codes not equivalent to them (Rosenthal et al., 2017). This was further expanded into generalized twisted Gabidulin codes, providing even more variety in the construction of rank-metric codes (Rosenthal et al., 2017).

Linearized Reed-Solomon (LRS) codes represent another important development in the field, as they achieve the Singleton bound in the sum-rank metric (Puchinger et al., 2021). In the extreme cases of the sum-rank metric, LRS codes coincide with Reed-Solomon codes (Hamming metric) and Gabidulin codes (rank metric), bridging these two important code families (Puchinger et al., 2021).

The development of new constructions beyond Gabidulin codes is particularly important for cryptographic applications, where the strong algebraic structure of Gabidulin codes can be exploited in attacks. These newer constructions have enabled the development of various rank-based cryptographic schemes, including RankSign, identity-based encryption systems, ROLLO, and the Durandal signature scheme (Franch et al., 2023)(Gaborit et al., 2014).

The sum-rank metric was initially conceived for applications in MIMO block-fading channels and the design of AM-PSK constellations (Lu et al., 2005)(Lu, 2006), before being explicitly introduced for multishot network coding (Nobrega et al., 2009). In multishot network coding, messages are transmitted across multiple consecutive shots of a network rather than in a single transmission, making the sum-rank metric particularly suitable for error correction in this context (Nobrega et al., 2010). This metric naturally hybridizes the Hamming and rank metrics, providing a unified framework that includes both as special cases (D'Alconzo, 2024)(Camps-Moreno et al., 2021). When the number of blocks is one, the sum-rank metric reduces to the rank metric, and when each block has size 1×m, it becomes equivalent to the Hamming metric (D'Alconzo, 2024).

The most significant advancement in sum-rank metric codes came with the introduction of linearized Reed-Solomon (LRS) codes, which achieve the Singleton bound in this metric and are therefore known as Maximum Sum-Rank Distance (MSRD) codes (Martinez-Penas et al., 2018). These codes represent a generalization that bridges Reed-Solomon codes (optimal in the Hamming metric) and Gabidulin codes (optimal in the rank metric) (Puchinger et al., 2021). LRS codes have proven particularly valuable for multishot network coding in adversarial settings, where they can achieve both zero-error communication and perfect secrecy (Martinez-Penas et al., 2018). Notably, these codes require significantly smaller field sizes compared to Gabidulin codes tailored for multiple shots, resulting in more efficient implementations (Martinez-Penas et al., 2018).

The theoretical foundations of sum-rank metric codes have been comprehensively developed in recent years. Byrne et al. established fundamental results on bounds, duality theory, and constructions for sum-rank metric codes with variable block sizes (Byrne et al., 2020). Ott et al. derived simplified sphere-packing and Gilbert-Varshamov bounds, along with asymptotic bounds for families of sum-rank metric codes (Ott et al., 2021). Furthermore, Camps-Moreno et al. provided an Anticode Bound for the sum-rank metric and classified optimal anticodes, which led to the definition of generalized sum-rank weights (Camps-Moreno et al., 2021).

The study of convolutional codes in the sum-rank metric has also yielded important results. Wachter-Zeh et al. introduced the concepts of free rank distance, extended row rank distance, and slope for (partial) unit memory codes in the sum-rank metric (Wachter-Zeh et al., 2011). Mahmood et al. demonstrated that the column sum rank parallels column Hamming distance when streaming over networks with link failures (Mahmood et al., 2015). Martinez-Penas et al. developed locally repairable convolutional codes (LRCCs) that enable both local repair for single node erasures and sliding-window global repair for multiple erasures (Martinez-Penas et al., 2019).

Beyond their theoretical significance, sum-rank metric codes have found diverse practical applications. In network coding, they provide efficient error correction for multishot scenarios (Bartz et al., 2021)(Martinez-Penas et al., 2018). For space-time coding, they enable the construction of rate-diversity optimal multiblock space-time codes (Shehadeh et al., 2020)(Shehadeh et al., 2021). In distributed storage systems, sum-rank metric codes have been employed to construct maximally recoverable locally repairable codes (Martinez-Penas et al._1, 2018)(Cai et al., 2020). Additionally, these codes have applications in network streaming, robust coded distributed storage systems, and post-quantum secure cryptosystems (Ott et al., 2023)(Liu et al., 2022).

The extensive research on sum-rank metric codes encompasses fundamental theoretical properties, constructions of optimal codes, and efficient decoding algorithms (Liu et al., 2022). Recent developments include genericity results showing that random linear codes achieve almost the sum-rank metric Gilbert-Varshamov bound with high probability (Ott et al., 2021), as well as studies on the covering properties of these codes (Ott et al., 2022). These advances continue to expand the theoretical understanding and practical utility of sum-rank metric codes across various domains in coding theory and its applications.

The adaptation of Schur product techniques from Hamming-metric codes to rank-metric codes has emerged as a valuable approach for developing new invariants to distinguish between different code families. Astore et al. introduced a novel geometric invariant for linear rank-metric codes inspired by the Schur product concept, which provides a method for distinguishing Gabidulin codes from random codes (Astore et al., 2024). Their approach reveals that directly applying Schur powers to rank-metric codes is insufficient and not rank-equivalence-invariant. To overcome this limitation, they proposed examining the Schur products of the "extended Hamming code" associated with a linear rank-metric code. This geometric perspective effectively investigates the vanishing ideal of the linear set corresponding to the rank-metric code, demonstrating that the behavior of forms of certain degrees can distinguish generalized Gabidulin codes from random ones (Astore et al., 2024).

In related work, Martinez-Penas et al. developed the concept of rank error-correcting pairs, which extends the classical error-correcting pairs used for decoding in the Hamming metric (Martinez-Penas et al., 2015). Their approach defines new types of vector products that extend the coordinatewise product, some preserving symbolic products of linearized polynomials after evaluation, while others coincide with usual matrix products. This framework provides rank error-correcting pairs for codes that are linear over either the extension field or the base field, establishing relationships between both types (Martinez-Penas et al., 2015). Their work also derives bounds on the minimum rank distance of codes and conditions for MRD codes, demonstrating that several well-known families of rank-metric codes admit rank error-correcting pairs. Importantly, their algorithm generalizes the classical algorithm using error-correcting pairs for the Hamming metric, providing a unified approach to decoding across different metric spaces (Martinez-Penas et al., 2015).

----------------------------------------------------------------------------------

