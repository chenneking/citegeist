In recent years, the evaluation of large language models (LLMs) in the domain of code understanding and generation has garnered significant attention. A common theme across several studies is the critique of existing benchmarks, such as HumanEval, for their inadequacy in capturing the full spectrum of LLMs' code reasoning abilities. For instance, Allamanis et al. (2024) introduce CodeJudge-Eval, a benchmark that assesses LLMs' ability to judge code correctness, highlighting the limitations of traditional benchmarks in evaluating code reasoning capabilities. This aligns with our research, which emphasizes the need for more comprehensive evaluation methods to assess LLMs' understanding of code execution paths, particularly in complex scenarios. The concept of round-trip correctness (RTC) proposed by Allamanis et al. offers an unsupervised evaluation approach that could potentially be adapted to assess execution tracing capabilities, a core focus of our study.

Similarly, Liang et al. (2024) and Dou et al. (2024) underscore the limitations of current benchmarks like HumanEval in evaluating LLMs' real-world coding capabilities. Both studies advocate for more complex and context-rich tasks to better assess LLMs' code reasoning abilities. Our research complements these findings by introducing the Benchmark CoCoNUT, which specifically targets the evaluation of code control flow understanding, including advanced structural components such as recursion and object-oriented programming. The introduction of specialized subsets in our work parallels the call for more robust benchmarks, as seen in the REPOCOD benchmark introduced by Dou et al., which aims to address similar challenges.

The need for multidimensional evaluation metrics is further echoed by Fu et al. (2023) and Yan et al. (2023), who highlight the importance of assessing LLMs beyond mere correctness. Fu et al. emphasize the need for evaluation metrics that consider readability, maintainability, and efficiency, which aligns with our critique of the overemphasis on correctness in existing benchmarks. Yan et al.'s CodeScope benchmark, which evaluates LLMs across multiple programming languages and tasks, complements our focus on execution path tracing by highlighting the necessity for comprehensive evaluation frameworks that capture the nuanced capabilities and limitations of LLMs in real-world programming scenarios.

Moreover, the introduction of datasets like the Mostly Hard Python Problems (MHPP) by Dai et al. (2024) and frameworks like CodeMind by Liu et al. (2024) further illustrate the ongoing efforts to challenge LLMs' code reasoning abilities. These works emphasize the gap between code generation and reasoning, a theme central to our research. While MHPP questions the sufficiency of existing benchmarks in evaluating function-level code generation, CodeMind focuses on tasks like Independent Execution Reasoning, underscoring the need for advancements in LLMs' reasoning capabilities. Our work, which demonstrates LLMs' struggles with tracing execution paths, particularly in complex structures, aligns with these studies' findings.

Finally, Ma et al. (2023) and Zheng et al. (2024) explore the limitations of LLMs in understanding code semantics and reasoning, respectively. Ma et al. highlight the gap between LLMs' proficiency in syntax comprehension and their understanding of dynamic semantics, which resonates with our findings on the limited ability of LLMs to trace execution paths. Zheng et al.'s REval framework, which evaluates code reasoning and consistency with program execution, parallels our introduction of CoCoNUT, both revealing the challenges LLMs face in code reasoning.

In summary, the collective body of related work underscores a critical need for more comprehensive and multidimensional benchmarks to evaluate LLMs' code reasoning abilities. Our research contributes to this discourse by introducing the Benchmark CoCoNUT, which specifically targets the evaluation of code control flow understanding, including advanced structural components. By situating our work within this broader context, we aim to bridge the gap between code generation and reasoning, ultimately advancing the capabilities of LLMs in software engineering tasks.