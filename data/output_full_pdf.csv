longlist_length,longlist_ratings,shortlist_length,shortlist_ratings,rating_source_ours,rating_ours_gpt,related_works,citations
30,5|8|7|8|8|8|8|6|8|5|7|8|7|7|7|7|8|8|6|7|8|7|7|8|7|8|7|4|7|8,10,8|8|7|8|8|4|8|7|8|8,source,source,"## Related Work

The evaluation of code reasoning capabilities in large language models (LLMs) has been a focal point of recent research, with various benchmarks and frameworks being proposed to address the limitations of existing methods. SpecEval, introduced by Lezhi Ma et al. (2024), offers a black-box evaluation framework that assesses code comprehension in LLMs through formal program specifications, contrasting with CoCoNUT's focus on tracing complex control flows. Both studies highlight the inadequacies of current benchmarks in understanding program semantics and stress the need for more robust evaluation frameworks to enhance code comprehension capabilities (Ma et al., 2024). Similarly, CRUXEval, presented by Alex Gu et al. (2024), evaluates code reasoning and execution capabilities, revealing that models excelling in HumanEval often underperform in CRUXEval. This aligns with CoCoNUT's findings on the challenges models face with complex control flows, underscoring the necessity for comprehensive benchmarks to assess code reasoning (Gu et al., 2024).

Further exploring the challenges in code reasoning, ML-Bench, as discussed by Xiangru Tang et al. (2023), evaluates LLMs' ability to understand and interact with complex code repositories. While ML-Bench focuses on repository-level tasks, CoCoNUT assesses the ability of LLMs to trace execution paths and understand advanced control structures. Both benchmarks aim to improve LLMs' code reasoning capabilities, albeit at different scales and complexities, highlighting the need for sophisticated benchmarks (Tang et al., 2023). Wei Ma et al. (2023) also investigate the limitations of LLMs in comprehending code semantics, particularly dynamic behaviors, which aligns with CoCoNUT's focus on execution path tracing. Both studies emphasize the models' struggles with dynamic code analysis, advocating for improved benchmarks to evaluate and enhance LLMs' code reasoning capabilities (Ma et al., 2023).

In addition to these, frameworks like CodeMind and Code-Optimise offer complementary perspectives on evaluating LLMs' code reasoning abilities. CodeMind, introduced by Changshu Liu et al. (2024), highlights the limitations of relying solely on test passing for assessment, presenting tasks that reveal LLMs' declining performance with complexity. This aligns with CoCoNUT's identification of gaps in understanding complex code structures, emphasizing the need for benchmarks that assess deeper code reasoning capabilities (Liu et al., 2024). On the other hand, Code-Optimise, as proposed by Leonidas Gee et al. (2024), focuses on balancing correctness and runtime efficiency in code generation, complementing CoCoNUT by addressing the runtime aspect of code execution. Both works contribute to advancing the understanding and capabilities of LLMs in code-related tasks from different perspectives (Gee et al., 2024).

The REval framework, introduced by Junkai Chen et al. (2024), evaluates LLMs' reasoning abilities by focusing on runtime behavior and incremental consistency, addressing gaps in existing benchmarks that overlook intermediate program execution states. This aligns with CoCoNUT's goal of assessing code reasoning capabilities beyond mere input-output prediction, emphasizing the importance of understanding control flow and complex programming structures (Chen et al., 2024). Similarly, CodeScope, presented by Weixiang Yan et al. (2023), evaluates LLMs across multiple programming languages and tasks, addressing limitations in existing benchmarks by focusing on multilingual, multitask, and execution-based evaluations. While CodeScope emphasizes multilingual and multitask dimensions, CoCoNUT focuses on control flow understanding, both highlighting the need for robust benchmarks to assess LLMs' performance in real-world programming scenarios (Yan et al., 2023).

Lastly, CodeJudge-Eval, introduced by Yuwei Zhao et al. (2024), assesses LLMs' code understanding abilities by evaluating their capacity to judge the correctness of code solutions, rather than generating code. This approach complements CoCoNUT by addressing the limitations of traditional benchmarks that focus on code generation, highlighting the need for deeper evaluation of LLMs' understanding of code structures and execution (Zhao et al., 2024). Additionally, BESTER, as discussed by Jialin Song et al. (2024), enhances the debugging capabilities of LLMs through self-reflection and best-first tree search, achieving state-of-the-art results in code generation benchmarks. This work is relevant to CoCoNUT as it addresses the limitations of LLMs in handling complex programming tasks, similar to CoCoNUT's findings on their struggles with tracing execution paths in code (Song et al., 2024). Together, these studies underscore the importance of developing benchmarks that go beyond surface-level code generation to truly assess the reasoning and comprehension capabilities of LLMs in programming tasks.","Alex Gu, Baptiste Rozi√®re, Hugh Leather, Armando Solar-Lezama, Gabriel Synnaeve, Sida I. Wang (2024). CRUXEval: A Benchmark for Code Reasoning, Understanding and Execution. arXiv:2401.03065. https://arxiv.org/abs/2401.03065
Jialin Song, Jonathan Raiman, Bryan Catanzaro (2024). Effective Large Language Model Debugging with Best-first Tree Search. arXiv:2407.19055. https://arxiv.org/abs/2407.19055
Wei Ma, Shangqing Liu, Zhihao Lin, Wenhan Wang, Qiang Hu, Ye Liu, Cen Zhang, Liming Nie, Li Li, Yang Liu (2023). LMs: Understanding Code Syntax and Semantics for Code Analysis. arXiv:2305.12138. https://arxiv.org/abs/2305.12138
Leonidas Gee, Milan Gritta, Gerasimos Lampouras, Ignacio Iacobacci (2024). Code-Optimise: Self-Generated Preference Data for Correctness and Efficiency. arXiv:2406.12502. https://arxiv.org/abs/2406.12502
Weixiang Yan, Haitian Liu, Yunkun Wang, Yunzhe Li, Qian Chen, Wen Wang, Tingyu Lin, Weishan Zhao, Li Zhu, Hari Sundaram, Shuiguang Deng (2023). CodeScope: An Execution-based Multilingual Multitask Multidimensional Benchmark for Evaluating LLMs on Code Understanding and Generation. arXiv:2311.08588. https://arxiv.org/abs/2311.08588
Junkai Chen, Zhiyuan Pan, Xing Hu, Zhenhao Li, Ge Li, Xin Xia (2024). Reasoning Runtime Behavior of a Program with LLM: How Far Are We?. arXiv:2403.16437. https://arxiv.org/abs/2403.16437
Yuwei Zhao, Ziyang Luo, Yuchen Tian, Hongzhan Lin, Weixiang Yan, Annan Li, Jing Ma (2024). CodeJudge-Eval: Can Large Language Models be Good Judges in Code Understanding?. arXiv:2408.10718. https://arxiv.org/abs/2408.10718
Lezhi Ma, Shangqing Liu, Lei Bu, Shangru Li, Yida Wang, Yang Liu (2024). SpecEval: Evaluating Code Comprehension in Large Language Models via Program Specifications. arXiv:2409.12866. https://arxiv.org/abs/2409.12866
Changshu Liu, Shizhuo Dylan Zhang, Ali Reza Ibrahimzada, Reyhaneh Jabbarvand (2024). CodeMind: A Framework to Challenge Large Language Models for Code Reasoning. arXiv:2402.09664. https://arxiv.org/abs/2402.09664
Xiangru Tang, Yuliang Liu, Zefan Cai, Yanjun Shao, Junjie Lu, Yichi Zhang, Zexuan Deng, Helan Hu, Kaikai An, Ruijun Huang, Shuzheng Si, Sheng Chen, Haozhe Zhao, Liang Chen, Yan Wang, Tianyu Liu, Zhiwei Jiang, Baobao Chang, Yin Fang, Yujia Qin, Wangchunshu Zhou, Yilun Zhao, Arman Cohan, Mark Gerstein (2023). ML-Bench: Evaluating Large Language Models and Agents for Machine Learning Tasks on Repository-Level Code. arXiv:2311.09835. https://arxiv.org/abs/2311.09835"
30,8|8|8|8|7|7|5|5|7|7|3|3|3|8|6|5|6|5|5|5|6|3|3|3|3|3|5|4|5|3,10,8|5|5|8|7|8|3|7|5|5,target,source,"In recent years, the intersection of zero-knowledge proofs and machine learning has garnered significant attention, particularly in the context of verifiable and privacy-preserving computations. A comprehensive survey by Xing et al. (2023) provides an extensive overview of zero-knowledge proof-based verifiable machine learning (ZKP-VML) technologies, addressing trustworthiness issues in outsourced model training and inference. This aligns closely with our research focus on ensuring verifiable and tamper-proof machine learning inference using zkSNARKs, as both works emphasize the importance of verifiability in distributed machine learning settings. The survey's analysis of potential verifiability issues and classification of existing approaches offers valuable insights that can inform the development and optimization of our proposed partial privacy solution for distributed ML inference (Xing et al., 2023).

Privacy-preserving machine learning (PPML) has been a focal point of research, with various methodologies proposed to address privacy concerns in distributed ML environments. Fung et al. (2018) introduce TorMentor, a brokered learning framework that enhances federated learning by providing provable privacy guarantees in untrusted settings. While TorMentor emphasizes protecting data providers from known ML attacks through a brokered learning model, our work extends privacy to model parameters during inference using zkSNARKs, offering a complementary approach to ensuring secure and verifiable computations in distributed ML systems (Fung et al., 2018). Similarly, Xu et al. (2021) provide a comprehensive review of PPML solutions, highlighting the critical need for such approaches due to privacy concerns and adversarial attacks. Their exploration of cryptographic techniques and privacy-preserving model serving complements our approach of enabling partial privacy in distributed ML inference, emphasizing the importance of secure and verifiable computations in the ML pipeline (Xu et al., 2021).

The challenges of maintaining privacy during distributed ML inference are further underscored by Knott et al. (2021), who discuss vulnerabilities in secure multi-party computation (MPC) protocols for machine learning, particularly how adversaries can exploit data poisoning to breach privacy. Our research addresses these concerns by proposing a solution that ensures partial privacy and verifiable inference using zkSNARKs, offering a complementary approach to the privacy issues identified in MPC settings (Knott et al., 2021). Additionally, the work by Yu et al. (2024) on privacy threats and defenses in Vertical Federated Learning (VFL) highlights the importance of safeguarding model parameters and training data, which aligns with our focus on ensuring privacy during distributed ML inference using zkSNARKs (Yu et al., 2024).

The application of zero-knowledge proofs in machine learning extends beyond privacy to include verifiability and trust. South et al. (2024) present a method for verifiable model evaluation using zkSNARKs, enabling end-users to confirm model performance claims without accessing private model weights. This aligns with our research on privacy-preserving machine learning inference, as both works leverage zkSNARKs to ensure verifiable computations. While their focus is on verifying model evaluation results, our work extends this concept to enable partial privacy during distributed inference, allowing selective model section revelation (South et al., 2024). Similarly, Waiwitlikhit et al. (2024) introduce ZkAudit, a protocol that enables trustless audits of machine learning models and data using zero-knowledge proofs, maintaining privacy and trust in distributed settings. While ZkAudit focuses on auditing model properties without revealing sensitive information, our work extends the application of zero-knowledge proofs to enable partial privacy during inference, ensuring verifiable computations across distributed nodes (Waiwitlikhit et al., 2024).

Finally, several works have explored privacy-preserving techniques in collaborative and distributed learning settings. Liu et al. (2024) introduce Pencil, a framework for private collaborative learning that ensures both data and model privacy without relying on the non-colluding assumption, addressing a key limitation in existing methods like federated learning and secure multiparty computation. While Pencil emphasizes training privacy, our work extends this by focusing on inference privacy using zkSNARKs, offering a complementary approach to ensuring privacy throughout the machine learning lifecycle (Liu et al., 2024). Similarly, Froelicher et al. (2020) introduce SPINDLE, a system for privacy-preserving distributed learning using multiparty homomorphic encryption, which aligns with the privacy-preserving goals of our work by ensuring data and model confidentiality in distributed settings. While SPINDLE focuses on the complete ML workflow under a passive-adversary model, our research extends privacy to model parameters during inference using zkSNARKs, emphasizing verifiable inference and partial privacy of model parameters (Froelicher et al., 2020).","Brian Knott, Shobha Venkataraman, Awni Hannun, Shubho Sengupta, Mark Ibrahim, Laurens van der Maaten (2021). CrypTen: Secure Multi-Party Computation Meets Machine Learning. arXiv:2109.00984. https://arxiv.org/abs/2109.00984
Suppakit Waiwitlikhit, Ion Stoica, Yi Sun, Tatsunori Hashimoto, Daniel Kang (2024). Trustless Audits without Revealing Data or Models. arXiv:2404.04500. https://arxiv.org/abs/2404.04500
David Froelicher, Juan R. Troncoso-Pastoriza, Apostolos Pyrgelis, Sinem Sav, Joao Sa Sousa, Jean-Philippe Bossuat, Jean-Pierre Hubaux (2020). Scalable Privacy-Preserving Distributed Learning. arXiv:2005.09532. https://arxiv.org/abs/2005.09532
Clement Fung, Jamie Koerner, Stewart Grant, Ivan Beschastnikh (2018). Dancing in the Dark: Private Multi-Party Machine Learning in an Untrusted Setting. arXiv:1811.09712. https://arxiv.org/abs/1811.09712
Lei Yu, Meng Han, Yiming Li, Changting Lin, Yao Zhang, Mingyang Zhang, Yan Liu, Haiqin Weng, Yuseok Jeon, Ka-Ho Chow, Stacy Patterson (2024). A Survey of Privacy Threats and Defense in Vertical Federated Learning: From Model Life Cycle Perspective. arXiv:2402.03688. https://arxiv.org/abs/2402.03688
Runhua Xu, Nathalie Baracaldo, James Joshi (2021). Privacy-Preserving Machine Learning: Methods, Challenges and Directions. arXiv:2108.04417. https://arxiv.org/abs/2108.04417
Zhibo Xing, Zijian Zhang, Jiamou Liu, Ziang Zhang, Meng Li, Liehuang Zhu, Giovanni Russello (2023). Zero-knowledge Proof Meets Machine Learning in Verifiability: A Survey. arXiv:2310.14848. https://arxiv.org/abs/2310.14848
Xuanqi Liu, Zhuotao Liu, Qi Li, Ke Xu, Mingwei Xu (2024). Pencil: Private and Extensible Collaborative Learning without the Non-Colluding Assumption. arXiv:2403.11166. https://arxiv.org/abs/2403.11166
Tobin South, Alexander Camuto, Shrey Jain, Shayla Nguyen, Robert Mahari, Christian Paquin, Jason Morton, Alex 'Sandy' Pentland (2024). Verifiable evaluations of machine learning models using zkSNARKs. arXiv:2402.02675. https://arxiv.org/abs/2402.02675"
30,8|7|3|5|5|5|8|8|5|3|5|5|3|7|7|5|6|2|7|8|7|4|3|5|7|6|7|5|7|7,10,8|8|7|7|7|5|8|8|7|5,target,source,"The exploration of spiking neural networks (SNNs) as a means to enhance energy efficiency in natural language processing (NLP) has gained significant traction in recent years. This research aligns with the broader trend of leveraging SNNs to address the high energy consumption associated with traditional artificial neural networks (ANNs) in language models. Several studies have contributed to this field by proposing innovative approaches to spike-based language modeling. For instance, Xing et al. (2024) introduced SpikeLM, a fully spike-driven language model that employs a bi-directional, elastic spike formulation to bridge the performance gap between SNNs and ANNs in language tasks. This work shares a common goal with our SpikeDecoder model, as both aim to improve energy efficiency in language models using SNNs, albeit with different focuses: SpikeLM explores a general spike-driven mechanism, while SpikeDecoder is specifically designed for a spike-based Transformer decoder in NLP.

In parallel, the development of energy-efficient spiking language models through extreme quantization has been explored by Bal et al. (2024), who achieved a novel 1/1.58-bit spiking architecture. This approach complements our research on SpikeDecoder by addressing the energy consumption challenges of large language models using SNNs. Both studies emphasize the potential of SNNs to enhance energy efficiency in NLP tasks, with the cited work focusing on quantization and knowledge distillation techniques to further reduce energy costs. Similarly, the work by Zhou et al. (2024) on QKFormer, a hierarchical spiking transformer, introduces a novel spike-form Q-K attention mechanism to enhance the performance of SNNs in transformer architectures. This aligns with our focus on developing a low-power, spike-based version of the Transformer decoder, highlighting the shared objective of improving energy efficiency and performance in SNN-based transformer models.

The challenge of encoding real-valued data into spikes, a critical step in implementing SNNs for NLP, has been addressed by Yarga et al. (2022) through efficient spike encoding methods for neuromorphic speech recognition. This work provides valuable insights into optimizing spike density and classification accuracy, which can inform our approach in developing the SpikeDecoder model. Additionally, Bia≈Ças et al. (2020) presented a biologically plausible mechanism for generating low-dimensional spike-based text representations using Spike-Timing-Dependent Plasticity (STDP). This foundational work on spike-based text encoding complements our research on embedding methods to project text data into spike-range for energy-efficient processing, underscoring the importance of effective spike encoding in SNN-based NLP models.

The training challenges of SNNs and their solutions have been a focal point in the literature, as demonstrated by Stanojevic et al. (2023), who proposed robust gradient descent algorithms for SNNs using time-to-first-spike (TTFS) coding. Their findings on achieving performance parity with ANNs provide insights that could enhance the training and optimization of our spike-based Transformer decoder. Similarly, Eshraghian et al. (2021) explored the application of gradient-based learning to SNNs, offering a comprehensive tutorial on leveraging insights from neuroscience and deep learning to develop biologically plausible SNNs. This work complements our investigation into embedding methods for projecting text data into spike-range, supporting the evolving landscape of SNNs that our research contributes to.

Finally, the exploration of scalable spiking models for sequence learning has been advanced by Bal and Sengupta (2024) through the introduction of a probabilistic spiking learning framework for long-range dependency tasks. Their work, which includes a SpikeSampler layer and a SpikeMixer block, is relevant to our research as it explores the use of spiking models for sequence learning, similar to our SpikeDecoder approach for NLP. Both studies aim to improve the computational efficiency of SNNs, with the cited paper focusing on long-range dependencies and our work targeting energy-efficient Transformer models. The integration of state space models in their research complements our exploration of spike-based alternatives in Transformer architectures, highlighting the diverse approaches to enhancing the performance and efficiency of SNNs in NLP applications.","Xingrun Xing, Zheng Zhang, Ziyi Ni, Shitao Xiao, Yiming Ju, Siqi Fan, Yequan Wang, Jiajun Zhang, Guoqi Li (2024). SpikeLM: Towards General Spike-Driven Language Modeling via Elastic Bi-Spiking Mechanisms. arXiv:2406.03287. https://arxiv.org/abs/2406.03287
Malyaban Bal, Abhronil Sengupta (2024). P-SpikeSSM: Harnessing Probabilistic Spiking State Space Models for Long-Range Dependency Tasks. arXiv:2406.02923. https://arxiv.org/abs/2406.02923
Marcin Bia≈Ças, Marcin Micha≈Ç Miro≈Ñczuk, Jacek Ma≈Ñdziuk (2020). Biologically Plausible Learning of Text Representation with Spiking Neural Networks. arXiv:2006.14894. https://arxiv.org/abs/2006.14894
Malyaban Bal, Abhronil Sengupta (2023). SpikingBERT: Distilling BERT to Train Spiking Language Models Using Implicit Differentiation. arXiv:2308.10873. https://arxiv.org/abs/2308.10873
Ana Stanojevic, Stanis≈Çaw Wo≈∫niak, Guillaume Bellec, Giovanni Cherubini, Angeliki Pantazi, Wulfram Gerstner (2023). High-performance deep spiking neural networks with 0.3 spikes per neuron. arXiv:2306.08744. https://arxiv.org/abs/2306.08744
Malyaban Bal, Yi Jiang, Abhronil Sengupta (2024). Exploring Extreme Quantization in Spiking Language Models. arXiv:2405.02543. https://arxiv.org/abs/2405.02543
Chenlin Zhou, Han Zhang, Zhaokun Zhou, Liutao Yu, Liwei Huang, Xiaopeng Fan, Li Yuan, Zhengyu Ma, Huihui Zhou, Yonghong Tian (2024). QKFormer: Hierarchical Spiking Transformer using Q-K Attention. arXiv:2403.16552. https://arxiv.org/abs/2403.16552"
30,8|8|8|8|7|8|8|8|8|8|8|8|8|8|8|8|8|8|8|8|8|7|7|8|8|7|7|8|8|7,10,8|8|8|8|8|8|8|8|8|8,source,source,"## Related Work

The exploration of enhancing video understanding capabilities in Multimodal Large Language Models (MLLMs) has been a focal point in recent research, with various approaches addressing the challenges of temporal understanding and efficient training. Zou et al. (2024) provide a comprehensive review of advancements in MLLMs for long video understanding, emphasizing the unique challenges posed by long-term temporal dependencies and dynamic events. This aligns with our research, which identifies limitations in zero-shot inference and fine-tuning approaches, particularly in temporal understanding. Both works underscore the necessity for improved model design and training methodologies to handle the complexities of long video data, supporting our approach of using synthesized video-like samples to enrich instruction diversity and improve learning efficiency (Zou et al., 2024).

Several studies have focused on optimizing the architecture and training strategies of video-MLLMs to enhance temporal understanding and manage the abundance of visual tokens in videos. Fei et al. (2024) introduce Video-CCAM, which employs cross-attention layers with causal cross-attention masks to enhance temporal understanding, aligning with our focus on overcoming limitations in handling temporal information. Similarly, Ataallah et al. (2024) present MiniGPT4-Video, which extends the capabilities of MiniGPT-v2 to process sequences of frames, incorporating both visual and textual data for comprehensive video analysis. Both approaches aim to address the challenges of temporal understanding in videos, complementing our T2Vid method, which enriches instruction diversity for efficient training (Fei et al., 2024; Ataallah et al., 2024).

The challenge of long video understanding has also been addressed through innovative strategies to extend the visual context window and manage memory consumption. Wei and Chen (2024) propose a progressive pooling inference strategy to manage memory consumption without retraining on large-scale datasets, which aligns with our research focus on enhancing video understanding capabilities of MLLMs. Qu et al. (2024) introduce TS-LLaVA, a method for constructing visual tokens through a Thumbnail-and-Sampling strategy to enhance training-free video LLMs, addressing the challenge of limited high-quality video-text data by leveraging pre-trained image LLMs. Both works highlight the potential of using image-based models for video tasks, with TS-LLaVA focusing on visual token compression and T2Vid on instruction diversity to improve model performance (Wei & Chen, 2024; Qu et al., 2024).

Benchmarking and evaluation frameworks have also been explored to improve video understanding in MLLMs. Zhao et al. (2024) introduce VideoNIAH, a benchmark construction framework that uses synthetic video generation to evaluate video understanding, addressing inefficiencies in current benchmarks. This approach aligns with our research on enhancing video understanding in MLLMs, as both works aim to improve the evaluation and training processes for video models. Liu et al. (2024) present Kangaroo, a Video LMM designed to address challenges in processing long videos, emphasizing the importance of high-quality datasets and innovative training strategies to improve video comprehension. Both studies focus on overcoming limitations in existing models for long video comprehension, highlighting the importance of efficient data representation and processing in multimodal large language models (Zhao et al., 2024; Liu et al., 2024).

Finally, several works have explored resource-efficient methods for adapting pre-trained image-language models to video understanding. Xu et al. (2024) introduce PLLaVA, which employs a pooling strategy to smooth feature distribution, complementing our T2Vid approach that seeks to optimize training efficiency and performance in video tasks. Wang et al. (2024) propose Long Video Chat (LVChat), which addresses the challenge of over-compression in long video comprehension by proposing Frame-Scalable Encoding and Interleaved Frame Encoding to dynamically adjust embeddings based on video duration. Li et al. (2024) introduce the Text-Only Pre-Alignment (TOPA) framework, which extends large language models for video understanding without pre-training on real video data, by using textual video representations to simulate video dynamics. These approaches demonstrate complementary strategies for improving video-LLM alignment and performance, aligning with our research on enhancing video understanding through data augmentation (Xu et al., 2024; Wang et al., 2024; Li et al., 2024).","Kirolos Ataallah, Xiaoqian Shen, Eslam Abdelrahman, Essam Sleiman, Deyao Zhu, Jian Ding, Mohamed Elhoseiny (2024). MiniGPT4-Video: Advancing Multimodal LLMs for Video Understanding with Interleaved Visual-Textual Tokens. arXiv:2404.03413. https://arxiv.org/abs/2404.03413
Tingyu Qu, Mingxiao Li, Tinne Tuytelaars, Marie-Francine Moens (2024). TS-LLaVA: Constructing Visual Tokens through Thumbnail-and-Sampling for Training-Free Video Large Language Models. arXiv:2411.11066. https://arxiv.org/abs/2411.11066
Hongchen Wei, Zhenzhong Chen (2024). Visual Context Window Extension: A New Perspective for Long Video Understanding. arXiv:2409.20018. https://arxiv.org/abs/2409.20018
Jiajun Fei, Dian Li, Zhidong Deng, Zekun Wang, Gang Liu, Hui Wang (2024). Video-CCAM: Enhancing Video-Language Understanding with Causal Cross-Attention Masks for Short and Long Videos. arXiv:2408.14023. https://arxiv.org/abs/2408.14023
Yu Wang, Zeyuan Zhang, Julian McAuley, Zexue He (2024). LVCHAT: Facilitating Long Video Comprehension. arXiv:2402.12079. https://arxiv.org/abs/2402.12079
Lin Xu, Yilin Zhao, Daquan Zhou, Zhijie Lin, See Kiong Ng, Jiashi Feng (2024). PLLaVA : Parameter-free LLaVA Extension from Images to Videos for Video Dense Captioning. arXiv:2404.16994. https://arxiv.org/abs/2404.16994
Heqing Zou, Tianze Luo, Guiyang Xie, Victor, Zhang, Fengmao Lv, Guangcong Wang, Junyang Chen, Zhuochen Wang, Hansheng Zhang, Huaijian Zhang (2024). From Seconds to Hours: Reviewing MultiModal Large Language Models on Comprehensive Long Video Understanding. arXiv:2409.18938. https://arxiv.org/abs/2409.18938
Jiajun Liu, Yibing Wang, Hanghang Ma, Xiaoping Wu, Xiaoqi Ma, Xiaoming Wei, Jianbin Jiao, Enhua Wu, Jie Hu (2024). Kangaroo: A Powerful Video-Language Model Supporting Long-context Video Input. arXiv:2408.15542. https://arxiv.org/abs/2408.15542
Wei Li, Hehe Fan, Yongkang Wong, Mohan Kankanhalli, Yi Yang (2024). TOPA: Extending Large Language Models for Video Understanding via Text-Only Pre-Alignment. arXiv:2405.13911. https://arxiv.org/abs/2405.13911
Zijia Zhao, Haoyu Lu, Yuqi Huo, Yifan Du, Tongtian Yue, Longteng Guo, Bingning Wang, Weipeng Chen, Jing Liu (2024). Needle In A Video Haystack: A Scalable Synthetic Evaluator for Video MLLMs. arXiv:2406.09367. https://arxiv.org/abs/2406.09367"
30,7|8|3|5|8|8|7|5|7|8|6|7|7|8|3|7|7|8|7|7|7|5|5|7|7|7|5|5|7|3,10,8|7|7|5|7|8|7|7|7|8,source,source,"The exploration of reasoning capabilities in language models has been a focal point of recent research, with various methodologies proposed to enhance the reasoning performance of smaller models by leveraging the strengths of larger models. Our Reverse-Enhanced Thinking (REVTHINK) framework aligns with several contemporary approaches that aim to distill complex reasoning abilities into smaller, more efficient models. Notably, Fine-tune-CoT, introduced by Namgyu Ho et al. (2022), employs large language models as reasoning teachers to fine-tune smaller models with diverse reasoning samples. While Fine-tune-CoT emphasizes chain-of-thought reasoning, our work focuses on bidirectional reasoning, incorporating both forward and reverse thinking to enhance reasoning capabilities (Ho et al., 2022). Similarly, SIKeD, proposed by Shivam Adarsh et al. (2024), addresses strategy bias in traditional distillation by teaching smaller models to use multiple strategies, which parallels our bidirectional reasoning approach in REVTHINK (Adarsh et al., 2024).

The utilization of additional data dimensions to improve reasoning performance is another theme in recent research. Yiwei Li et al. (2023) explore the use of negative data in distilling reasoning capabilities, complementing the positive data traditionally used. This approach aligns with our research on reverse thinking, as both works aim to enhance reasoning performance by leveraging diverse data dimensions (Li et al., 2023). Similarly, the KPOD framework by Kaituo Feng et al. (2024) emphasizes the importance of token significance and progressive learning order, focusing on structured reasoning processes that resonate with our reverse thinking approach (Feng et al., 2024). These studies highlight the potential of utilizing varied data types and structured learning to improve reasoning capabilities in language models.

Innovative prompting techniques and restructuring training objectives have also been explored to enhance reasoning capabilities. The BRAINTEASER task, as discussed by Harshit Gupta et al. (2024), employs innovative prompting techniques to enhance lateral thinking, which aligns with our reverse thinking approach by challenging conventional thinking patterns (Gupta et al., 2024). Similarly, CasCoD, introduced by Chengwei Dai et al. (2024), restructures training objectives to improve reasoning diversity and generalizability, complementing our bidirectional reasoning framework in REVTHINK (Dai et al., 2024). These approaches underscore the importance of innovative methodologies in refining the reasoning processes of language models.

The teacher-student model dynamic is a common strategy employed to enhance reasoning capabilities, as seen in SuperCorrect by Ling Yang et al. (2024). This framework enhances reasoning and self-correction abilities by leveraging insights from a large teacher model, similar to our use of bidirectional reasoning to enhance consistency in REVTHINK (Yang et al., 2024). Additionally, Dialogue-guided Chain-of-Thought (DialCoT) by Chengcheng Han et al. (2023) and QuestCoT by Kushal Jain et al. (2023) both emphasize the importance of strategic reasoning paths, albeit through different mechanisms. DialCoT uses a dialogue format to decompose complex reasoning tasks, while QuestCoT focuses on initiating the correct reasoning chain, both of which align with our emphasis on structured guidance to improve reasoning accuracy and efficiency (Han et al., 2023; Jain et al., 2023).

Finally, the transfer of reasoning capabilities through knowledge distillation is a prevalent theme in the literature. Lucie Charlotte Magister et al. (2022) explore this transfer using chain-of-thought prompting, which aligns with our research on enhancing reasoning in LLMs. While their work focuses on CoT prompting to improve task performance, our REVTHINK framework introduces bidirectional reasoning to further enhance reasoning capabilities in smaller models (Magister et al., 2022). Collectively, these studies highlight the significance of structured reasoning processes and the potential of leveraging large models to improve the reasoning performance of smaller models.","Harshit Gupta, Manav Chaudhary, Tathagata Raha, Shivansh Subramanian, Vasudeva Varma (2024). iREL at SemEval-2024 Task 9: Improving Conventional Prompting Methods for Brain Teasers. arXiv:2405.16129. https://arxiv.org/abs/2405.16129
Kushal Jain, Moritz Miller, Niket Tandon, Kumar Shridhar (2023). First-Step Advantage: Importance of Starting Right in Multi-Step Math Reasoning. arXiv:2311.07945. https://arxiv.org/abs/2311.07945
Kaituo Feng, Changsheng Li, Xiaolu Zhang, Jun Zhou, Ye Yuan, Guoren Wang (2024). Keypoint-based Progressive Chain-of-Thought Distillation for LLMs. arXiv:2405.16064. https://arxiv.org/abs/2405.16064
Chengwei Dai, Kun Li, Wei Zhou, Songlin Hu (2024). Improve Student's Reasoning Generalizability through Cascading Decomposed CoTs Distillation. arXiv:2405.19842. https://arxiv.org/abs/2405.19842
Lucie Charlotte Magister, Jonathan Mallinson, Jakub Adamek, Eric Malmi, Aliaksei Severyn (2022). Teaching Small Language Models to Reason. arXiv:2212.08410. https://arxiv.org/abs/2212.08410
Ling Yang, Zhaochen Yu, Tianjun Zhang, Minkai Xu, Joseph E. Gonzalez, Bin Cui, Shuicheng Yan (2024). SuperCorrect: Supervising and Correcting Language Models with Error-Driven Insights. arXiv:2410.09008. https://arxiv.org/abs/2410.09008
Namgyu Ho, Laura Schmid, Se-Young Yun (2022). Large Language Models Are Reasoning Teachers. arXiv:2212.10071. https://arxiv.org/abs/2212.10071
Yiwei Li, Peiwen Yuan, Shaoxiong Feng, Boyuan Pan, Bin Sun, Xinglin Wang, Heda Wang, Kan Li (2023). Turning Dust into Gold: Distilling Complex Reasoning Capabilities from LLMs by Leveraging Negative Data. arXiv:2312.12832. https://arxiv.org/abs/2312.12832
Shivam Adarsh, Kumar Shridhar, Caglar Gulcehre, Nicholas Monath, Mrinmaya Sachan (2024). SIKeD: Self-guided Iterative Knowledge Distillation for mathematical reasoning. arXiv:2410.18574. https://arxiv.org/abs/2410.18574
Chengcheng Han, Xiaowei Du, Che Zhang, Yixin Lian, Xiang Li, Ming Gao, Baoyuan Wang (2023). DialCoT Meets PPO: Decomposing and Exploring Reasoning Paths in Smaller Language Models. arXiv:2310.05074. https://arxiv.org/abs/2310.05074"
30,8|7|8|7|5|7|7|7|5|7|7|5|7|7|7|5|6|4|7|7|8|5|7|6|7|7|6|7|7|6,10,8|7|7|7|5|6|7|7|7|6,source,source,"In recent years, the study of Bayesian persuasion and mediated communication has garnered significant attention, with various researchers exploring mechanisms to enhance sender commitment and optimize communication outcomes. A foundational aspect of this discourse is the concept of ex-post individually rational (ex-post IR) Bayesian persuasion, as introduced by Zhang et al. (2023). This work addresses the challenge of maintaining sender credibility in information disclosure, which is particularly relevant to our research on mediated communication with money-burning tactics. Both studies explore mechanisms to enhance sender commitment and trust, with Zhang et al.'s findings on the optimality of ex-post IR persuasion policies providing a theoretical foundation that complements our exploration of commitment power in communication. The geometric characterization of persuasion policies in their work aligns with our study's geometric interpretations of sender payoffs (Zhang et al., 2023).

Another critical theme in the literature is the notion of credibility in Bayesian persuasion, as discussed by Lin and Liu (2022). Their work focuses on the sender's inability to profit from message tampering without altering the message distribution, a concept crucial for understanding the limitations of persuasion when the sender's payoff is state-independent. This framework complements our exploration of communication mechanisms by highlighting conditions under which the sender's strategies remain credible, even when commitment is valuable. This connection is particularly relevant in contexts like Web 3.0 communities, where commitment and credibility are intertwined (Lin & Liu, 2022). Similarly, Arieli, Babichenko, and Sandomirskiy (2022) investigate the dynamics of Bayesian persuasion through a sequence of uninformed mediators, providing foundational insights into how mediators can influence communication strategies. While their study shows that mediators can sometimes improve the sender's value, our research extends this by demonstrating that money-burning can further enhance the sender's payoff, particularly in contexts where commitment is crucial (Arieli et al., 2022).

The dynamics of information transmission and sender-receiver interactions are further explored in the work of Gradwohl et al. (2020), who examine a Bayesian persuasion model with multiple senders. Their study contrasts with our research, which focuses on a single sender using money-burning tactics to enhance commitment power in mediated communication. While both studies explore the dynamics of information transmission, our work specifically addresses the sender's ability to improve payoffs through commitment mechanisms, whereas Gradwohl et al. highlight the impact of sender competition on information distribution (Gradwohl et al., 2020). In a similar vein, Kosenko (2020) examines a strategic information design game involving a sender, mediator, and receiver, providing insights into the conditions for information revelation and the impact of mediation on equilibrium outcomes. While Kosenko's work emphasizes the role of the mediator in information transmission, our research focuses on the strategic use of money-burning to enhance sender outcomes (Kosenko, 2020).

The strategic role of constraints and commitment in optimizing communication outcomes is a recurring theme in the literature. Su, Kempe, and Subramanian (2021) examine a Bayesian persuasion framework where the receiver's constraints can lead to higher utility by compelling the sender to disclose more information. This aligns with our research on mediated communication, where commitment mechanisms, such as money burning, enhance the sender's payoff by influencing information transmission. Both studies highlight the strategic role of constraints and commitment, though the cited work focuses on receiver constraints, while our work emphasizes sender commitment through money-burning tactics (Su et al., 2021). Additionally, Zhang (2022) examines verifiable disclosure games where the sender's payoff is state-independent, similar to the context of our research. Zhang's work identifies conditions under which the sender does not benefit from commitment power, complementing our findings that money-burning can enhance commitment power and improve the sender's payoff (Zhang, 2022).

Finally, the computational complexity of the Bayesian persuasion model, as examined by Dughmi and Xu (2015), provides a fundamental framework for understanding persuasive communication where a sender influences a receiver's decision through strategic information disclosure. This model is relevant to our research on mediated communication enhanced by money-burning tactics, as both studies explore the sender's optimization task in influencing receiver actions under uncertainty. While Dughmi and Xu focus on algorithmic solutions and complexity, our research extends the concept by incorporating money-burning as a commitment device, thereby enhancing the sender's payoff in communication scenarios (Dughmi & Xu, 2015). Additionally, the work of Paes Leme, Schneider, and Zheng (2023) on Bayesian conversations highlights the unique ability of mediator protocols to correlate posteriors, contrasting with unmediated conversations. This foundational understanding of mediated communication, enhanced by money-burning tactics, contributes to the broader discourse on strategic communication and mechanism design (Paes Leme et al., 2023).","Itai Arieli, Yakov Babichenko, Fedor Sandomirskiy (2022). Bayesian Persuasion with Mediators. arXiv:2203.04285. https://arxiv.org/abs/2203.04285
Xiao Lin, Ce Liu (2022). Credible Persuasion. arXiv:2205.03495. https://arxiv.org/abs/2205.03495
Kun Zhang (2022). Withholding Verifiable Information. arXiv:2206.09918. https://arxiv.org/abs/2206.09918
Ronen Gradwohl, Niklas Hahn, Martin Hoefer, Rann Smorodinsky (2020). Reaping the Informational Surplus in Bayesian Persuasion. arXiv:2006.02048. https://arxiv.org/abs/2006.02048
Shih-Tang Su, Vijay G. Subramanian (2022). Order of Commitments in Bayesian Persuasion with Partial-informed Senders. arXiv:2202.06479. https://arxiv.org/abs/2202.06479
Shaddin Dughmi, Haifeng Xu (2015). Algorithmic Bayesian Persuasion. arXiv:1503.05988. https://arxiv.org/abs/1503.05988
Shih-Tang Su, David Kempe, Vijay G. Subramanian (2021). On the benefits of being constrained when receiving signals. arXiv:2110.10909. https://arxiv.org/abs/2110.10909
Andrew Kosenko (2020). Mediated Persuasion. arXiv:2012.00098. https://arxiv.org/abs/2012.00098
Jiahao Zhang, Shuran Zheng, Renato Paes Leme, Zhiwei Steven Wu (2023). Ex-post Individually Rational Bayesian Persuasion. arXiv:2312.04973. https://arxiv.org/abs/2312.04973
Renato Paes Leme, Jon Schneider, Shuran Zheng (2023). Bayesian Conversations. arXiv:2307.08827. https://arxiv.org/abs/2307.08827"
30,4|3|3|7|7|7|6|8|6|7|3|8|3|8|5|2|3|6|5|3|7|3|3|3|3|3|7|7|7|5,10,6|8|7|3|8|3|5|8|3|3,source,source,"The study of Condorcet-consistent voting rules has been a focal point in the literature on social choice theory, particularly due to the challenges posed by paradoxes such as the reinforcement and no-show paradoxes. Dominik Peters (2017) delves into the manipulability of Condorcet-consistent voting rules, especially when voters reverse their preference rankings. This work is significant to our research as it underscores the inherent challenges faced by Condorcet extensions, akin to the paradoxes we explore for three-candidate elections. The use of SAT solvers in Peters' work to prove impossibility results parallels our approach in examining the axiomatic properties of voting rules like maximin, Nanson‚Äôs rule, and leximin (Peters, 2017).

The robustness of Minimax and its refinements in three-candidate elections is further highlighted by Wesley H. Holliday and Eric Pacuit (2023), who extend May's Theorem to three alternatives by introducing axioms addressing spoiler effects and the strong no-show paradox. Their axiomatic approach complements our findings on the suitability of Minimax and its refinements, such as Nanson‚Äôs rule and leximin, for overcoming paradoxes in three-candidate scenarios (Holliday & Pacuit, 2023). Similarly, the work by Felix Brandt, Christian Geist, and Dominik Peters (2016) on the compatibility of Condorcet-consistency and participation in voting rules provides insights into the limitations and possibilities of Condorcet-consistent rules, which align with our investigation into the reinforcement and no-show paradoxes (Brandt et al., 2016).

The tension between expansion consistency and resoluteness in voting, as explored by Wesley H. Holliday et al. (2022), highlights the challenges in designing voting rules that balance fairness axioms like anonymity and neutrality with the need for consistent candidate selection. Their findings on the incompatibility of certain axioms provide a broader context for understanding the limitations and trade-offs inherent in Condorcet-consistent choice mechanisms, particularly in three-candidate elections (Holliday et al., 2022). This theme is echoed in the work of Richard B. Darlington (2017), who argues for the necessity of Condorcet consistency in voting systems, emphasizing its role in ensuring fair election outcomes. While Darlington focuses on the broader implications of Condorcet consistency across various voting systems, our study specifically examines its susceptibility to paradoxes in three-candidate elections (Darlington, 2017).

The strategic aspects of voting rules are further explored by J√©r√¥me Lang et al. (2013), who investigate candidacy games and the existence of Nash equilibria for different voting rules. Their findings demonstrate the robustness of Condorcet-consistent rules in strategic candidacy scenarios, emphasizing their suitability in elections with a limited number of candidates (Lang et al., 2013). This complements the work of Elkind, Lang, and Saffidine (2010), who explore the distance rationalizability of Condorcet-consistent voting rules, focusing on Young's rule and Maximin rule. Their findings on the computational challenges of Young's rule provide insights into the complexities of defining consensus, aligning with our exploration of Condorcet extensions' axiomatic characterizations (Elkind et al., 2010). Additionally, the exploration of strategyproofness in social choice functions by Felix Brandt et al. (2021) and the study of consular election rules by Egor Ianovski and Mark C. Wilson (2016) further underscore the inherent trade-offs and limitations when designing voting systems that aim to be both strategyproof and Condorcet-consistent (Brandt et al., 2021; Ianovski & Wilson, 2016). These works collectively enrich the discourse on the design and evaluation of Condorcet-consistent voting rules, providing a comprehensive backdrop for our research on their paradoxes and refinements in three-candidate elections.","Wesley H. Holliday, Eric Pacuit (2023). An extension of May's Theorem to three alternatives: axiomatizing Minimax voting. arXiv:2312.14256. https://arxiv.org/abs/2312.14256
Dominik Peters (2017). Condorcet's Principle and the Preference Reversal Paradox. arXiv:1707.08760. https://arxiv.org/abs/1707.08760
Richard B. Darlington (2017). Why Condorcet Consistency is Essential. arXiv:1706.01841. https://arxiv.org/abs/1706.01841
J√©r√¥me Lang, Nicolas Maudet, Maria Polukarov, Alice Cohen-Hadria (2013). New Results on Equilibria in Strategic Candidacy. arXiv:1306.1849. https://arxiv.org/abs/1306.1849
Wesley H. Holliday, Chase Norman, Eric Pacuit, Saam Zahedian (2022). Impossibility theorems involving weakenings of expansion consistency and resoluteness in voting. arXiv:2208.06907. https://arxiv.org/abs/2208.06907
Edith Elkind, Piotr Faliszewski, Arkadii Slinko (2010). Rationalizations of Condorcet-Consistent Rules via Distances of Hamming Type. arXiv:1009.0300. https://arxiv.org/abs/1009.0300
Felix Brandt, Christian Geist, Dominik Peters (2016). Optimal Bounds for the No-Show Paradox via SAT Solving. arXiv:1602.08063. https://arxiv.org/abs/1602.08063
Felix Brandt, Martin Bullinger, Patrick Lederer (2021). On the Indecisiveness of Kelly-Strategyproof Social Choice Functions. arXiv:2102.00499. https://arxiv.org/abs/2102.00499
Egor Ianovski, Mark C. Wilson (2016). Manipulability of consular election rules. arXiv:1611.07102. https://arxiv.org/abs/1611.07102"
30,7|7|7|5|5|3|5|5|3|5|5|7|7|7|5|7|5|2|7|5|5|5|3|6|5|6|3|7|3|7,10,5|7|7|7|3|7|7|5|5|5,target,source,"The study of chaotic transport in many-body systems has garnered significant attention, with various models and approaches providing insights into the complex dynamics that govern these phenomena. Our research on a lattice of coupled cat maps, which explores the ergodic properties and diffusive transport resulting from microscopic chaos, aligns with several existing studies that investigate similar themes. For instance, Schanz and Prusty (2005) examine chaotic transport in Hamiltonian systems, specifically focusing on a billiard chain under a magnetic field. Their work highlights how deterministic chaos can manifest as a biased random walk, offering a complementary perspective to our findings on diffusive transport in coupled cat maps. The analysis of phase-space structures and transport velocities in their study provides a valuable context for understanding the complex dynamics of chaotic many-body systems, particularly in relation to higher-order transport coefficients and non-Gaussian features.

The exploration of non-Gaussian aspects of chaotic transport is further elaborated by Venegeroles and Saa (2007), who investigate two-dimensional area-preserving maps. Their focus on kurtosis and diffusion coefficients, which are analytically derived, complements our research by providing insights into the statistical properties of transport, such as the onset of the Markovian regime. This statistical approach is echoed in the work of Alus, Fishman, and Meiss (2014), who study the transport properties in the phase space of the H√©non map. Their examination of the statistical distribution of rotation numbers and fluxes through island chains offers a broader context for analyzing transport in mixed phase spaces, paralleling our investigation of diffusive transport arising from microscopic chaos.

The role of chaotic dynamics in influencing transport properties is a recurring theme in the literature. Bouchet and Woillez (2019) explore transport in Hamiltonian systems with slowly changing phase space structures, focusing on mechanisms like noise-driven transport and transport by slow deformation of chaotic regions. This study provides a broader context for our research by modeling transport in phase space through stochastic and chaotic perturbations. Similarly, the work of Markoviƒá and ƒåubroviƒá (2024) on chaotic dynamics and anomalous transport in a semiclassical Bose-Hubbard chain highlights the impact of mixed phase space on diffusion. Both studies emphasize the role of initial conditions and the universality of transport behavior, offering insights into the underlying mechanisms of transport in chaotic systems.

The transition between normal and anomalous diffusion is another critical aspect of chaotic transport, as explored by Harsoula and Contopoulos (2018) in the standard map. Their investigation into the role of phase space structures and control parameters in determining diffusion behavior complements our findings on the ballistic spread and diffusive transport in chaotic many-body systems. This theme is further developed in the work of Bouchara et al. (2015), who examine the distribution of finite-time observable averages and their connection to transport properties in low-dimensional Hamiltonian systems. Their theoretical framework, which explores the influence of eigenfunctions of the transfer operator on the distribution of Lyapunov exponents, provides a valuable complement to our study of chaotic transport in coupled cat maps.

Finally, the exploration of self-consistent chaotic transport in high-dimensional mean-field Hamiltonian models by Mugnaine et al. (2024) and the dynamics of chaotic transport in nontwist Hamiltonian systems by Basko (2010) offer additional perspectives on the role of chaos in influencing transport properties. Both studies investigate how chaotic structures, such as periodic orbits, coherent structures, and shearless curves, affect transport mechanisms. These insights into the interplay between chaos and transport dynamics enrich our understanding of diffusive transport in coupled chaotic systems, highlighting the broader applicability of chaotic dynamics in understanding transport phenomena across different systems.","M. Mugnaine, J. D. Szezech Jr., R. L. Viana, I. L. Caldas, P. J. Morrison (2024). Shearless effective barriers to chaotic transport induced by even twin islands in nontwist systems. arXiv:2406.19947. https://arxiv.org/abs/2406.19947
Mirella Harsoula, George Contopoulos (2018). Global and Local diffusion in the Standard Map. arXiv:1807.06320. https://arxiv.org/abs/1807.06320
Holger Schanz, Manamohan Prusty (2005). Directed chaos in a billiard chain with transversal magnetic field. arXiv:nlin/0509003. https://arxiv.org/abs/nlin/0509003
Or Alus, Shmuel Fishman, James D. Meiss (2014). Statistics of the Island-Around-Island Hierarchy in Hamiltonian Phase Space. arXiv:1411.1783. https://arxiv.org/abs/1411.1783
Freddy Bouchet, Eric Woillez (2019). Transport in Hamiltonian systems with slowly changing phase space structure. arXiv:1902.06309. https://arxiv.org/abs/1902.06309
Roberto Venegeroles, Alberto Saa (2007). Non-Gaussian features of chaotic Hamiltonian transport. arXiv:0711.4539. https://arxiv.org/abs/0711.4539
Lydia Bouchara, Ouerdia Ourrad, Sandro Vaienti, Xavier Leoncini (2015). Anomalous transport and observable average in the standard map. arXiv:1509.00798. https://arxiv.org/abs/1509.00798"
30,7|8|8|8|7|7|8|8|8|8|8|8|8|7|8|8|8|7|7|8|7|6|8|7|8|8|7|7|8|7,10,8|8|8|8|7|8|8|8|8|7,target,source,"In recent years, the integration of machine learning with data assimilation has emerged as a promising approach to address the challenges of modeling complex dynamical systems, particularly those exhibiting chaotic behavior. This research area has seen a variety of methodologies aimed at improving prediction accuracy without relying heavily on noise statistics, a challenge that our work also addresses through a novel topological data analysis-based algorithm. Several studies have explored hybrid modeling approaches that combine machine learning techniques with data assimilation to enhance the predictive capabilities of models. For instance, Pawar et al. (2021) and Brajard et al. (2020) both propose hybrid frameworks that integrate machine learning with data assimilation to improve predictions in chaotic systems like the Lorenz model. Pawar et al. utilize a recurrent neural network with an LSTM-based correction term to address unknown dynamics, while Brajard et al. employ an ensemble Kalman filter alongside a neural network to update surrogate models, demonstrating convergence and forecast skill up to two Lyapunov times. These studies align with our research by focusing on the integration of data assimilation and machine learning to enhance model accuracy without relying on noise statistics (Pawar et al., 2021; Brajard et al., 2020).

Another significant theme in the literature is the development of novel data assimilation frameworks that do not require precise noise statistics, which is a key aspect of our research. McCabe and Brown (2021) introduce amortized assimilation, leveraging self-supervised denoising and differentiable simulation to improve effectiveness over traditional methods. Similarly, Nguyen et al. (2019) adopt a Bayesian formulation to infer hidden dynamics and model parameters from noisy and partially observed data, using neural networks combined with data assimilation schemes. Both studies focus on refining initial condition estimates through innovative data assimilation techniques, complementing our approach of using topological data analysis to optimize model coefficients (McCabe and Brown, 2021; Nguyen et al., 2019).

The exploration of variational data assimilation and the use of learned inverse observation operators also feature prominently in the literature. Frerix et al. (2021) employ a learned inverse observation operator to enhance the initialization and optimization of chaotic systems, transforming the optimization problem into a more manageable physics space. This approach aligns with our research focus on improving data assimilation techniques by minimizing topological differences without noise information. Similarly, Bocquet et al. (2020) explore online learning of both the dynamics and state of chaotic systems using ensemble Kalman filters within a Bayesian framework, addressing the challenge of updating model estimates in real-time with new observations. Both studies highlight the potential for integrating machine learning with data assimilation to improve predictions in complex dynamical systems (Frerix et al., 2021; Bocquet et al., 2020).

The integration of machine learning with data assimilation to enhance forecast capabilities is further explored by Gottwald and Reich (2020) and Cheng et al. (2023). Gottwald and Reich introduce RAFDA, which combines random feature maps with ensemble Kalman filters to sequentially learn model parameters from noisy observations, achieving computational efficiency and improved forecast skill. Cheng et al. provide a comprehensive overview of integrating data assimilation and uncertainty quantification with machine learning techniques, addressing challenges in high-dimensional dynamical systems. Both works align with our topological data assimilation method that optimizes model coefficients to minimize topological differences between measurements and forecasts, aiming to enhance prediction accuracy in complex systems like the Lorenz model (Gottwald and Reich, 2020; Cheng et al., 2023).

Finally, the use of hybrid modeling approaches to improve data assimilation in complex dynamical systems is exemplified by Chen et al. (2024) and Blanke et al. (2024). Chen et al. introduce the Conditional Gaussian Neural Stochastic Differential Equation (CGNSDE), which combines knowledge-based models with machine learning to enhance data assimilation accuracy without relying on noise statistics. Blanke et al. propose a deep learning approach to data assimilation by modeling physical systems as sequences of Gaussian prior distributions, parametrized by neural networks. Both studies seek to enhance data assimilation techniques through different methodologies, highlighting the potential of machine learning in handling complex systems with sparse observations, similar to our use of topological data analysis to optimize model coefficients (Chen et al., 2024; Blanke et al., 2024).","Duong Nguyen, Said Ouala, Lucas Drumetz, Ronan Fablet (2019). EM-like Learning Chaotic Dynamics from Noisy and Partial Observations. arXiv:1903.10335. https://arxiv.org/abs/1903.10335
Julien Brajard, Alberto Carassi, Marc Bocquet, Laurent Bertino (2020). Combining data assimilation and machine learning to emulate a dynamical model from sparse and noisy observations: a case study with the Lorenz 96 model. arXiv:2001.01520. https://arxiv.org/abs/2001.01520
Marc Bocquet, Alban Farchi, Quentin Malartic (2020). Online learning of both state and dynamics using ensemble Kalman filters. arXiv:2006.03859. https://arxiv.org/abs/2006.03859
Chuanqi Chen, Nan Chen, Jin-Long Wu (2024). CGNSDE: Conditional Gaussian Neural Stochastic Differential Equation for Modeling Complex Systems and Data Assimilation. arXiv:2404.06749. https://arxiv.org/abs/2404.06749
Suraj Pawar, Omer San, Adil Rasheed, Ionel M. Navon (2021). A nonintrusive hybrid neural-physics modeling of incomplete dynamical systems: Lorenz equations. arXiv:2104.00114. https://arxiv.org/abs/2104.00114
Thomas Frerix, Dmitrii Kochkov, Jamie A. Smith, Daniel Cremers, Michael P. Brenner, Stephan Hoyer (2021). Variational Data Assimilation with a Learned Inverse Observation Operator. arXiv:2102.11192. https://arxiv.org/abs/2102.11192
Sibo Cheng, Cesar Quilodran-Casas, Said Ouala, Alban Farchi, Che Liu, Pierre Tandeo, Ronan Fablet, Didier Lucor, Bertrand Iooss, Julien Brajard, Dunhui Xiao, Tijana Janjic, Weiping Ding, Yike Guo, Alberto Carrassi, Marc Bocquet, Rossella Arcucci (2023). Machine learning with data assimilation and uncertainty quantification for dynamical systems: a review. arXiv:2303.10462. https://arxiv.org/abs/2303.10462
Michael McCabe, Jed Brown (2021). Learning to Assimilate in Chaotic Dynamical Systems. arXiv:2111.01058. https://arxiv.org/abs/2111.01058
Matthieu Blanke, Ronan Fablet, Marc Lelarge (2024). Neural Incremental Data Assimilation. arXiv:2406.15076. https://arxiv.org/abs/2406.15076"
30,5|6|7|6|7|7|6|6|7|4|4|3|6|5|6|7|7|5|5|3|3|7|6|5|7|3|3|5|3|5,10,6|7|5|7|5|6|3|7|5|6,target,source,"The Age of Information (AoI) has emerged as a critical metric for evaluating the timeliness of updates in various networked systems. Foundational work by Yates and Kaul (2016) introduced a general framework for assessing AoI in systems with multiple independent sources updating a monitor through simple queues. Their exploration of AoI under different queueing disciplines provides a basis for understanding AoI performance in multi-source environments, which is complementary to our focus on optimizing AoI in multi-step update processing systems with series and parallel server setups. This foundational framework informs the development of analytical models for more complex systems involving multiple computational steps (Yates and Kaul, 2016).

Further extending the analysis of AoI in networked systems, Javani et al. (2021) and Javani et al. (2019) investigate AoI in multiple-source, multiple-server networks. Their work focuses on the timeliness of updates in systems where updates arrive at servers via Poisson processes and are transmitted through queues with exponential service times. These studies provide insights into AoI dynamics in multi-server systems, which parallels our exploration of AoI in multi-step update processing systems. While their analysis primarily addresses single-step updates in parallel server setups, our research extends this by exploring multi-step processing in both series and parallel configurations, highlighting the age-power trade-off in such systems (Javani et al., 2021; Javani et al., 2019).

The exploration of AoI in single-server multi-source queueing models by Moltafet et al. (2019) and the analysis of AoI using a stochastic hybrid systems (SHS) approach by Yates (2018) provide foundational insights into AoI metrics. Moltafet et al. focus on the average AoI under a first-come first-served policy, offering exact and approximate expressions for AoI in M/M/1 and M/G/1 models. This complements our research by providing a basis for evaluating AoI in more complex systems, such as those involving multi-step processing with series and parallel configurations. Yates' work on the age distribution and convergence properties in networked systems further complements our study by offering insights into optimizing AoI in multi-step update processing systems (Moltafet et al., 2019; Yates, 2018).

The investigation of AoI in systems using the Processor Sharing (PS) discipline by Gandarias et al. (2023) and the analysis of age-delay tradeoffs in M server systems by Talak and Modiano (2019) highlight different aspects of AoI optimization. Gandarias et al. provide closed-form expressions for average AoI in M/M/1/2 queues, contrasting with the series and parallel processing setups analyzed in our research. Talak and Modiano's exploration of age-delay tradeoffs offers insights into the complexities of managing AoI in multi-step processing systems, particularly in understanding how different server configurations and scheduling policies can impact the timeliness and efficiency of updates. These studies underscore the importance of optimizing service rates under power constraints to balance age and energy efficiency (Gandarias et al., 2023; Talak and Modiano, 2019).

Finally, the work by Buyukates and Ulukus (2020) and Chen et al. (2022) further enrich the understanding of AoI in complex systems. Buyukates and Ulukus investigate AoI in systems with Gilbert-Elliot servers and samplers, focusing on the impact of Markovian state transitions on service and interarrival times. This complements our study by providing insights into AoI optimization under varying service conditions. Chen et al. address the challenges posed by out-of-order data arrivals in parallel transmission setups, offering insights into optimizing system configurations for improved timeliness. These studies contribute to a comprehensive understanding of AoI dynamics in multi-step update processing systems, emphasizing the need for novel analytical frameworks to evaluate such systems effectively (Buyukates and Ulukus, 2020; Chen et al., 2022).","Mohammad Moltafet, Markus Leinonen, Marian Codreanu (2019). On the Age of Information in Multi-Source Queueing Models. arXiv:1911.07029. https://arxiv.org/abs/1911.07029
Rajat Talak, Eytan Modiano (2019). Age-Delay Tradeoffs in Queueing Systems. arXiv:1911.05601. https://arxiv.org/abs/1911.05601
Be√±at Gandarias, Josu Doncel, Mohamad Assaad (2023). On the Age of Information of Processor Sharing Systems. arXiv:2309.02083. https://arxiv.org/abs/2309.02083
Alireza Javani, Marwen Zorgui, Zhiying Wang (2021). Age of Information for Multiple-Source Multiple-Server Networks. arXiv:2106.07247. https://arxiv.org/abs/2106.07247
Baturalp Buyukates, Sennur Ulukus (2020). Age of Information with Gilbert-Elliot Servers and Samplers. arXiv:2002.05711. https://arxiv.org/abs/2002.05711
Zhengchuan Chen, Dapeng Deng, Howard H. Yang, Nikolaos Pappas, Limei Hu, Yunjian Jia, Min Wang, Tony Q. S. Quek (2022). Analysis of Age of Information in Dual Updating Systems. arXiv:2207.00781. https://arxiv.org/abs/2207.00781
Alireza Javani, Marwen Zorgui, Zhiying Wang (2019). Age of Information in Multiple Sensing. arXiv:1902.01975. https://arxiv.org/abs/1902.01975"
30,9|7|8|8|8|8|8|8|7|7|3|8|5|8|8|5|8|8|8|8|8|8|7|7|8|8|7|6|8|8,10,9|8|8|8|7|8|8|8|8|8,tie,source,"The study of rank-metric codes has been enriched by various approaches that explore their algebraic structures and invariants, which are crucial for distinguishing different families of these codes. A significant contribution in this area is the work by Neri, Puchinger, and Horlemann-Trautmann, who introduced dimension sequences as invariants for rank-metric codes. Their research provides criteria for determining code inequivalence and characterizing Gabidulin codes, offering a complementary perspective to our geometric invariant approach (Neri et al., 2019). This work aligns with our focus on differentiating Gabidulin and twisted Gabidulin codes, as both approaches aim to enhance the understanding of code equivalence and classification through distinct invariants (Neri et al., 2019).

Further exploration into the algebraic structures of rank-metric codes is presented by Ndiaye, Kidoudou, and Kalachi, who generalize Subspace Subcodes in the rank metric. Their algorithm for constructing generator and parity-check matrices, along with bounds for code cardinalities, addresses the need for new families of rank-metric codes to overcome structural vulnerabilities (Ndiaye et al., 2023). This complements our research on geometric invariants by providing alternative methods for distinguishing Gabidulin codes from random ones. Similarly, the work by Augot, Couvreur, Lavauzelle, and Neri extends the study of rank-metric codes as subspaces of group algebras, introducing a decoding algorithm for codes with an abelian Galois group. Their exploration of multivariate skew polynomial rings and tensor rank in Gabidulin codes aligns with our investigation into algebraic structures and invariants (Augot et al., 2020).

The concept of invariants is further explored by Neri, Puchinger, and Horlemann-Trautmann, who propose a sequence of dimensions of linear spaces generated by a rank-metric code under field automorphisms as an invariant for the code's equivalence class (Neri et al., 2019). This approach, like our geometric invariant, seeks to identify and differentiate rank-metric codes, contributing to a broader understanding of code equivalence. Additionally, Bik and Neri's extension of symmetric Gabidulin codes to polynomials of higher degrees introduces essential-rank metric codes, which parallels our focus on new algebraic structures within rank-metric codes (Bik & Neri, 2023). Their efficient decoding algorithm and bounds on minimal distance and dimension offer practical insights that could inform our study of code applications.

The characterization of generator matrices in standard form for generalized Gabidulin codes by Neri introduces the concept of q-Cauchy matrices, providing a new criterion for identifying these codes (Neri, 2018). This complements our exploration of geometric invariants by enhancing the understanding of structured matrices, such as Toeplitz and Hankel, within rank-metric codes. Moreover, the work by Neri, Santonastaso, and Zullo on constructing new families of maximum rank distance (MRD) codes extends the classification of MRD codes, which is crucial for distinguishing different algebraic structures in rank-metric codes (Neri et al., 2021). Their findings on code inequivalence and equivalence classes offer insights that align with our study of geometric invariants.

Lastly, the investigation into the generic properties of linear rank-metric codes by Neri, Horlemann-Trautmann, Randrianarisoa, and Rosenthal provides a probabilistic framework for understanding the prevalence of non-Gabidulin MRD codes (Neri et al., 2016). This work complements our research by exploring the likelihood of randomly chosen generator matrices producing MRD and non-Gabidulin codes over large extension fields. Additionally, Randrianarisoa's geometric approach to rank metric codes, which introduces a simpler definition for generalized rank weights, offers a perspective similar to our investigation of the vanishing ideal of linear sets in rank-metric codes (Randrianarisoa, 2019). Both studies emphasize the importance of geometric invariants in distinguishing and classifying rank-metric codes, thereby contributing to the broader understanding of their algebraic structures and applications.","Alessandro Neri, Anna-Lena Horlemann-Trautmann, Tovohery Randrianarisoa, Joachim Rosenthal (2016). On the Genericity of Maximum Rank Distance and Gabidulin Codes. arXiv:1605.05972. https://arxiv.org/abs/1605.05972
Alessandro Neri (2018). Systematic encoders for generalized Gabidulin codes and the $q$-analogue of Cauchy matrices. arXiv:1805.06706. https://arxiv.org/abs/1805.06706
Alessandro Neri, Sven Puchinger, Anna-Lena Horlemann-Trautmann (2019). Invariants and Inequivalence of Linear Rank-Metric Codes. arXiv:1905.11326. https://arxiv.org/abs/1905.11326
Alessandro Neri, Paolo Santonastaso, Ferdinando Zullo (2021). Extending two families of maximum rank distance codes. arXiv:2104.07602. https://arxiv.org/abs/2104.07602
Ousmane Ndiaye, Peter Arnaud Kidoudou, Herv√© Tale Kalachi (2023). Generalized Subspace Subcodes in the Rank Metric. arXiv:2301.12523. https://arxiv.org/abs/2301.12523
Daniel Augot, Alain Couvreur, Julien Lavauzelle, Alessandro Neri (2020). Rank-metric codes over arbitrary Galois extensions and rank analogues of Reed-Muller codes. arXiv:2006.14489. https://arxiv.org/abs/2006.14489
Arthur Bik, Alessandro Neri (2023). Higher-degree symmetric rank-metric codes. arXiv:2303.06745. https://arxiv.org/abs/2303.06745
Alessandro Neri, Sven Puchinger, Anna-Lena Horlemann-Trautmann (2019). Equivalence and Characterizations of Linear Rank-Metric Codes Based on Invariants. arXiv:1911.13059. https://arxiv.org/abs/1911.13059"
