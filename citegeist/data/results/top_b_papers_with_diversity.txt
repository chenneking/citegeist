Input:
Large Language Models have shown impressive per- formance across a wide array of tasks involving both structured and unstructured textual data. More recently, adaptions of these models have drawn attention to their abilities to work with code across different programming languages. On this notion, different benchmarks for code generation, repair, or completion suggest that certain models have programming abilities comparable to or even surpass humans. In this work, we demonstrate that the performance on this benchmark does not translate to the innate ability of humans to appreciate the structural control flow of code. For this purpose, we extract code solutions from the Hu- manEval benchmark, which the relevant models perform very strongly on, and trace their execution path using function calls sampled from the respective test set. Using this dataset, we investigate the ability of 5 state-of-the-art LLMs to match the execution trace and find that, despite the modelâ€™s abilities to generate semantically identical code, they possess only limited ability to trace the execution path, especially for traces with increased length. We find that even the top-performing model, Gemini 1.5 Pro can only fully correctly generate the trace of 47% of HumanEval tasks. In addition, we introduce a specific subset for three key structures not, or only contained to a limited extent in Hu- manEval: Recursion, Parallel Processing, and Object Oriented Programming principles, including concepts like Inheritance and Polymorphism. Besides OOP, we show that none of the investigated models achieve an average accuracy of over 5% on the relevant traces. Aggregating these specialized parts with the ubiquitous HumanEval tasks, we present the Benchmark CoCoNUT: Code Control Flow for Navigation Understanding and Testing, which measures a models ability to trace the execu- tion of code upon relevant calls, including advanced structural components. We conclude that the current generation LLMs still need to significantly improve to enhance their code reasoning abilities. We hope our dataset can help researchers bridge this gap in the near future.

Breadth: 10
Depth: 2
Diversity: 0

Related Works:

The exploration of Large Language Models (LLMs) in the domain of code understanding and generation has been a focal point of recent research, with various studies highlighting both the potential and limitations of these models. A significant body of work has concentrated on developing benchmarks that assess the capabilities of LLMs beyond traditional metrics. For instance, Allamanis et al. (2024) introduce the concept of round-trip correctness (RTC) as an unsupervised evaluation method, which contrasts with benchmarks like HumanEval by focusing on semantic equivalence through code synthesis and editing. This approach complements our research by underscoring the need for comprehensive evaluation methods that can enhance LLMs' code reasoning capabilities, particularly in tracing execution paths and handling complex structures.

Several studies have identified the inadequacies of existing benchmarks in capturing the full spectrum of LLMs' code reasoning abilities. Dou et al. (2024) conduct an empirical study revealing challenges in handling complex problems and the tendency of LLMs to produce shorter yet more complicated code. Similarly, Yan et al. (2023) with CodeScope, and Fu et al. (2023) with CodeApex, emphasize the need for robust benchmarks that evaluate LLMs across multiple languages and tasks. These works align with our research, which critiques current benchmarks for not fully capturing the structural control flow understanding of LLMs, a gap we aim to address with the CoCoNUT benchmark.

The limitations of LLMs in comprehending code semantics, particularly dynamic behaviors, have been a recurring theme in the literature. Ma et al. (2023) focus on the challenges LLMs face in understanding both static and dynamic code behaviors, which complements our findings on the limited ability of LLMs to navigate complex code structures such as recursion and parallel processing. Liu et al. (2024) with CodeMind further highlight the struggle of LLMs with complex code reasoning tasks, reinforcing the need for improved evaluation methods that can enhance LLMs' code reasoning capabilities, a gap our research aims to bridge.

In addition to the focus on code reasoning, several studies have proposed innovative approaches to improve LLMs' programming capabilities. Zhang et al. (2023) introduce Self-Edit, a generate-and-edit approach that enhances code accuracy by using execution results to guide a fault-aware code editor. This method significantly improves pass rates on benchmarks like HumanEval, highlighting complementary aspects of enhancing LLMs' programming abilities. Similarly, Nguyen et al. (2024) with CodeMMLU and Liang et al. (2024) with REPOCOD emphasize the need for robust evaluation metrics that go beyond correctness, aligning with our research's focus on execution traceability and advanced structural components.

In summary, the current body of research underscores the limitations of LLMs in code reasoning and the inadequacy of existing benchmarks in capturing the full spectrum of programming capabilities. Our work contributes to this discourse by introducing the CoCoNUT benchmark, which specifically targets the structural control flow understanding of LLMs. By focusing on execution traceability and advanced structural components, our research complements existing studies and highlights the need for more comprehensive evaluation methods to enhance LLMs' code reasoning abilities, ultimately aiming to bridge the gap between code generation and understanding.

Citations:

Miltiadis Allamanis, Sheena Panthaplackel, Pengcheng Yin (2024). Unsupervised Evaluation of Code LLMs with Round-Trip Correctness. arXiv:2402.08699. https://arxiv.org/abs/2402.08699
Shihan Dou, Haoxiang Jia, Shenxi Wu, Huiyuan Zheng, Weikang Zhou, Muling Wu, Mingxu Chai, Jessica Fan, Caishuang Huang, Yunbo Tao, Yan Liu, Enyu Zhou, Ming Zhang, Yuhao Zhou, Yueming Wu, Rui Zheng, Ming Wen, Rongxiang Weng, Jingang Wang, Xunliang Cai, Tao Gui, Xipeng Qiu, Qi Zhang, Xuanjing Huang (2024). What's Wrong with Your Code Generated by Large Language Models? An Extensive Study. arXiv:2407.06153. https://arxiv.org/abs/2407.06153
Weixiang Yan, Haitian Liu, Yunkun Wang, Yunzhe Li, Qian Chen, Wen Wang, Tingyu Lin, Weishan Zhao, Li Zhu, Hari Sundaram, Shuiguang Deng (2023). CodeScope: An Execution-based Multilingual Multitask Multidimensional Benchmark for Evaluating LLMs on Code Understanding and Generation. arXiv:2311.08588. https://arxiv.org/abs/2311.08588
Lingyue Fu, Huacan Chai, Shuang Luo, Kounianhua Du, Weiming Zhang, Longteng Fan, Jiayi Lei, Renting Rui, Jianghao Lin, Yuchen Fang, Yifan Liu, Jingkuan Wang, Siyuan Qi, Kangning Zhang, Weinan Zhang, Yong Yu (2023). CodeApex: A Bilingual Programming Evaluation Benchmark for Large Language Models. arXiv:2309.01940. https://arxiv.org/abs/2309.01940
Wei Ma, Shangqing Liu, Zhihao Lin, Wenhan Wang, Qiang Hu, Ye Liu, Cen Zhang, Liming Nie, Li Li, Yang Liu (2023). LMs: Understanding Code Syntax and Semantics for Code Analysis. arXiv:2305.12138. https://arxiv.org/abs/2305.12138
Changshu Liu, Shizhuo Dylan Zhang, Ali Reza Ibrahimzada, Reyhaneh Jabbarvand (2024). CodeMind: A Framework to Challenge Large Language Models for Code Reasoning. arXiv:2402.09664. https://arxiv.org/abs/2402.09664
Dung Nguyen Manh, Thang Phan Chau, Nam Le Hai, Thong T. Doan, Nam V. Nguyen, Quang Pham, Nghi D. Q. Bui (2024). CodeMMLU: A Multi-Task Benchmark for Assessing Code Understanding Capabilities of CodeLLMs. arXiv:2410.01999. https://arxiv.org/abs/2410.01999
Jiasheng Zheng, Boxi Cao, Zhengzhao Ma, Ruotong Pan, Hongyu Lin, Yaojie Lu, Xianpei Han, Le Sun (2024). Beyond Correctness: Benchmarking Multi-dimensional Code Generation for Large Language Models. arXiv:2407.11470. https://arxiv.org/abs/2407.11470
Shanchao Liang, Yiran Hu, Nan Jiang, Lin Tan (2024). Can Language Models Replace Programmers? REPOCOD Says 'Not Yet'. arXiv:2410.21647. https://arxiv.org/abs/2410.21647
Kechi Zhang, Zhuo Li, Jia Li, Ge Li, Zhi Jin (2023). Self-Edit: Fault-Aware Code Editor for Code Generation. arXiv:2305.04087. https://arxiv.org/abs/2305.04087