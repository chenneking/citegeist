longlist_length,longlist_ratings,shortlist_length,shortlist_ratings,rating_source_ours,rating_ours_gpt,related_works,citations,rating_score_1,rating_score_2
30,8|7|8|6|7|7|8|7|7|7|8|8|7|7|8|7|8|8|8|8|5|7|7|7|7|8|6|8|8|8,10,7|8|8|7|8|8|8|7|8|7,target,source,"In recent years, the evaluation of Large Language Models (LLMs) in code-related tasks has garnered significant attention, with various benchmarks and methodologies being proposed to assess their capabilities. A notable approach is the introduction of round-trip correctness (RTC) as an unsupervised evaluation method, which allows for the assessment of code LLMs across diverse software domains without the need for extensive human curation. This method complements existing benchmarks like HumanEval by focusing on semantic equivalence through unit tests, highlighting the need for comprehensive evaluation methods to enhance LLMs' code reasoning capabilities (Allamanis et al., 2024). Similarly, Dou et al. (2024) conducted an empirical study revealing the limitations of LLMs in handling complex problems and proposed a real-world benchmark to better evaluate LLM performance. Both studies align with our research, which underscores the limitations of LLMs in tracing execution paths, particularly in complex code structures, and emphasizes the need for improved benchmarks like CoCoNUT to assess advanced structural components.

The need for robust benchmarks is further echoed in the work of Yan et al. (2023), who introduced CodeScope, a comprehensive benchmark designed to evaluate LLMs on code understanding and generation tasks across multiple programming languages and tasks. This benchmark addresses the limitations of existing evaluations by focusing on multilingual, multitask, and execution-based assessments. Similarly, Fu et al. (2023) introduced CodeApex, a bilingual benchmark that evaluates the programming comprehension, code generation, and correction abilities of LLMs. Both works highlight the importance of evaluating LLMs beyond mere code generation to include execution consistency and structural understanding, which is a central theme in our research with the CoCoNUT benchmark.

The challenges faced by LLMs in comprehending code semantics, particularly dynamic behaviors, are explored by Ma et al. (2023), who emphasize the models' proficiency in syntax understanding but reveal significant challenges in semantic comprehension, especially in dynamic contexts. This aligns with our findings that LLMs struggle with advanced structural components like recursion and parallel processing. Liu et al. (2024) further support this by introducing CodeMind, a framework that highlights the limitations of relying solely on test passing for assessment, underscoring the gap between code generation and reasoning. Both studies complement our research by emphasizing the need for benchmarks that test LLMs on advanced structural components.

The limitations of correctness-centric benchmarks are critiqued by Zheng et al. (2024) through the RACE benchmark, which evaluates LLMs on multiple dimensions of code quality, including readability, maintainability, correctness, and efficiency. This aligns with our research, which critiques current benchmarks for not fully capturing the nuanced capabilities of LLMs, particularly in understanding code execution paths. Similarly, Liang et al. (2024) introduced REPOCOD, a benchmark that evaluates the real-world code generation capabilities of LLMs, highlighting their limitations in achieving high pass rates compared to simpler benchmarks like HumanEval. Both studies underscore the need for more comprehensive evaluation metrics to improve LLMs' real-world applicability and code reasoning abilities.

Finally, Zhang et al. (2023) introduced a generate-and-edit approach named Self-Edit, which enhances the accuracy of code generated by LLMs for competitive programming tasks by using execution results to guide a fault-aware code editor. This method significantly improves pass rates on benchmarks like HumanEval, which is also used in our research to evaluate LLMs' code generation capabilities. While our work focuses on assessing LLMs' ability to trace execution paths and understand code control flow, the cited paper addresses improving code quality through post-processing, highlighting complementary aspects of enhancing LLMs' programming performance. Together, these studies provide a comprehensive view of the current landscape of LLM evaluation in code-related tasks, underscoring the need for more nuanced and robust benchmarks like CoCoNUT to bridge the gap between LLM performance and human-level programming skills.","Wei Ma, Shangqing Liu, Zhihao Lin, Wenhan Wang, Qiang Hu, Ye Liu, Cen Zhang, Liming Nie, Li Li, Yang Liu (2023). LMs: Understanding Code Syntax and Semantics for Code Analysis. arXiv:2305.12138. https://arxiv.org/abs/2305.12138
Shanchao Liang, Yiran Hu, Nan Jiang, Lin Tan (2024). Can Language Models Replace Programmers? REPOCOD Says 'Not Yet'. arXiv:2410.21647. https://arxiv.org/abs/2410.21647
Weixiang Yan, Haitian Liu, Yunkun Wang, Yunzhe Li, Qian Chen, Wen Wang, Tingyu Lin, Weishan Zhao, Li Zhu, Hari Sundaram, Shuiguang Deng (2023). CodeScope: An Execution-based Multilingual Multitask Multidimensional Benchmark for Evaluating LLMs on Code Understanding and Generation. arXiv:2311.08588. https://arxiv.org/abs/2311.08588
Changshu Liu, Shizhuo Dylan Zhang, Ali Reza Ibrahimzada, Reyhaneh Jabbarvand (2024). CodeMind: A Framework to Challenge Large Language Models for Code Reasoning. arXiv:2402.09664. https://arxiv.org/abs/2402.09664
Jiasheng Zheng, Boxi Cao, Zhengzhao Ma, Ruotong Pan, Hongyu Lin, Yaojie Lu, Xianpei Han, Le Sun (2024). Beyond Correctness: Benchmarking Multi-dimensional Code Generation for Large Language Models. arXiv:2407.11470. https://arxiv.org/abs/2407.11470
Miltiadis Allamanis, Sheena Panthaplackel, Pengcheng Yin (2024). Unsupervised Evaluation of Code LLMs with Round-Trip Correctness. arXiv:2402.08699. https://arxiv.org/abs/2402.08699
Kechi Zhang, Zhuo Li, Jia Li, Ge Li, Zhi Jin (2023). Self-Edit: Fault-Aware Code Editor for Code Generation. arXiv:2305.04087. https://arxiv.org/abs/2305.04087
Shihan Dou, Haoxiang Jia, Shenxi Wu, Huiyuan Zheng, Weikang Zhou, Muling Wu, Mingxu Chai, Jessica Fan, Caishuang Huang, Yunbo Tao, Yan Liu, Enyu Zhou, Ming Zhang, Yuhao Zhou, Yueming Wu, Rui Zheng, Ming Wen, Rongxiang Weng, Jingang Wang, Xunliang Cai, Tao Gui, Xipeng Qiu, Qi Zhang, Xuanjing Huang (2024). What's Wrong with Your Code Generated by Large Language Models? An Extensive Study. arXiv:2407.06153. https://arxiv.org/abs/2407.06153
Lingyue Fu, Huacan Chai, Shuang Luo, Kounianhua Du, Weiming Zhang, Longteng Fan, Jiayi Lei, Renting Rui, Jianghao Lin, Yuchen Fang, Yifan Liu, Jingkuan Wang, Siyuan Qi, Kangning Zhang, Weinan Zhang, Yong Yu (2023). CodeApex: A Bilingual Programming Evaluation Benchmark for Large Language Models. arXiv:2309.01940. https://arxiv.org/abs/2309.01940",0,0
30,8|8|8|7|5|7|3|8|7|4|7|7|6|3|7|8|7|7|7|3|3|7|6|3|6|3|6|7|3|5,10,7|8|7|5|8|6|8|3|7|3,target,target,"The increasing demand for privacy in machine learning (ML) has led to a variety of approaches aimed at safeguarding both data and model parameters during inference and training. A significant body of work has focused on ensuring client data privacy and inference verifiability, as exemplified by the vPIN framework, which employs partial homomorphic encryption and commit-and-prove SNARKs to protect client data while ensuring verifiable inference (Riasi et al., 2024). This approach complements our research by emphasizing client data privacy, whereas our work extends the privacy focus to model parameters during inference using zkSNARKs. Both studies underscore the importance of verifiable and secure ML inference, albeit from different privacy perspectives.

Zero-knowledge proof-based verifiable machine learning (ZKP-VML) technologies have been extensively surveyed to address trustworthiness issues in outsourced model training and inference (Xing et al., 2023). This survey provides a foundational understanding of the verifiability challenges and solutions in distributed ML environments, aligning with our research's focus on ensuring verifiable and tamper-proof inference using zkSNARKs. Similarly, the work on ZkAudit leverages zero-knowledge proofs to enable trustless audits of model and data properties, maintaining privacy while ensuring algorithmic transparency (Waiwitlikhit et al., 2024). Both approaches highlight the critical role of zero-knowledge proofs in enhancing trust and privacy in ML computations, though our work specifically targets inference privacy and scalability in distributed settings.

Privacy-preserving ML inference protocols have also been explored through various cryptographic techniques. The privateMDI protocol employs additive secret sharing and linearly homomorphic encryption for linear calculations, alongside garbled circuits for non-linear functions, to enhance privacy and reduce communication overhead in distributed ML inference (Dehkordi et al., 2024). This aligns with our research's goal of ensuring privacy in distributed settings, though our approach utilizes zkSNARKs for partial privacy. Additionally, the VeriSplit framework addresses privacy, confidentiality, and integrity in ML inference through masking techniques and Merkle tree-based verification, offering an alternative to our zkSNARKs-based method (Zhang et al., 2024). Both works aim to enhance trust in distributed ML environments, albeit through different cryptographic strategies.

The privacy risks associated with edge computing and federated learning have been highlighted in several studies. Malekzadeh and Gündüz (2022) demonstrate how a server can reconstruct input data from model outputs during inference, underscoring vulnerabilities in privacy-preserving inference that our approach seeks to address by ensuring partial privacy of model parameters. Similarly, the TorMentor framework enhances federated learning by decoupling data coordination and learning definition roles to ensure privacy in untrusted settings (Fung et al., 2018). While TorMentor focuses on protecting data providers' privacy during model training, our work extends privacy considerations to model parameters during inference, using zkSNARKs to ensure verifiable and tamper-proof inference.

Finally, the potential threats to privacy in ML, such as training data poisoning, have been explored in works like Truth Serum, which highlights the vulnerability of cryptographic privacy guarantees in multiparty computation protocols (Tramèr et al., 2022). This work emphasizes the importance of addressing both data integrity and privacy, suggesting that even with secure inference, training data poisoning remains a critical threat. In contrast, our research focuses on protecting model parameters during inference, complementing these findings by providing a method to enhance overall privacy in distributed ML inference. Additionally, data obfuscation methodologies have been proposed to protect training data privacy, contrasting with our focus on model parameter privacy during inference (Zhang et al., 2018). Both approaches aim to enhance privacy in machine learning, albeit targeting different aspects of the ML lifecycle.","Tianwei Zhang, Zecheng He, Ruby B. Lee (2018). Privacy-preserving Machine Learning through Data Obfuscation. arXiv:1807.01860. https://arxiv.org/abs/1807.01860
Florian Tramèr, Reza Shokri, Ayrton San Joaquin, Hoang Le, Matthew Jagielski, Sanghyun Hong, Nicholas Carlini (2022). Truth Serum: Poisoning Machine Learning Models to Reveal Their Secrets. arXiv:2204.00032. https://arxiv.org/abs/2204.00032
Clement Fung, Jamie Koerner, Stewart Grant, Ivan Beschastnikh (2018). Dancing in the Dark: Private Multi-Party Machine Learning in an Untrusted Setting. arXiv:1811.09712. https://arxiv.org/abs/1811.09712
Arman Riasi, Jorge Guajardo, Thang Hoang (2024). Privacy-Preserving Verifiable Neural Network Inference Service. arXiv:2411.07468. https://arxiv.org/abs/2411.07468
Mohammad Malekzadeh, Deniz Gunduz (2022). Vicious Classifiers: Assessing Inference-time Data Reconstruction Risk in Edge Computing. arXiv:2212.04223. https://arxiv.org/abs/2212.04223
Han Zhang, Zifan Wang, Mihir Dhamankar, Matt Fredrikson, Yuvraj Agarwal (2024). VeriSplit: Secure and Practical Offloading of Machine Learning Inferences across IoT Devices. arXiv:2406.00586. https://arxiv.org/abs/2406.00586
Suppakit Waiwitlikhit, Ion Stoica, Yi Sun, Tatsunori Hashimoto, Daniel Kang (2024). Trustless Audits without Revealing Data or Models. arXiv:2404.04500. https://arxiv.org/abs/2404.04500
Zhibo Xing, Zijian Zhang, Jiamou Liu, Ziang Zhang, Meng Li, Liehuang Zhu, Giovanni Russello (2023). Zero-knowledge Proof Meets Machine Learning in Verifiability: A Survey. arXiv:2310.14848. https://arxiv.org/abs/2310.14848",0,1
30,9|8|8|8|8|8|8|8|8|9|7|9|8|7|6|7|6|5|8|8|6|7|8|8|5|7|6|5|7|7,10,8|8|9|9|8|8|9|8|8|7,target,source,"The exploration of spiking neural networks (SNNs) in natural language processing (NLP) has gained significant traction due to their potential for energy efficiency, a key focus of our proposed SpikeDecoder model. Several studies have addressed the challenge of encoding text into spike trains, which is crucial for the development of low-power, spike-based Transformer models. For instance, Knipper et al. (2024) propose a novel method that improves upon traditional rate-coding techniques, demonstrating significant energy efficiency gains in encoding text for SNNs. This work complements our research by addressing the encoding aspect of NLP tasks in SNNs, which is essential for the effective implementation of SpikeDecoder. Similarly, Lv et al. (2024) explore the use of SNNs for text classification, introducing a ""conversion + fine-tuning"" method that achieves comparable performance to deep neural networks (DNNs) with reduced energy consumption. These studies provide valuable insights into encoding techniques and training strategies that could inform the development of our SpikeDecoder model.

The development of spike-driven language models has been a focal point in recent research, with several studies exploring the application of SNNs in language modeling. Xing et al. (2024) introduce SpikeLM, a fully spike-driven model for general language tasks, addressing the limitations of binary spikes in existing SNNs by proposing a bi-directional, elastic spike encoding mechanism. This work is relevant to our research as it highlights advancements in spike encoding to improve semantic representation and bridge the performance gap between SNNs and ANNs. Similarly, Zhu et al. (2023) present SpikeGPT, a spiking neural network model for language generation, which aligns with our research on energy-efficient SNNs for NLP. While SpikeGPT modifies the transformer block to reduce computational complexity, our work focuses on developing a spike-based decoder model, SpikeDecoder, for NLP. These studies collectively contribute to the development of energy-efficient, spike-driven models for language processing.

The challenge of maintaining accuracy in SNNs compared to their ANN counterparts is a critical issue in developing energy-efficient models like SpikeDecoder. You et al. (2024) introduce SpikeZIP-TF, a novel ANN-to-SNN conversion method that achieves equivalent accuracy to ANNs without degradation, specifically targeting Transformer-based architectures. This work is relevant to our research as it addresses the challenge of maintaining accuracy in SNNs, a key issue in developing energy-efficient models like our proposed SpikeDecoder. Additionally, Bal et al. (2024) explore extreme quantization in spiking models, specifically using a BERT-based encoder architecture, which could inform our exploration of spike-compatible normalization and embedding methods. These studies highlight complementary approaches to enhancing SNN performance in Transformer architectures.

The potential of SNNs in reducing energy consumption has been extensively explored in the context of Transformer-based models. Hu et al. (2024) provide a comprehensive survey of spiking neural networks, focusing on energy-efficient alternatives to traditional deep learning models, including Spiking Transformers. Their discussion on emerging Spiking Transformers and their application in computer vision underscores the relevance of our work in extending these concepts to NLP, particularly through the development of SpikeDecoder. Similarly, Zhou et al. (2023) introduce Spikingformer, a transformer-based spiking neural network that addresses the challenge of non-spike computations in existing SNN models by proposing a spike-driven residual learning architecture. While Spikingformer is evaluated primarily in computer vision tasks, its approach to minimizing non-spike computations and enhancing energy efficiency aligns with our goal of creating a low-power, spike-based Transformer model for NLP.

Finally, the integration of SNNs with existing transformer-based models has been explored through various methods, including knowledge distillation and synaptic plasticity simulation. Tang and Zhu (2024) introduce BrainTransformers, a Large Language Model (LLM) using SNNs, which highlights the development of SNN-compatible Transformer components and synaptic plasticity simulation. This work aligns with our exploration of spike-based alternatives in Transformer architectures. Additionally, Lv et al. (2023) present SpikeBERT, an advancement in SNNs for language tasks, employing a two-stage knowledge distillation method to transfer knowledge from BERT, achieving comparable results with significantly reduced energy consumption. These studies underscore the importance of energy-efficient models in NLP and provide valuable insights into the development of our SpikeDecoder model.","Yangfan Hu, Qian Zheng, Guoqi Li, Huajin Tang, Gang Pan (2024). Toward Large-scale Spiking Neural Networks: A Comprehensive Survey and Future Directions. arXiv:2409.02111. https://arxiv.org/abs/2409.02111
Changze Lv, Jianhan Xu, Xiaoqing Zheng (2024). Spiking Convolutional Neural Networks for Text Classification. arXiv:2406.19230. https://arxiv.org/abs/2406.19230
Changze Lv, Tianlong Li, Jianhan Xu, Chenxi Gu, Zixuan Ling, Cenyuan Zhang, Xiaoqing Zheng, Xuanjing Huang (2023). SpikeBERT: A Language Spikformer Learned from BERT with Knowledge Distillation. arXiv:2308.15122. https://arxiv.org/abs/2308.15122
Kang You, Zekai Xu, Chen Nie, Zhijie Deng, Qinghai Guo, Xiang Wang, Zhezhi He (2024). SpikeZIP-TF: Conversion is All You Need for Transformer-based SNN. arXiv:2406.03470. https://arxiv.org/abs/2406.03470
Chenlin Zhou, Liutao Yu, Zhaokun Zhou, Zhengyu Ma, Han Zhang, Huihui Zhou, Yonghong Tian (2023). Spikingformer: Spike-driven Residual Learning for Transformer-based Spiking Neural Network. arXiv:2304.11954. https://arxiv.org/abs/2304.11954
Xingrun Xing, Zheng Zhang, Ziyi Ni, Shitao Xiao, Yiming Ju, Siqi Fan, Yequan Wang, Jiajun Zhang, Guoqi Li (2024). SpikeLM: Towards General Spike-Driven Language Modeling via Elastic Bi-Spiking Mechanisms. arXiv:2406.03287. https://arxiv.org/abs/2406.03287
Rui-Jie Zhu, Qihang Zhao, Guoqi Li, Jason K. Eshraghian (2023). SpikeGPT: Generative Pre-trained Language Model with Spiking Neural Networks. arXiv:2302.13939. https://arxiv.org/abs/2302.13939
Malyaban Bal, Yi Jiang, Abhronil Sengupta (2024). Exploring Extreme Quantization in Spiking Language Models. arXiv:2405.02543. https://arxiv.org/abs/2405.02543
Zhengzheng Tang, Eva Zhu (2024). BrainTransformers: SNN-LLM. arXiv:2410.14687. https://arxiv.org/abs/2410.14687",0,1
30,8|8|8|8|8|8|9|8|8|8|8|8|8|8|8|7|8|8|8|8|7|8|7|8|8|8|7|8|7|5,10,8|8|7|8|8|8|8|8|8|8,target,target,"The field of Multimodal Large Language Models (MLLMs) has seen significant advancements, particularly in extending their capabilities from image to video understanding. This transition is marked by the development of models like MiniGPT4-Video, which integrates visual and textual tokens to enhance video comprehension, aligning with our research focus on enriching instruction diversity through synthesized video-like samples (Kirolos Ataallah et al., 2024). Similarly, VideoGPT+ employs both image and video encoders to capture spatial and temporal details, addressing the limitations of existing video LMMs and complementing our T2Vid method aimed at improving learning efficiency (Muhammad Maaz et al., 2024). These works collectively underscore the importance of multimodal integration in advancing video understanding.

A critical challenge in video understanding is the handling of long video contexts, which has been addressed by several studies. LVBench provides a benchmark for evaluating MLLMs' capabilities in long video comprehension, highlighting the limitations of current models in extended temporal contexts, which resonates with our findings on zero-shot inference limitations (Weihan Wang et al., 2024). LongVILA extends video frame processing significantly, achieving performance improvements that complement our approach of synthesizing video-like samples to enhance training efficiency (Fuzhao Xue et al., 2024). These studies, along with the comprehensive review by Heqing Zou et al. (2024), emphasize the need for models to manage fine-grained spatiotemporal details and long-term dependencies, aligning with our research's emphasis on improving temporal understanding capabilities.

Innovative data handling techniques have been proposed to overcome the challenges of limited generalization and temporal understanding in video data. The Memory-Augmented Large Multimodal Model (MA-LMM) processes video frames sequentially and stores historical data in a memory bank, addressing context length constraints and GPU memory limits (Bo He et al., 2024). This approach complements our exploration of fine-tuning pre-trained image-LLMs for video understanding. Similarly, Kangaroo's data curation system and curriculum training pipeline emphasize the importance of high-quality datasets and innovative training strategies, paralleling our T2Vid method for instruction diversity (Jiajun Liu et al., 2024).

Architectural innovations also play a crucial role in enhancing video understanding. Video-CCAM employs causal cross-attention masks to maintain temporal consistency and performance across varying video lengths, aligning with our focus on improving temporal understanding and efficiency (Jiajun Fei et al., 2024). The integration of video-specific information into LLM-based frameworks, as explored by Kanchana Ranasinghe et al. (2024), achieves state-of-the-art performance by leveraging object-centric modalities and natural language fusion, which complements our data augmentation strategy to enrich instruction diversity.

Finally, the adaptation of image-language pre-trained models for video understanding is a shared focus in the field. The work by Lin Xu et al. (2024) identifies performance issues in directly fine-tuning these models with video data and proposes a pooling strategy to mitigate biases in visual features. This aligns with our findings on the limitations of zero-shot inference and the need for diverse instruction data in fine-tuning. Collectively, these studies contribute to advancing the capabilities of MLLMs in video understanding, each offering unique methodologies to address the inherent challenges of this domain.","Bo He, Hengduo Li, Young Kyun Jang, Menglin Jia, Xuefei Cao, Ashish Shah, Abhinav Shrivastava, Ser-Nam Lim (2024). MA-LMM: Memory-Augmented Large Multimodal Model for Long-Term Video Understanding. arXiv:2404.05726. https://arxiv.org/abs/2404.05726
Jiajun Fei, Dian Li, Zhidong Deng, Zekun Wang, Gang Liu, Hui Wang (2024). Video-CCAM: Enhancing Video-Language Understanding with Causal Cross-Attention Masks for Short and Long Videos. arXiv:2408.14023. https://arxiv.org/abs/2408.14023
Jiajun Liu, Yibing Wang, Hanghang Ma, Xiaoping Wu, Xiaoqi Ma, Xiaoming Wei, Jianbin Jiao, Enhua Wu, Jie Hu (2024). Kangaroo: A Powerful Video-Language Model Supporting Long-context Video Input. arXiv:2408.15542. https://arxiv.org/abs/2408.15542
Fuzhao Xue, Yukang Chen, Dacheng Li, Qinghao Hu, Ligeng Zhu, Xiuyu Li, Yunhao Fang, Haotian Tang, Shang Yang, Zhijian Liu, Ethan He, Hongxu Yin, Pavlo Molchanov, Jan Kautz, Linxi Fan, Yuke Zhu, Yao Lu, Song Han (2024). LongVILA: Scaling Long-Context Visual Language Models for Long Videos. arXiv:2408.10188. https://arxiv.org/abs/2408.10188
Kanchana Ranasinghe, Xiang Li, Kumara Kahatapitiya, Michael S. Ryoo (2024). Understanding Long Videos with Multimodal Language Models. arXiv:2403.16998. https://arxiv.org/abs/2403.16998
Heqing Zou, Tianze Luo, Guiyang Xie, Victor, Zhang, Fengmao Lv, Guangcong Wang, Juanyang Chen, Zhuochen Wang, Hansheng Zhang, Huaijian Zhang (2024). From Seconds to Hours: Reviewing MultiModal Large Language Models on Comprehensive Long Video Understanding. arXiv:2409.18938. https://arxiv.org/abs/2409.18938
Lin Xu, Yilin Zhao, Daquan Zhou, Zhijie Lin, See Kiong Ng, Jiashi Feng (2024). PLLaVA : Parameter-free LLaVA Extension from Images to Videos for Video Dense Captioning. arXiv:2404.16994. https://arxiv.org/abs/2404.16994
Weihan Wang, Zehai He, Wenyi Hong, Yean Cheng, Xiaohan Zhang, Ji Qi, Xiaotao Gu, Shiyu Huang, Bin Xu, Yuxiao Dong, Ming Ding, Jie Tang (2024). LVBench: An Extreme Long Video Understanding Benchmark. arXiv:2406.08035. https://arxiv.org/abs/2406.08035
Kirolos Ataallah, Xiaoqian Shen, Eslam Abdelrahman, Essam Sleiman, Deyao Zhu, Jian Ding, Mohamed Elhoseiny (2024). MiniGPT4-Video: Advancing Multimodal LLMs for Video Understanding with Interleaved Visual-Textual Tokens. arXiv:2404.03413. https://arxiv.org/abs/2404.03413
Muhammad Maaz, Hanoona Rasheed, Salman Khan, Fahad Khan (2024). VideoGPT+: Integrating Image and Video Encoders for Enhanced Video Understanding. arXiv:2406.09418. https://arxiv.org/abs/2406.09418",0,0
30,7|5|5|7|8|7|7|3|7|7|7|7|7|7|4|8|7|8|7|6|7|6|8|8|3|7|7|7|7|8,10,7|8|7|7|5|7|7|5|7|4,source,source,"The field of enhancing reasoning capabilities in large language models (LLMs) has seen significant advancements, with various approaches exploring different methodologies to improve model performance on complex reasoning tasks. A prominent theme in recent research is the optimization of reasoning paths and the structuring of reasoning processes. Reasoning Paths Optimization (RPO) and the Reverse-Enhanced Thinking (REVTHINK) framework both aim to enhance reasoning by addressing complex problem-solving challenges. While RPO focuses on optimizing diverse reasoning paths without large-scale human annotations (Chia et al., 2024), REVTHINK emphasizes reverse thinking through structured forward-backward reasoning. These complementary strategies highlight the potential of diverse reasoning paths and structured reasoning to improve LLM capabilities.

Another significant area of research involves the distillation of reasoning capabilities from large to smaller models. The Socratic CoT method and SIKeD both focus on transferring multistep reasoning skills to smaller models. Socratic CoT achieves this by decomposing problems into subproblems, similar to REVTHINK's use of structured reasoning processes (Shridhar et al., 2022). SIKeD, on the other hand, employs strategy diversity and self-guided iterative training to address the bias towards single strategies in smaller models (Adarsh et al., 2024). Both methods, along with REVTHINK, demonstrate the effectiveness of structured reasoning frameworks in enhancing the reasoning capabilities of smaller models.

The importance of internal thought processes and reasoning consistency is further explored in methods like Quiet-STaR and RECKONING. Quiet-STaR enhances reasoning by generating rationales for each token, focusing on implicit reasoning in arbitrary text (Zelikman et al., 2024). In contrast, RECKONING improves reasoning robustness by encoding contextual knowledge into model parameters, thereby enhancing generalization to longer reasoning chains (Chen et al., 2023). While Quiet-STaR and RECKONING focus on different aspects of reasoning, they both align with REVTHINK's goal of leveraging internal thought processes to improve reasoning performance.

Data augmentation and the use of diverse data sources are also critical themes in recent research. ThoughtSource and the use of negative data in distillation both aim to enhance reasoning by leveraging diverse data. ThoughtSource focuses on enhancing chain-of-thought reasoning through a meta-dataset and software library (Ott et al., 2023), while the use of negative data complements positive data in distilling reasoning capabilities, improving model specialization and reasoning abilities (Li et al., 2023). These approaches, along with REVTHINK's structured forward-backward reasoning, underscore the potential of innovative data augmentation techniques in advancing LLM reasoning abilities.

Finally, the exploration of alternative reasoning paths and adaptation strategies is evident in works like BRAINTEASER and the exploration of strategies for commonsense reasoning. BRAINTEASER evaluates lateral thinking in language models, challenging default commonsense associations and exploring alternative reasoning paths (Jiang et al., 2023). Similarly, strategies for adapting pre-trained models to commonsense reasoning tasks, such as prefix-tuning and autoprompt, maintain model robustness and generalization by leveraging pre-trained knowledge (Ma et al., 2021). These works, alongside REVTHINK, highlight the importance of exploring diverse reasoning paths and adaptation strategies to enhance model reasoning capabilities and generalization.","Simon Ott, Konstantin Hebenstreit, Valentin Liévin, Christoffer Egeberg Hother, Milad Moradi, Maximilian Mayrhauser, Robert Praas, Ole Winther, Matthias Samwald (2023). ThoughtSource: A central hub for large language model reasoning data. arXiv:2301.11596. https://arxiv.org/abs/2301.11596
Yifan Jiang, Filip Ilievski, Kaixin Ma, Zhivar Sourati (2023). BRAINTEASER: Lateral Thinking Puzzles for Large Language Models. arXiv:2310.05057. https://arxiv.org/abs/2310.05057
Shivam Adarsh, Kumar Shridhar, Caglar Gulcehre, Nicholas Monath, Mrinmaya Sachan (2024). SIKeD: Self-guided Iterative Knowledge Distillation for mathematical reasoning. arXiv:2410.18574. https://arxiv.org/abs/2410.18574
Zeming Chen, Gail Weiss, Eric Mitchell, Asli Celikyilmaz, Antoine Bosselut (2023). RECKONING: Reasoning through Dynamic Knowledge Encoding. arXiv:2305.06349. https://arxiv.org/abs/2305.06349
Eric Zelikman, Georges Harik, Yijia Shao, Varuna Jayasiri, Nick Haber, Noah D. Goodman (2024). Quiet-STaR: Language Models Can Teach Themselves to Think Before Speaking. arXiv:2403.09629. https://arxiv.org/abs/2403.09629
Yiwei Li, Peiwen Yuan, Shaoxiong Feng, Boyuan Pan, Bin Sun, Xinglin Wang, Heda Wang, Kan Li (2023). Turning Dust into Gold: Distilling Complex Reasoning Capabilities from LLMs by Leveraging Negative Data. arXiv:2312.12832. https://arxiv.org/abs/2312.12832
Kumar Shridhar, Alessandro Stolfo, Mrinmaya Sachan (2022). Distilling Reasoning Capabilities into Smaller Language Models. arXiv:2212.00193. https://arxiv.org/abs/2212.00193
Kaixin Ma, Filip Ilievski, Jonathan Francis, Satoru Ozaki, Eric Nyberg, Alessandro Oltramari (2021). Exploring Strategies for Generalizable Commonsense Reasoning with Pre-trained Models. arXiv:2109.02837. https://arxiv.org/abs/2109.02837",0,0
30,6|7|7|7|5|5|5|7|4|7|7|3|8|7|6|7|5|8|7|7|3|5|7|7|5|7|7|7|7|3,10,7|7|7|6|7|7|5|7|4|5,target,target,"In recent years, the study of Bayesian persuasion has garnered significant attention, with various researchers exploring different facets of strategic communication. A foundational work by Dughmi and Xu (2015) delves into the computational complexity of the Bayesian persuasion model, focusing on the sender's optimization task across various input models. This work provides a crucial understanding of sender-receiver dynamics, which my research extends by incorporating money-burning tactics to enhance commitment power, particularly within Web 3.0 communities. While Dughmi and Xu address algorithmic challenges in traditional Bayesian persuasion, my study explores how these principles can be applied and enhanced through strategic communication mechanisms (Dughmi & Xu, 2015).

The notion of credibility in Bayesian persuasion is another critical area of exploration. Lin and Liu (2022) introduce a concept of credibility where the sender cannot profit from message tampering while maintaining the original message distribution. This contrasts with my research, which demonstrates that money-burning can improve the sender's payoff even when the sender's preferences are state-independent. Similarly, Zhang et al. (2023) address sender credibility by introducing ex-post individually rational Bayesian persuasion, ensuring that the sender never sends a signal leading to a worse outcome than no disclosure. Both studies contribute to understanding strategic communication, with my work extending these insights to Web 3.0 communities by focusing on commitment power through money-burning tactics (Lin & Liu, 2022; Zhang et al., 2023).

The dynamics of multiple senders and the role of competition in Bayesian persuasion have also been explored. Gradwohl et al. (2020) examine a model with multiple senders, where the receiver captures all informational surplus when senders are uncertain about each other's preferences. This contrasts with my focus on a single sender using money-burning tactics to enhance commitment power in a non-competitive setting. Su and Subramanian (2022) further investigate the impact of commitment order in games with multiple senders, highlighting the strategic importance of commitment power, akin to the money-burning tactics I explore. Both studies underscore the significance of commitment strategies in influencing equilibrium outcomes, albeit in different contexts (Gradwohl et al., 2020; Su & Subramanian, 2022).

The role of mediators and indirect communication channels in Bayesian persuasion is another area of interest. Arieli et al. (2022) explore the dynamics of strategic information transmission through a sequence of uninformed mediators, contrasting with direct persuasion models by introducing constraints from mediated communication. This work complements my research by providing insights into how indirect communication channels, similar to those in Web 3.0 communities, can influence commitment power and the effectiveness of persuasion tactics. Arieli et al. (2022) also investigate a mediated communication framework with two informed senders and a receiver, highlighting the limitations of non-commitment settings and the potential benefits of mediated communication, aligning with my exploration of money-burning tactics (Arieli, Babichenko, & Sandomirskiy, 2022; Arieli, Geffner, & Tennenholtz, 2022).

Finally, the challenges of persuasion under uncertainty and the role of platforms in information disclosure are explored by Che et al. (2020) and Arieli et al. (2023). Che et al. examine a dynamic model where information generation and processing are costly, contrasting with traditional models that assume known distributions. This complements my investigation into how money-burning tactics can enhance commitment power in communication. Arieli et al. focus on a two-stage model where a third-party platform controls information, emphasizing the platform's role in information disclosure and reputation management. Both studies provide a broader context for understanding the commitment value in communication, relevant to my model's application in Web 3.0 communities (Che, Kim, & Mierendorff, 2020; Arieli, Madmon, & Tennenholtz, 2023).","Xiao Lin, Ce Liu (2022). Credible Persuasion. arXiv:2205.03495. https://arxiv.org/abs/2205.03495
Yeon-Koo Che, Kyungmin Kim, Konrad Mierendorff (2020). Keeping the Listener Engaged: a Dynamic Model of Bayesian Persuasion. arXiv:2003.07338. https://arxiv.org/abs/2003.07338
Shih-Tang Su, Vijay G. Subramanian (2022). Order of Commitments in Bayesian Persuasion with Partial-informed Senders. arXiv:2202.06479. https://arxiv.org/abs/2202.06479
Ronen Gradwohl, Niklas Hahn, Martin Hoefer, Rann Smorodinsky (2020). Reaping the Informational Surplus in Bayesian Persuasion. arXiv:2006.02048. https://arxiv.org/abs/2006.02048
Shaddin Dughmi, Haifeng Xu (2015). Algorithmic Bayesian Persuasion. arXiv:1503.05988. https://arxiv.org/abs/1503.05988
Jiahao Zhang, Shuran Zheng, Renato Paes Leme, Zhiwei Steven Wu (2023). Ex-post Individually Rational Bayesian Persuasion. arXiv:2312.04973. https://arxiv.org/abs/2312.04973
Itai Arieli, Yakov Babichenko, Fedor Sandomirskiy (2022). Bayesian Persuasion with Mediators. arXiv:2203.04285. https://arxiv.org/abs/2203.04285
Itai Arieli, Ivan Geffner, Moshe Tennenholtz (2022). Mediated Cheap Talk Design (with proofs). arXiv:2211.14670. https://arxiv.org/abs/2211.14670
Itai Arieli, Omer Madmon, Moshe Tennenholtz (2023). Reputation-based Persuasion Platforms. arXiv:2305.16694. https://arxiv.org/abs/2305.16694",0,0
30,3|6|8|3|7|5|2|5|8|8|5|7|7|3|3|2|5|7|7|8|2|3|3|7|3|5|4|2|2|3,10,6|8|5|3|8|8|5|2|2|7,target,source,"The study of Condorcet extensions and their susceptibility to paradoxes has been a significant area of research in voting theory, with various scholars exploring different aspects of these voting rules. A central theme in this body of work is the challenge of maintaining Condorcet consistency while addressing strategic manipulation and paradoxes. Dominik Peters (2017) delves into the manipulability of Condorcet-consistent voting rules through preference reversals, particularly in scenarios with at least four alternatives. This work complements our research by highlighting the broader susceptibility of Condorcet extensions to strategic issues, reinforcing the need for robust voting rule refinements like maximin. Similarly, Richard B. Darlington (2017) emphasizes the necessity of Condorcet consistency in voting systems, arguing for majority rule as a standard due to its resistance to strategic insincere voting. Both studies underscore the critical role of Condorcet consistency in ensuring fair election outcomes, aligning with our focus on paradoxes in three-candidate elections.

The exploration of axiomatic characterizations and the probability of a Condorcet winner in three-candidate elections is another significant area of research. M. S. Krishnamoorthy and M. Raghavachari (2005) provide a comprehensive analysis of the probability of a Condorcet winner, highlighting the distinctiveness of the Minimax voting rule in maintaining Condorcet consistency and immunity to spoilers. This aligns with our findings on the rule's immunity to the reinforcement paradox with a limited number of voters. Additionally, the paper's exploration of axiomatic characterizations of voting methods complements our axiomatic characterizations of maximin and its refinements, reinforcing the suitability of these methods for three-candidate elections. Wesley H. Holliday and Eric Pacuit (2023) extend May's Theorem to three alternatives, introducing axioms that mitigate spoiler effects and avoid the strong no-show paradox, ultimately characterizing the Minimax method as a solution. This further supports our research on the significance of Minimax in addressing the no-show paradox.

The development of alternative voting methods to address paradoxes like the reinforcement and no-show paradoxes is also a key focus in the literature. Wesley H. Holliday and Eric Pacuit (2020) introduce the Split Cycle method, which adheres to a set of axioms, including a weakened version of the Independence of Irrelevant Alternatives. This method offers insights into designing voting rules that maintain Condorcet consistency while potentially avoiding the pitfalls identified in our study of maximin and other Condorcet extensions. Similarly, Felix Brandt, Christian Geist, and Dominik Peters (2016) explore the compatibility of Condorcet-consistent voting rules with the participation criterion, introducing the Split Cycle method as a potential avenue for refining Condorcet extensions to mitigate paradoxes like the no-show paradox in smaller elections. These studies provide valuable perspectives on alternative methods that could enhance the robustness of Condorcet extensions.

The tension between consistency, fairness, and strategic considerations in voting methods is another critical theme in the literature. Wesley H. Holliday, Chase Norman, Eric Pacuit, and Saam Zahedian (2022) explore the conflicts between expansion consistency and fairness conditions like anonymity and neutrality, highlighting the challenges in designing voting rules that balance these aspects. This is relevant to our research on Condorcet extensions, as it underscores the challenges in designing voting rules that balance consistency and fairness, similar to the paradoxes faced by Condorcet extensions. The use of formal verification and SAT solving in their study offers methodological insights that could be applied to further analyze the axiomatic properties of voting rules in our research. Additionally, Dominik Peters (2021) examines the tradeoff between proportionality and strategyproofness in multiwinner voting rules, addressing fundamental paradoxes and limitations in voting systems. Both studies contribute to the broader understanding of the complexities and paradoxes inherent in voting rule design.

Finally, the exploration of alternative majority rule philosophies and their implications for Condorcet extensions is another area of interest. Ross Hyman, Deb Otis, Seamus Allen, and Greg Dennis (2023) introduce the core support criterion, contrasting it with the Condorcet criterion by focusing on a different majority rule philosophy, as exemplified by Instant Runoff Voting (IRV). This criterion highlights the importance of freedom of association in determining which ballots to count in pairwise elections, offering a perspective that may address some of the paradoxes associated with Condorcet extensions, such as the reinforcement and no-show paradoxes discussed in our research. The practical context provided by their exploration of IRV's potential advantages in specific political contexts offers valuable insights for evaluating the applicability of Condorcet-consistent rules, which is relevant to our investigation of Condorcet extensions' susceptibility to paradoxes.","Wesley H. Holliday, Eric Pacuit (2020). Axioms for Defeat in Democratic Elections. arXiv:2008.08451. https://arxiv.org/abs/2008.08451
Wesley H. Holliday, Eric Pacuit (2023). An extension of May's Theorem to three alternatives: axiomatizing Minimax voting. arXiv:2312.14256. https://arxiv.org/abs/2312.14256
M. S. Krishnamoorthy, M. Raghavachari (2005). Condorcet Winner Probabilities - A Statistical Perspective. arXiv:math/0511140. https://arxiv.org/abs/math/0511140
Felix Brandt, Christian Geist, Dominik Peters (2016). Optimal Bounds for the No-Show Paradox via SAT Solving. arXiv:1602.08063. https://arxiv.org/abs/1602.08063
Dominik Peters (2017). Condorcet's Principle and the Preference Reversal Paradox. arXiv:1707.08760. https://arxiv.org/abs/1707.08760
Richard B. Darlington (2017). Why Condorcet Consistency is Essential. arXiv:1706.01841. https://arxiv.org/abs/1706.01841
Wesley H. Holliday, Chase Norman, Eric Pacuit, Saam Zahedian (2022). Impossibility theorems involving weakenings of expansion consistency and resoluteness in voting. arXiv:2208.06907. https://arxiv.org/abs/2208.06907
Dominik Peters (2021). Proportionality and Strategyproofness in Multiwinner Elections. arXiv:2104.08594. https://arxiv.org/abs/2104.08594
Ross Hyman, Deb Otis, Seamus Allen, Greg Dennis (2023). A Majority Rule Philosophy for Instant Runoff Voting. arXiv:2308.08430. https://arxiv.org/abs/2308.08430",0,1
30,7|7|8|8|7|7|8|7|8|7|3|6|5|8|3|7|5|7|7|5|5|7|7|7|5|7|8|5|4|7,10,8|7|8|7|7|7|8|7|8|8,target,source,"The study of chaotic dynamics and transport phenomena in coupled map lattices has garnered significant attention, with various works exploring the intricate interplay between chaos, diffusion, and system configurations. A foundational contribution to this field is the work by Minos Axenides, Emmanuel Floratos, and Stam Nicolis, which constructs Arnol'd cat map lattice field theories and delves into their chaotic properties through correlation functions. This research provides a theoretical framework for understanding the chaotic dynamics and ergodic properties of coupled cat maps, which are crucial for analyzing the ballistic spread of perturbations and diffusive transport in phase space (Axenides et al., 2022). Similarly, P. Muruganandam and M. Senthilvelan's exploration of coupled map lattices (CMLs) with a focus on Arnol'd cat maps offers insights into the role of symmetries and chaotic properties in lattice systems. Their investigation into the spatial spread of out-of-time-ordered correlators (OTOCs) and the use of Lyapunov exponents complements the study of chaos-induced diffusion in phase space (Muruganandam & Senthilvelan, 2021).

Furthering the understanding of chaotic dynamics, Xu-Yao Hu and Vladimir Rosenhaus explore high-dimensional chaotic dynamics using a one-dimensional lattice of coupled cat maps, focusing on diffusive coupling and spatiotemporal chaos. Their computational approach and analysis of Lyapunov exponents align with the determination of diffusion from microscopic chaos, enhancing the understanding of chaotic dynamics and transport phenomena in coupled map lattices (Hu & Rosenhaus, 2022). In a related vein, Luis G. Moyano, Ana P. Majtey, and Constantino Tsallis investigate a system of globally coupled standard maps, focusing on the scaling behavior of the largest Lyapunov exponent and the presence of metastable states. This study provides insights into the behavior of Lyapunov exponents and chaos in coupled map systems, similar to the lattice of coupled cat maps, and highlights the role of chaos in determining system dynamics (Moyano et al., 2006).

The dynamics of covariant Lyapunov vectors (CLVs) and exponents in high-dimensional chaos are explored by Johnathon Barbish and Mark Paul, who focus on the spatial localization and delocalization of CLVs under the influence of a conservation law. This study is relevant to understanding the spread of perturbations in chaotic systems and complements research on the ergodic properties and diffusive transport in a lattice of coupled cat maps (Barbish & Paul, 2023). Henok Tenaw Moges, Thanos Manos, and Charalampos Skokos further investigate the diffusion and chaos properties of single and coupled standard maps, focusing on anomalous diffusion caused by accelerator modes. Their exploration of how different configurations of coupled maps can suppress anomalous transport and lead to normal diffusion underscores the broader theme of understanding diffusion from microscopic chaos in dynamical systems (Moges et al., 2021).

The role of diffusive coupling in spatiotemporal chaos is examined by A. Raj and M. R. Paul, who analyze how increasing diffusion strength affects the leading Lyapunov exponent and the localization of CLVs. Their work provides a detailed examination of how diffusion influences chaotic transport and stability in coupled map lattices, which is relevant to understanding the diffusion inferred from microscopic chaos in models of coupled cat maps (Raj & Paul, 2024). F. Ginelli, R. Livi, and A. Politi's investigation into the dynamics of diffusive motion in chains of coupled symplectic maps reveals subdiffusive behavior in both weakly and strongly chaotic regimes. This aligns with research on the ergodic properties and diffusive transport in coupled cat maps, as both studies explore how chaos and coupling influence diffusion in spatially extended systems (Ginelli et al., 2001).

Finally, the propagation of localized perturbations in chaotic coupled map lattices with long-range couplings is investigated by Alessandro Torcini and Stefano Lepri, who find that perturbations spread exponentially with a rate linked to the Lyapunov exponent and coupling power. This complements research on coupled cat maps, where perturbations spread ballistically, and the chaotic dynamics lead to diffusive transport in phase space (Torcini & Lepri, 1996). Mario Mulansky, Karsten Ahnert, Arkady Pikovsky, and Dima Shepelyansky's study of chaos in one-dimensional nonlinear Hamiltonian lattices with weak coupling reveals that the measure of chaos is proportional to the coupling strength and lattice length. Their analysis of energy spreading and ergodicity in large lattices complements findings on the ballistic spread of perturbations and the resulting diffusive transport (Mulansky et al., 2011). Collectively, these works provide a comprehensive understanding of the mechanisms driving chaotic transport and diffusion in coupled map lattices, offering valuable insights into the role of microscopic chaos in macroscopic diffusion.","Xu-Yao Hu, Vladimir Rosenhaus (2022). Correlation functions in linear chaotic maps. arXiv:2204.13655. https://arxiv.org/abs/2204.13655
P. Muruganandam, M. Senthilvelan (2021). Manifestation of strange nonchaotic attractors in extended systems: A study through out-of-time-ordered correlators. arXiv:2109.07412. https://arxiv.org/abs/2109.07412
Henok Tenaw Moges, Thanos Manos, Charalampos Skokos (2021). Anomalous diffusion in single and coupled standard maps with extensive chaotic phase spaces. arXiv:2107.14635. https://arxiv.org/abs/2107.14635
Luis G. Moyano, Ana P. Majtey, Constantino Tsallis (2006). Weak chaos and metastability in a symplectic system of many long-range-coupled standard maps. arXiv:cond-mat/0602513. https://arxiv.org/abs/cond-mat/0602513
Minos Axenides, Emmanuel Floratos, Stam Nicolis (2022). Arnol'd cat map lattices. arXiv:2208.03267. https://arxiv.org/abs/2208.03267
F. Ginelli, R. Livi, A. Politi (2001). Emergence of chaotic behaviour in linearly stable systems. arXiv:nlin/0102005. https://arxiv.org/abs/nlin/0102005
Alessandro Torcini, Stefano Lepri (1996). Disturbance propagation in chaotic extended systems with long-range coupling. arXiv:chao-dyn/9609003. https://arxiv.org/abs/chao-dyn/9609003
Johnathon Barbish, Mark Paul (2023). Using Covariant Lyapunov Vectors to Quantify High Dimensional Chaos with a Conservation Law. arXiv:2303.13977. https://arxiv.org/abs/2303.13977
Mario Mulansky, Karsten Ahnert, Arkady Pikovsky, Dima Shepelyansky (2011). Strong and weak chaos in weakly nonintegrable many-body Hamiltonian systems. arXiv:1103.2634. https://arxiv.org/abs/1103.2634
A. Raj, M. R. Paul (2024). Exploring the role of diffusive coupling in spatiotemporal chaos. arXiv:2410.03872. https://arxiv.org/abs/2410.03872",0,1
30,7|8|8|8|8|8|8|8|8|7|8|7|8|8|8|8|8|7|8|7|7|8|8|8|8|7|8|7|8|8,10,8|8|7|8|7|8|8|8|8|8,target,source,"In recent years, the integration of data assimilation and machine learning has emerged as a promising approach to address the challenges of modeling chaotic dynamical systems from sparse and noisy data. Several studies have explored this synergy, each contributing unique methodologies and insights. Nguyen et al. (2019) and Brajard et al. (2020) both focus on enhancing the prediction and understanding of chaotic systems like the Lorenz model. Nguyen et al. employ a Bayesian formulation with Expectation-Maximization-like procedures to infer hidden dynamics and model parameters, while Brajard et al. combine an ensemble Kalman filter with a neural network to create a data-driven surrogate model. Both approaches rely on known noise statistics, contrasting with my work, which minimizes topological differences without such reliance (Nguyen et al., 2019; Brajard et al., 2020).

The challenge of model selection and reconstruction in chaotic systems is addressed by Ribera et al. (2021) and Frerix et al. (2021). Ribera et al. utilize variational annealing and sparse optimization to identify underlying structures from incomplete data, emphasizing the importance of reconstructing system equations. Similarly, Frerix et al. enhance forecast quality by employing a learned inverse observation operator, transforming the optimization problem into a more manageable physics space. Both studies highlight the broader context of data-driven modeling in chaotic systems, aligning with my research's focus on improving predictions without relying on measurement noise statistics (Ribera et al., 2021; Frerix et al., 2021).

The exploration of hybrid modeling approaches is further exemplified by Pawar et al. (2021) and Gottwald and Reich (2020), who integrate machine learning with data assimilation to address unknown dynamics in physical systems. Pawar et al. employ a recurrent neural network to model hidden physics, using data assimilation to correct predictions, while Gottwald and Reich utilize random feature maps within an ensemble Kalman filter framework. Both studies demonstrate the potential of combining machine learning with data assimilation to enhance forecasting accuracy, a theme that resonates with my research's goal of minimizing topological differences between measurements and predictions (Pawar et al., 2021; Gottwald & Reich, 2020).

The use of machine learning techniques to handle incomplete or noisy data is also explored by Özalp et al. (2023) and Wikner et al. (2021). Özalp et al. employ LSTM networks to reconstruct and forecast chaotic systems, inferring hidden dynamics and stability properties from limited data. Wikner et al. introduce a hybrid approach that leverages machine learning to correct imperfections in knowledge-based models, focusing on improving predictions without relying on measurement noise statistics. Both studies underscore the potential of machine learning in enhancing the accuracy of predictions in chaotic systems, aligning with my research's emphasis on innovative data assimilation techniques (Özalp et al., 2023; Wikner et al., 2021).

Finally, the concept of online learning in data assimilation is addressed by McCabe and Brown (2021) and Bocquet et al. (2020). McCabe and Brown introduce amortized assimilation, leveraging self-supervised denoising and differentiable simulation to improve state estimation without requiring ground truth data. Bocquet et al. focus on online learning of both the dynamics and state using ensemble Kalman filters, contrasting with traditional methods that require long time series of data. Both studies highlight the importance of real-time data assimilation, complementing my approach of minimizing topological differences to update model coefficients (McCabe & Brown, 2021; Bocquet et al., 2020).","Michael McCabe, Jed Brown (2021). Learning to Assimilate in Chaotic Dynamical Systems. arXiv:2111.01058. https://arxiv.org/abs/2111.01058
Thomas Frerix, Dmitrii Kochkov, Jamie A. Smith, Daniel Cremers, Michael P. Brenner, Stephan Hoyer (2021). Variational Data Assimilation with a Learned Inverse Observation Operator. arXiv:2102.11192. https://arxiv.org/abs/2102.11192
Elise Özalp, Georgios Margazoglou, Luca Magri (2023). Reconstruction, forecasting, and stability of chaotic dynamics from partial data. arXiv:2305.15111. https://arxiv.org/abs/2305.15111
Duong Nguyen, Said Ouala, Lucas Drumetz, Ronan Fablet (2019). EM-like Learning Chaotic Dynamics from Noisy and Partial Observations. arXiv:1903.10335. https://arxiv.org/abs/1903.10335
H. Ribera, S. Shirman, A. V. Nguyen, N. M. Mangan (2021). Model selection of chaotic systems from data with hidden variables using sparse data assimilation. arXiv:2105.10068. https://arxiv.org/abs/2105.10068
Julien Brajard, Alberto Carassi, Marc Bocquet, Laurent Bertino (2020). Combining data assimilation and machine learning to emulate a dynamical model from sparse and noisy observations: a case study with the Lorenz 96 model. arXiv:2001.01520. https://arxiv.org/abs/2001.01520
Alexander Wikner, Jaideep Pathak, Brian R. Hunt, Istvan Szunyogh, Michelle Girvan, Edward Ott (2021). Using Data Assimilation to Train a Hybrid Forecast System that Combines Machine-Learning and Knowledge-Based Components. arXiv:2102.07819. https://arxiv.org/abs/2102.07819
Suraj Pawar, Omer San, Adil Rasheed, Ionel M. Navon (2021). A nonintrusive hybrid neural-physics modeling of incomplete dynamical systems: Lorenz equations. arXiv:2104.00114. https://arxiv.org/abs/2104.00114
Marc Bocquet, Alban Farchi, Quentin Malartic (2020). Online learning of both state and dynamics using ensemble Kalman filters. arXiv:2006.03859. https://arxiv.org/abs/2006.03859",0,0
30,5|7|5|2|7|3|7|6|6|2|3|3|2|7|6|6|6|7|3|2|2|6|5|7|3|7|7|3|2|3,10,7|5|2|3|5|3|7|2|7|7,target,source,"The exploration of the Age of Information (AoI) in systems with sequential processing steps has garnered significant attention in recent research, particularly concerning the trade-off between performance and power consumption. A foundational aspect of this research is the optimization of service rates to balance these factors, as highlighted in the work of Melih Bastopcu and Sennur Ulukus (2019), which investigates energy management in distributed information retrieval systems. Their study models these systems as (n, k) fork-join queues, akin to the series server setups in our research, and provides an analytical approach to optimize energy management while meeting performance constraints. This aligns with our focus on optimizing service rates to balance power consumption and information freshness, emphasizing the importance of efficient system operation under specific constraints.

The power-performance trade-off in computing systems is further explored by Yanpei Liu et al. (2013), who use queuing theory to identify optimal combinations of processing speed and system settings for power efficiency. This study complements our research by addressing the age-power trade-off in systems with sequential processing steps, as both works explore optimal configurations to balance performance and power consumption. While Liu et al. focus on power efficiency in server operations, our research extends this by addressing the AoI in systems with parallel and series server setups, highlighting the occurrence of wasted power when processing does not reduce age. Similarly, the work of Ajay Badita et al. (2021) on energy minimization in data-intensive applications through probabilistic speed selection for server cores parallels our efforts to optimize service rates under power constraints, contributing to a broader understanding of energy efficiency in computational systems.

The investigation of AoI in various system configurations is further enriched by the work of Han Dong et al. (2021), which provides insights into AoI in G/G/1/1 systems with service discipline models that either block or preempt updates. This study offers foundational age expressions and optimization strategies applicable to analyzing AoI performance in systems with sequential processing steps, complementing our focus on optimizing service rates under power constraints. The exploration of optimal interarrival and service times aligns with our efforts to minimize wasted power in processing, particularly in series server setups where preemption due to fresher updates is a concern. Additionally, the research by Ismail Akturk and Ulya R. Karpuzcu (2017) on the trade-off between computation and communication in power-efficient systems, which models a tandem computation-transmission queue with power constraints, provides valuable insights into managing power consumption, further informing our optimization strategies.

The theme of maintaining information freshness while managing resource limitations is also addressed in the work of Parisa Rafiee et al. (2020), which investigates AoI in edge computing systems with a focus on power-efficient computation and transmission in a tandem queue setup. This study highlights the age-delay tradeoff, a concept relevant to our work on optimizing service rates under power constraints. The insights into scheduling and service time distributions provided by Rafiee et al. could inform our optimization strategies for series and parallel server setups. Similarly, the research by Peng Zou et al. (2019) on optimizing AoI through computation-transmission tradeoffs and queue management in edge computing complements our investigation into optimizing processing steps under power constraints, as both studies focus on maintaining information freshness while managing resource limitations.

Finally, the exploration of energy efficiency in computational systems is further supported by the work of Jiajie Huang and Jie Gong (2023), which investigates the age-energy trade-off in a single-source single-server system through server sleep states and wake-up policies. This study complements our research on optimizing service rates for age-power trade-offs in multi-step processing systems, as both works address the challenge of maintaining data freshness while minimizing energy consumption. The use of stochastic hybrid systems to analyze age and energy consumption parallels our approach of modeling and optimizing system designs for efficient information processing. Additionally, the work of Ege Orkun Gamgam et al. (2023) on energy proportionality in scale-out workloads, focusing on power and resource provisioning techniques, relates to our research on optimizing AoI in processing systems, as both studies address the trade-off between performance and power consumption, aiming to achieve efficient system operation under power constraints.","Jiajie Huang, Jie Gong (2023). Age-Energy Trade-off in Status Update System with Wake-up Control. arXiv:2305.07221. https://arxiv.org/abs/2305.07221
Ajay Badita, Rooji Jinan, Balajee Vamanan, Parimal Parag (2021). Modeling Performance and Energy trade-offs in Online Data-Intensive Applications. arXiv:2108.08199. https://arxiv.org/abs/2108.08199
Melih Bastopcu, Sennur Ulukus (2019). Age of Information for Updates with Distortion. arXiv:1904.10444. https://arxiv.org/abs/1904.10444
Peng Zou, Omur Ozel, Suresh Subramaniam (2019). Optimizing Information Freshness Through Computation-Transmission Tradeoff and Queue Management in Edge Computing. arXiv:1912.02692. https://arxiv.org/abs/1912.02692
Han Dong, Sanjay Arora, Yara Awad, Tommy Unger, Orran Krieger, Jonathan Appavoo (2021). Slowing Down for Performance and Energy: An OS-Centric Study in Network Driven Workloads. arXiv:2112.07010. https://arxiv.org/abs/2112.07010
Parisa Rafiee, Peng Zou, Omur Ozel, Suresh Subramaniam (2020). Maintaining Information Freshness in Power-Efficient Status Update Systems. arXiv:2003.13577. https://arxiv.org/abs/2003.13577
Yanpei Liu, Stark C. Draper, Nam Sung Kim (2013). Queuing Theoretic Analysis of Power-performance Tradeoff in Power-efficient Computing. arXiv:1303.1561. https://arxiv.org/abs/1303.1561
Ege Orkun Gamgam, Nail Akar, Sennur Ulukus (2023). Minimizing Age of Information with Generate at Will Status Updates and Age-Agnostic Cyclic Scheduling. arXiv:2311.18791. https://arxiv.org/abs/2311.18791
Ismail Akturk, Ulya R. Karpuzcu (2017). Trading Computation for Communication: A Taxonomy. arXiv:1709.06555. https://arxiv.org/abs/1709.06555",0,1
30,8|8|8|7|8|8|7|9|8|7|8|6|8|8|7|7|7|8|8|8|7|8|8|8|8|8|8|8|6|6,10,9|8|8|8|7|8|8|8|8|7,target,source,"The study of rank-metric codes has garnered significant attention due to their applications in various fields such as network coding, distributed storage, and post-quantum cryptography. A central theme in recent research is the exploration of algebraic and geometric invariants to distinguish and classify these codes. Neri, Puchinger, and Horlemann-Trautmann (2019) delve into the use of algebraic invariants, specifically sequences of dimensions generated by field automorphisms, to differentiate between families of rank-metric codes like Gabidulin and twisted Gabidulin codes. This approach provides a framework for code inequivalence, which complements my research on geometric invariants inspired by the Schur product (Neri et al., 2019). Similarly, Bik and Neri (2023) extend the construction of symmetric Gabidulin codes to higher-degree polynomials, introducing essential-rank metric codes. Their work aligns with my focus on new algebraic structures and offers a complementary perspective on differentiating rank-metric codes through the essential rank metric (Bik & Neri, 2023).

The exploration of geometric methods in understanding rank-metric codes is further advanced by Randrianarisoa (2019), who introduces a geometric approach to studying these codes, including a simpler definition for generalized rank weight and the classification of constant rank weight codes. This aligns closely with my research, which employs a geometric perspective to explore rank-metric codes through novel invariants (Randrianarisoa, 2019). Additionally, the work by Ndiaye et al. (2023) on the generalization of Subspace Subcodes in the rank metric provides an algorithm for constructing generator and parity-check matrices, addressing the need for new code families to mitigate structural attacks. This complements my study of geometric invariants by emphasizing the importance of novel algebraic structures to enhance the robustness of rank-metric codes (Ndiaye et al., 2023).

The algebraic structures of Gabidulin codes are further explored by Bartoli, Zini, and Zullo (2022), who investigate the tensor rank of these codes, identifying an infinite family that is not minimum tensor rank (MTR). This work provides insights into the algebraic properties of Gabidulin codes, which are crucial for distinguishing them from random codes using geometric invariants (Bartoli et al., 2022). Byrne et al. (2019) also contribute to this understanding by introducing the concepts of generator and parity check tensors, defining the tensor rank as a significant parameter. Their introduction of minimal tensor rank (MTR) codes and their connection to MDS codes complements my investigation into differentiating Gabidulin codes from random ones (Byrne et al., 2019).

The foundational concepts of rank-metric codes are comprehensively covered by Gorla (2019), who discusses essential topics such as rank distance, equivalence, and generalized weights. This foundational work aligns with my research focus on constructing new families of rank-metric codes and distinguishing them using novel invariants. The exploration of MRD codes, optimal anticodes, and q-polymatroids offers valuable insights that complement my investigation into the geometric invariants of linear rank-metric codes (Gorla, 2019). Ravagnani (2014) further explores generalized weights as algebraic invariants for codes, focusing on anticodes in both Hamming and rank metrics. The introduction of ""Delsarte generalized weights"" for Delsarte rank-metric codes provides a framework for understanding algebraic invariants, aligning with my focus on developing new geometric invariants (Ravagnani, 2014).

Finally, the work by Neri, Santonastaso, and Zullo (2021) on the geometric characterization of sum-rank metric codes extends the family of linearized Reed-Solomon codes to doubly-extended versions, maintaining maximum sum-rank distance properties. This exploration of one-weight codes and their geometric properties aligns with my focus on using geometric invariants to distinguish rank-metric codes, offering insights into the algebraic structures and constraints of such codes (Neri et al., 2021). Collectively, these studies underscore the importance of both algebraic and geometric methods in advancing the understanding and classification of rank-metric codes, providing a rich context for my research on novel geometric invariants.","Alessandro Neri, Paolo Santonastaso, Ferdinando Zullo (2021). Extending two families of maximum rank distance codes. arXiv:2104.07602. https://arxiv.org/abs/2104.07602
Alberto Ravagnani (2014). Generalized weights: an anticode approach. arXiv:1410.7207. https://arxiv.org/abs/1410.7207
Alessandro Neri, Paolo Santonastaso, Ferdinando Zullo (2021). The geometry of one-weight codes in the sum-rank metric. arXiv:2112.04989. https://arxiv.org/abs/2112.04989
Elisa Gorla (2019). Rank-metric codes. arXiv:1902.02650. https://arxiv.org/abs/1902.02650
Eimear Byrne, Alessandro Neri, Alberto Ravagnani, John Sheekey (2019). Tensor Representation of Rank-Metric Codes. arXiv:1904.05227. https://arxiv.org/abs/1904.05227
Daniele Bartoli, Giovanni Zini, Ferdinando Zullo (2022). Non-minimum tensor rank Gabidulin codes. arXiv:2201.08242. https://arxiv.org/abs/2201.08242
Ousmane Ndiaye, Peter Arnaud Kidoudou, Hervé Tale Kalachi (2023). Generalized Subspace Subcodes in the Rank Metric. arXiv:2301.12523. https://arxiv.org/abs/2301.12523
Arthur Bik, Alessandro Neri (2023). Higher-degree symmetric rank-metric codes. arXiv:2303.06745. https://arxiv.org/abs/2303.06745
Alessandro Neri, Sven Puchinger, Anna-Lena Horlemann-Trautmann (2019). Equivalence and Characterizations of Linear Rank-Metric Codes Based on Invariants. arXiv:1911.13059. https://arxiv.org/abs/1911.13059",0,1
