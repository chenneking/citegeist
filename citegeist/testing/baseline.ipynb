{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-11-29T03:35:48.353063Z",
     "start_time": "2024-11-29T03:35:42.659936Z"
    }
   },
   "source": [
    "from bertopic import BERTopic\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from pymilvus import MilvusClient\n",
    "import json\n",
    "from citegeist.utils.helpers import (\n",
    "    load_api_key,\n",
    "    generate_summary_prompt,\n",
    "    generate_related_work_prompt,\n",
    ")\n",
    "from citegeist.utils.azure_client import AzureClient\n",
    "from citegeist.utils.citations import get_arxiv_abstract, get_arxiv_citation\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "topic_model = BERTopic.load(\"MaartenGr/BERTopic_ArXiv\")\n",
    "embedding_model = SentenceTransformer(\"sentence-transformers/all-mpnet-base-v2\")\n",
    "client = MilvusClient(\"./database.db\")\n",
    "prompting_client = AzureClient(\n",
    "    endpoint=os.getenv(\"AZURE_ENDPOINT\"),\n",
    "    deployment_id=os.getenv(\"AZURE_PROMPTING_MODEL\"),\n",
    "    api_key=load_api_key(os.getenv(\"KEY_LOCATION\")),\n",
    ")"
   ],
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-29T03:37:52.161177Z",
     "start_time": "2024-11-29T03:36:03.762816Z"
    }
   },
   "cell_type": "code",
   "source": [
    "abstract = \"Large Language Models have shown impressive per- formance across a wide array of tasks involving both structured and unstructured textual data. More recently, adaptions of these models have drawn attention to their abilities to work with code across different programming languages. On this notion, different benchmarks for code generation, repair, or completion suggest that certain models have programming abilities comparable to or even surpass humans. In this work, we demonstrate that the performance on this benchmark does not translate to the innate ability of humans to appreciate the structural control flow of code. For this purpose, we extract code solutions from the Hu- manEval benchmark, which the relevant models perform very strongly on, and trace their execution path using function calls sampled from the respective test set. Using this dataset, we investigate the ability of 5 state-of-the-art LLMs to match the execution trace and find that, despite the model’s abilities to generate semantically identical code, they possess only limited ability to trace the execution path, especially for traces with increased length. We find that even the top-performing model, Gemini 1.5 Pro can only fully correctly generate the trace of 47% of HumanEval tasks. In addition, we introduce a specific subset for three key structures not, or only contained to a limited extent in Hu- manEval: Recursion, Parallel Processing, and Object Oriented Programming principles, including concepts like Inheritance and Polymorphism. Besides OOP, we show that none of the investigated models achieve an average accuracy of over 5% on the relevant traces. Aggregating these specialized parts with the ubiquitous HumanEval tasks, we present the Benchmark CoCoNUT: Code Control Flow for Navigation Understanding and Testing, which measures a models ability to trace the execu- tion of code upon relevant calls, including advanced structural components. We conclude that the current generation LLMs still need to significantly improve to enhance their code reasoning abilities. We hope our dataset can help researchers bridge this gap in the near future.\"\n",
    "embedded_abstract = embedding_model.encode(abstract)\n",
    "topic = topic_model.transform(abstract)\n",
    "topic_id = topic[0][0]\n",
    "\n",
    "res = client.search(\n",
    "    collection_name=\"abstracts\",\n",
    "    data=[embedded_abstract],\n",
    "    limit=10,\n",
    "    # filter = f'topic == {topic_id}',\n",
    "    search_params={\"metric_type\": \"COSINE\", \"params\": {}},\n",
    "    # output_fields = []\n",
    ")\n",
    "formatted_res = json.dumps(res, indent=4)\n",
    "print(formatted_res)"
   ],
   "id": "12f31531c99f743a",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "8292a25e0af94567972c0c2b6da1c042"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-28 22:36:04,542 - BERTopic - Predicting topic assignments through cosine similarity of topic and document embeddings.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\n",
      "    [\n",
      "        {\n",
      "            \"id\": \"2408.10718\",\n",
      "            \"distance\": 0.8272675275802612,\n",
      "            \"entity\": {}\n",
      "        },\n",
      "        {\n",
      "            \"id\": \"2408.13001\",\n",
      "            \"distance\": 0.8230882883071899,\n",
      "            \"entity\": {}\n",
      "        },\n",
      "        {\n",
      "            \"id\": \"2410.21647\",\n",
      "            \"distance\": 0.812175989151001,\n",
      "            \"entity\": {}\n",
      "        },\n",
      "        {\n",
      "            \"id\": \"2309.15432\",\n",
      "            \"distance\": 0.8043964505195618,\n",
      "            \"entity\": {}\n",
      "        },\n",
      "        {\n",
      "            \"id\": \"2407.11470\",\n",
      "            \"distance\": 0.803741455078125,\n",
      "            \"entity\": {}\n",
      "        },\n",
      "        {\n",
      "            \"id\": \"2407.19055\",\n",
      "            \"distance\": 0.8031219840049744,\n",
      "            \"entity\": {}\n",
      "        },\n",
      "        {\n",
      "            \"id\": \"2311.08588\",\n",
      "            \"distance\": 0.8028398752212524,\n",
      "            \"entity\": {}\n",
      "        },\n",
      "        {\n",
      "            \"id\": \"2402.08699\",\n",
      "            \"distance\": 0.8003471493721008,\n",
      "            \"entity\": {}\n",
      "        },\n",
      "        {\n",
      "            \"id\": \"2406.15877\",\n",
      "            \"distance\": 0.8000816106796265,\n",
      "            \"entity\": {}\n",
      "        },\n",
      "        {\n",
      "            \"id\": \"2403.04811\",\n",
      "            \"distance\": 0.7978211641311646,\n",
      "            \"entity\": {}\n",
      "        }\n",
      "    ]\n",
      "]\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-29T03:38:06.613601Z",
     "start_time": "2024-11-29T03:38:06.606277Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# we need to remove the best match because that's the same input paper (this only has to be done for papers that are already in the arxiv corpus)\n",
    "# res = res[0][1:]\n",
    "\n",
    "res = res[0]"
   ],
   "id": "ada7380cc8fb5a6e",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-29T03:38:39.174449Z",
     "start_time": "2024-11-29T03:38:12.375964Z"
    }
   },
   "cell_type": "code",
   "source": [
    "for obj in res:\n",
    "    arxiv_id = obj[\"id\"]\n",
    "    arxiv_abstract = get_arxiv_abstract(arxiv_id)\n",
    "    response: str = prompting_client.get_completions(\n",
    "        generate_summary_prompt(abstract, arxiv_abstract),\n",
    "        os.getenv(\"AZURE_PROMPTING_MODEL_VERSION\"),\n",
    "    )\n",
    "    obj[\"summary\"] = response\n",
    "    obj[\"citation\"] = get_arxiv_citation(arxiv_id)"
   ],
   "id": "c2f3c9cbf90465c8",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-29T03:38:51.480454Z",
     "start_time": "2024-11-29T03:38:51.476610Z"
    }
   },
   "cell_type": "code",
   "source": "print(generate_related_work_prompt(abstract, res))",
   "id": "8e913230b74b0b30",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    I am working on a research paper, and I need a well-written \"Related Work\" section. Below I'm providing you with the abstract of the paper I'm writing and a list of summaries of related works I've identified.\n",
      "    \n",
      "    Here's the abstract of my paper:\n",
      "    \"Large Language Models have shown impressive per- formance across a wide array of tasks involving both structured and unstructured textual data. More recently, adaptions of these models have drawn attention to their abilities to work with code across different programming languages. On this notion, different benchmarks for code generation, repair, or completion suggest that certain models have programming abilities comparable to or even surpass humans. In this work, we demonstrate that the performance on this benchmark does not translate to the innate ability of humans to appreciate the structural control flow of code. For this purpose, we extract code solutions from the Hu- manEval benchmark, which the relevant models perform very strongly on, and trace their execution path using function calls sampled from the respective test set. Using this dataset, we investigate the ability of 5 state-of-the-art LLMs to match the execution trace and find that, despite the model’s abilities to generate semantically identical code, they possess only limited ability to trace the execution path, especially for traces with increased length. We find that even the top-performing model, Gemini 1.5 Pro can only fully correctly generate the trace of 47% of HumanEval tasks. In addition, we introduce a specific subset for three key structures not, or only contained to a limited extent in Hu- manEval: Recursion, Parallel Processing, and Object Oriented Programming principles, including concepts like Inheritance and Polymorphism. Besides OOP, we show that none of the investigated models achieve an average accuracy of over 5% on the relevant traces. Aggregating these specialized parts with the ubiquitous HumanEval tasks, we present the Benchmark CoCoNUT: Code Control Flow for Navigation Understanding and Testing, which measures a models ability to trace the execu- tion of code upon relevant calls, including advanced structural components. We conclude that the current generation LLMs still need to significantly improve to enhance their code reasoning abilities. We hope our dataset can help researchers bridge this gap in the near future.\"\n",
      "    \n",
      "    Here's the list of summaries of the other related works I've found:\n",
      "    \n",
      "        Paper 1:\n",
      "        Summary: The cited paper introduces CodeJudge-Eval, a benchmark that evaluates large language models' code understanding abilities by assessing their capability to judge the correctness of code solutions, rather than just generating code. This approach complements my research, which highlights the limitations of current LLMs in tracing code execution paths, by providing a different perspective on code comprehension. Both works underscore the need for improved benchmarks to better capture the nuanced understanding of code beyond mere generation, addressing gaps in current evaluations of LLMs' programming abilities.\n",
      "        Citation: Yuwei Zhao, Ziyang Luo, Yuchen Tian, Hongzhan Lin, Weixiang Yan, Annan Li, Jing Ma (2024). CodeJudge-Eval: Can Large Language Models be Good Judges in Code Understanding?. arXiv:2408.10718. https://arxiv.org/abs/2408.10718\n",
      "        \n",
      "        Paper 2:\n",
      "        Summary: The cited paper highlights the programming language and task biases in existing code benchmarks like HumanEval, which predominantly focus on Python and code generation tasks, respectively. This aligns with my research, which also critiques the limitations of current benchmarks, particularly in evaluating code reasoning and structural control flow understanding. While my work introduces the Benchmark CoCoNUT to assess execution path tracing, the cited paper proposes CRUXEVAL-X, a multi-lingual code reasoning benchmark, addressing the need for diverse language evaluation. Both studies underscore the necessity for more comprehensive benchmarks to enhance LLMs' code reasoning capabilities across different languages and tasks.\n",
      "        Citation: Ruiyang Xu, Jialun Cao, Yaojie Lu, Hongyu Lin, Xianpei Han, Ben He, Shing-Chi Cheung, Le Sun (2024). CRUXEval-X: A Benchmark for Multilingual Code Reasoning, Understanding and Execution. arXiv:2408.13001. https://arxiv.org/abs/2408.13001\n",
      "        \n",
      "        Paper 3:\n",
      "        Summary: The cited paper highlights the limitations of existing benchmarks in evaluating LLMs' code completion capabilities, emphasizing that these benchmarks do not reflect real-world software development tasks. It introduces REPOCOD, a benchmark with complex, real-world coding problems, revealing that current LLMs perform poorly on these tasks. This aligns with my research, which also identifies gaps in LLMs' abilities, particularly in tracing code execution paths and handling advanced structural components. Both works underscore the need for improved LLMs to enhance code reasoning and real-world applicability.\n",
      "        Citation: Shanchao Liang, Yiran Hu, Nan Jiang, Lin Tan (2024). Can Language Models Replace Programmers? REPOCOD Says 'Not Yet'. arXiv:2410.21647. https://arxiv.org/abs/2410.21647\n",
      "        \n",
      "        Paper 4:\n",
      "        Summary: The cited paper highlights the importance of leveraging structured code representations, such as LLVM IR, to enhance the performance of machine learning models in code-related tasks. This approach contrasts with the limitations observed in current large language models, as noted in my research, which struggle with tracing execution paths despite generating semantically correct code. By utilizing structured data, the cited work suggests a pathway to improve code reasoning abilities, aligning with my findings that current models need significant advancements in understanding code control flow.\n",
      "        Citation: Aiden Grossman, Ludger Paehler, Konstantinos Parasyris, Tal Ben-Nun, Jacob Hegna, William Moses, Jose M Monsalve Diaz, Mircea Trofin, Johannes Doerfert (2023). ComPile: A Large IR Dataset from Production Sources. arXiv:2309.15432. https://arxiv.org/abs/2309.15432\n",
      "        \n",
      "        Paper 5:\n",
      "        Summary: The cited paper introduces the RACE benchmark, which evaluates large language models (LLMs) on multiple dimensions of code quality, such as readability, maintainability, correctness, and efficiency, beyond just correctness. This aligns with my research, which highlights the limitations of current benchmarks in assessing LLMs' code reasoning abilities, particularly in tracing execution paths. Both works emphasize the need for more comprehensive evaluation metrics to capture the multifaceted requirements of real-world code development and improve LLMs' performance in complex coding tasks.\n",
      "        Citation: Jiasheng Zheng, Boxi Cao, Zhengzhao Ma, Ruotong Pan, Hongyu Lin, Yaojie Lu, Xianpei Han, Le Sun (2024). Beyond Correctness: Benchmarking Multi-dimensional Code Generation for Large Language Models. arXiv:2407.11470. https://arxiv.org/abs/2407.11470\n",
      "        \n",
      "        Paper 6:\n",
      "        Summary: The cited paper highlights the limitations of Large Language Models (LLMs) in handling complex code generation tasks, particularly in debugging, which is crucial for iterative code refinement. It introduces a novel algorithm, BESTER, that enhances LLMs' debugging capabilities through self-reflection and search, achieving state-of-the-art results. This work complements my research by addressing the gap in LLMs' ability to understand and trace code execution paths, as demonstrated in the Benchmark CoCoNUT, by focusing on improving their debugging skills, which is essential for accurate code reasoning and execution tracing.\n",
      "        Citation: Jialin Song, Jonathan Raiman, Bryan Catanzaro (2024). Effective Large Language Model Debugging with Best-first Tree Search. arXiv:2407.19055. https://arxiv.org/abs/2407.19055\n",
      "        \n",
      "        Paper 7:\n",
      "        Summary: The cited paper introduces CodeScope, a comprehensive benchmark designed to evaluate the code understanding and generation capabilities of Large Language Models (LLMs) across 43 programming languages and eight coding tasks, addressing limitations in existing benchmarks. This relates to my work by highlighting the need for more robust evaluation metrics, as my research also identifies gaps in current benchmarks, particularly in assessing LLMs' ability to trace execution paths and handle advanced code structures. Both works emphasize the importance of improving LLMs' code reasoning abilities to better align with real-world programming demands.\n",
      "        Citation: Weixiang Yan, Haitian Liu, Yunkun Wang, Yunzhe Li, Qian Chen, Wen Wang, Tingyu Lin, Weishan Zhao, Li Zhu, Hari Sundaram, Shuiguang Deng (2023). CodeScope: An Execution-based Multilingual Multitask Multidimensional Benchmark for Evaluating LLMs on Code Understanding and Generation. arXiv:2311.08588. https://arxiv.org/abs/2311.08588\n",
      "        \n",
      "        Paper 8:\n",
      "        Summary: The cited paper introduces round-trip correctness (RTC) as an alternative evaluation method for code large language models (LLMs), allowing assessment across a broader spectrum of real-world software domains without costly human curation. This approach contrasts with traditional benchmarks like HumanEval, which are limited in scope. In relation to my work, while my research highlights the limitations of LLMs in tracing execution paths and understanding structural code components, the cited paper's RTC method offers a complementary perspective by focusing on semantic equivalence through prediction and synthesis, potentially addressing broader domain applicability.\n",
      "        Citation: Miltiadis Allamanis, Sheena Panthaplackel, Pengcheng Yin (2024). Unsupervised Evaluation of Code LLMs with Round-Trip Correctness. arXiv:2402.08699. https://arxiv.org/abs/2402.08699\n",
      "        \n",
      "        Paper 9:\n",
      "        Summary: The cited paper introduces BigCodeBench, a benchmark designed to evaluate the ability of Large Language Models (LLMs) to perform complex tasks by invoking multiple function calls across various libraries and domains, highlighting the challenges LLMs face in compositional reasoning and following complex instructions. This relates to my research as both works underscore the limitations of current LLMs in understanding and executing complex code structures, with my work specifically focusing on their ability to trace execution paths and handle advanced structural components like recursion and object-oriented programming. Both studies emphasize the need for further advancements to enhance LLMs' code reasoning capabilities.\n",
      "        Citation: Terry Yue Zhuo, Minh Chien Vu, Jenny Chim, Han Hu, Wenhao Yu, Ratnadira Widyasari, Imam Nur Bani Yusuf, Haolan Zhan, Junda He, Indraneil Paul, Simon Brunner, Chen Gong, Thong Hoang, Armel Randy Zebaze, Xiaoheng Hong, Wen-Ding Li, Jean Kaddour, Ming Xu, Zhihan Zhang, Prateek Yadav, Naman Jain, Alex Gu, Zhoujun Cheng, Jiawei Liu, Qian Liu, Zijian Wang, David Lo, Binyuan Hui, Niklas Muennighoff, Daniel Fried, Xiaoning Du, Harm de Vries, Leandro Von Werra (2024). BigCodeBench: Benchmarking Code Generation with Diverse Function Calls and Complex Instructions. arXiv:2406.15877. https://arxiv.org/abs/2406.15877\n",
      "        \n",
      "        Paper 10:\n",
      "        Summary: The cited paper investigates data contamination in code generation benchmarks, highlighting the overlap between these benchmarks and pretraining data, which can lead to inflated model performance. This is relevant to our research as it underscores the need for robust evaluation metrics, like our proposed Benchmark CoCoNUT, to accurately assess LLMs' code reasoning abilities beyond mere memorization of training data. By addressing potential contamination, both studies aim to enhance the reliability and validity of LLMs in programming contexts.\n",
      "        Citation: Martin Riddell, Ansong Ni, Arman Cohan (2024). Quantifying Contamination in Evaluating Code Generation Capabilities of Language Models. arXiv:2403.04811. https://arxiv.org/abs/2403.04811\n",
      "        \n",
      "    \n",
      "    Instructions:\n",
      "    Using all the information given above, your goal is to write a cohesive and well-structured \"Related Work\" section. \n",
      "    Draw connections between the related papers and my research and highlight similarities and differences. \n",
      "    Please also make sure to put my work into the overall context of the provided related works in a summarizing paragraph at the end. \n",
      "    If multiple related works have a common point/theme, make sure to group them and refer to them in the same paragraph. \n",
      "    When referring to content from specific papers you must also cite the respective paper properly (i.e. cite right after your direct/indirect quotes).\n",
      "    Also, make sure the related works section consists of multiple paragraphs (6 at most) which are concise, but not too concise (e.g. avoid 2-sentence paragraphs).\n",
      "    \n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-29T03:39:20.095696Z",
     "start_time": "2024-11-29T03:39:07.646774Z"
    }
   },
   "cell_type": "code",
   "source": [
    "response: str = prompting_client.get_completions(\n",
    "    generate_related_work_prompt(abstract, res),\n",
    "    os.getenv(\"AZURE_PROMPTING_MODEL_VERSION\"),\n",
    ")\n",
    "print(response)"
   ],
   "id": "88f8849d1ab1726f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In recent years, the evaluation of Large Language Models (LLMs) in code-related tasks has garnered significant attention, with various benchmarks being developed to assess different aspects of code understanding and generation. A notable contribution in this domain is CodeJudge-Eval, which evaluates LLMs' ability to judge the correctness of code solutions rather than merely generating code (Zhao et al., 2024). This approach complements our research by highlighting the limitations of current LLMs in tracing code execution paths, underscoring the need for improved benchmarks that capture the nuanced understanding of code beyond generation. Similarly, the RACE benchmark evaluates LLMs on multiple dimensions of code quality, such as readability and maintainability, emphasizing the need for comprehensive evaluation metrics to capture the multifaceted requirements of real-world code development (Zheng et al., 2024).\n",
      "\n",
      "Another critical aspect of LLM evaluation is the consideration of programming language and task biases. The CRUXEVAL-X benchmark addresses this by proposing a multi-lingual code reasoning benchmark, highlighting the limitations of existing benchmarks like HumanEval, which predominantly focus on Python and code generation tasks (Xu et al., 2024). This aligns with our research, which critiques the limitations of current benchmarks, particularly in evaluating code reasoning and structural control flow understanding. In a similar vein, CodeScope evaluates LLMs across 43 programming languages and eight coding tasks, addressing the need for robust evaluation metrics to assess LLMs' ability to trace execution paths and handle advanced code structures (Yan et al., 2023).\n",
      "\n",
      "The limitations of existing benchmarks in reflecting real-world software development tasks are further highlighted by REPOCOD, which introduces complex, real-world coding problems to reveal the gaps in LLMs' abilities (Liang et al., 2024). This aligns with our findings that current LLMs struggle with tracing code execution paths and handling advanced structural components. Additionally, BigCodeBench evaluates LLMs' ability to perform complex tasks by invoking multiple function calls across various libraries and domains, emphasizing the challenges LLMs face in compositional reasoning and following complex instructions (Zhuo et al., 2024).\n",
      "\n",
      "Structured code representations, such as LLVM IR, have been proposed to enhance the performance of machine learning models in code-related tasks, as demonstrated by ComPile (Grossman et al., 2023). This approach contrasts with the limitations observed in current LLMs, which struggle with tracing execution paths despite generating semantically correct code. By utilizing structured data, this work suggests a pathway to improve code reasoning abilities, aligning with our findings that current models need significant advancements in understanding code control flow. Furthermore, the introduction of round-trip correctness (RTC) as an alternative evaluation method for code LLMs offers a complementary perspective by focusing on semantic equivalence through prediction and synthesis, potentially addressing broader domain applicability (Allamanis et al., 2024).\n",
      "\n",
      "Our research contributes to this growing body of work by introducing the Benchmark CoCoNUT, which specifically measures a model's ability to trace the execution of code upon relevant calls, including advanced structural components such as recursion, parallel processing, and object-oriented programming principles. This benchmark addresses the gaps identified in existing evaluations, emphasizing the need for LLMs to significantly improve their code reasoning abilities. By situating our work within the context of these related studies, we highlight the ongoing challenges and opportunities for enhancing LLMs' performance in complex coding tasks, ultimately aiming to bridge the gap between current capabilities and the nuanced understanding required for real-world programming.\n"
     ]
    }
   ],
   "execution_count": 11
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
