Input:
Large Language Models have shown impressive per- formance across a wide array of tasks involving both structured and unstructured textual data. More recently, adaptions of these models have drawn attention to their abilities to work with code across different programming languages. On this notion, different benchmarks for code generation, repair, or completion suggest that certain models have programming abilities comparable to or even surpass humans. In this work, we demonstrate that the performance on this benchmark does not translate to the innate ability of humans to appreciate the structural control flow of code. For this purpose, we extract code solutions from the Hu- manEval benchmark, which the relevant models perform very strongly on, and trace their execution path using function calls sampled from the respective test set. Using this dataset, we investigate the ability of 5 state-of-the-art LLMs to match the execution trace and find that, despite the modelâ€™s abilities to generate semantically identical code, they possess only limited ability to trace the execution path, especially for traces with increased length. We find that even the top-performing model, Gemini 1.5 Pro can only fully correctly generate the trace of 47% of HumanEval tasks. In addition, we introduce a specific subset for three key structures not, or only contained to a limited extent in Hu- manEval: Recursion, Parallel Processing, and Object Oriented Programming principles, including concepts like Inheritance and Polymorphism. Besides OOP, we show that none of the investigated models achieve an average accuracy of over 5% on the relevant traces. Aggregating these specialized parts with the ubiquitous HumanEval tasks, we present the Benchmark CoCoNUT: Code Control Flow for Navigation Understanding and Testing, which measures a models ability to trace the execu- tion of code upon relevant calls, including advanced structural components. We conclude that the current generation LLMs still need to significantly improve to enhance their code reasoning abilities. We hope our dataset can help researchers bridge this gap in the near future.

Breadth: 20
Depth: 2
Diversity: 0

Related Works:

The rapid advancement of Large Language Models (LLMs) in code generation and understanding has prompted a surge of research aimed at evaluating and enhancing their capabilities. A significant body of work has emerged, focusing on the limitations of current benchmarks and proposing new methodologies to assess LLMs' code reasoning abilities. This section reviews related works, highlighting their contributions and connections to our research on the Benchmark CoCoNUT, which evaluates LLMs' ability to trace code execution paths and understand advanced structural components.

Several studies have critiqued existing benchmarks like HumanEval for their narrow focus and have proposed alternative evaluation methods. Allamanis et al. (2024) introduce round-trip correctness (RTC) as an unsupervised evaluation method, contrasting with traditional benchmarks by assessing semantic equivalence across diverse software domains. This approach complements our work by emphasizing the need for comprehensive evaluation methods to enhance LLMs' code reasoning abilities (Allamanis et al., 2024). Similarly, Zalmanovici et al. (2024) present LiveCodeBench, a holistic benchmark addressing issues such as data contamination and the limited scope of natural language-to-code tasks, aligning with our critique of existing benchmarks' inadequacy in assessing LLMs' understanding of code execution paths (Zalmanovici et al., 2024).

The limitations of LLMs in handling complex code semantics and execution paths are a recurring theme in the literature. Tang et al. (2023) propose SpecEval, a framework using formal program specifications to assess code comprehension, which parallels our introduction of the CoCoNUT benchmark to evaluate execution trace comprehension (Tang et al., 2023). Similarly, Jain et al. (2024) introduce EMPICA, focusing on control and data dependencies, underscoring the need for improved semantic comprehension in LLMs, a goal shared by our research (Jain et al., 2024).

Several works have explored the challenges LLMs face in understanding and generating code, particularly in complex scenarios. Dou et al. (2024) conduct an empirical study revealing LLMs' struggles with complex problems and propose a self-critique method to improve code quality, aligning with our focus on evaluating LLMs' abilities to trace code execution paths (Dou et al., 2024). Similarly, Ma et al. (2023) investigate LLMs' limitations in comprehending dynamic code semantics, supporting our findings on the models' struggles with advanced structural components like recursion and parallel processing (Ma et al., 2023).

The need for improved benchmarks and methodologies to capture the multifaceted nature of coding skills in LLMs is emphasized in several studies. Yan et al. (2023) introduce CodeScope, a multilingual and multitask benchmark addressing limitations in existing benchmarks, which complements our work by offering a broader evaluation framework for LLM capabilities in diverse programming environments (Yan et al., 2023). Similarly, Fu et al. (2023) present CodeApex, a bilingual benchmark highlighting the need for comprehensive evaluation metrics, akin to our CoCoNUT benchmark (Fu et al., 2023).

The exploration of LLMs' code reasoning abilities extends to their debugging and semantic understanding capabilities. Song et al. (2024) introduce BESTER, an algorithm enhancing LLMs' debugging capabilities through self-reflection and best-first tree search, which aligns with our findings on the models' limited ability to trace execution paths (Song et al., 2024). Additionally, Ma et al. (2023) propose the Semantic Chain-of-Thought (SeCoT) approach, integrating semantic information to improve code generation, potentially aiding in better tracing of execution paths, a focus of our research (Ma et al., 2023).

The evaluation of LLMs' code reasoning abilities is further explored through frameworks like CodeMind and CodeMMLU. Liu et al. (2024) emphasize the gap between LLMs' code generation capabilities and their understanding of code logic, a theme echoed in our research with the Benchmark CoCoNUT (Liu et al., 2024). Similarly, Nguyen Manh et al. (2024) introduce CodeMMLU, focusing on code comprehension through multiple-choice questions, which supports our emphasis on execution traceability (Nguyen Manh et al., 2024).

The challenges of compositional reasoning and tool use in LLMs are addressed by Zhuo et al. (2024) with BigCodeBench, a benchmark evaluating LLMs' ability to handle complex programming tasks involving multiple function calls. This work underscores the limitations of LLMs in understanding and executing complex code structures, similar to the challenges identified in our research with CoCoNUT (Zhuo et al., 2024). Additionally, Liang et al. (2024) introduce REPOCOD, highlighting LLMs' limitations in achieving high accuracy on complex, context-rich tasks, further emphasizing the need for improved benchmarks and models (Liang et al., 2024).

Finally, the exploration of LLMs' semantic understanding capabilities is complemented by works like Cooper and Scholak (2024), who introduce PERPLEXED, a library analyzing LLMs' difficulties in code generation tasks. This aligns with our research, as both works focus on identifying and analyzing the weaknesses of LLMs in handling complex coding structures (Cooper and Scholak, 2024). Similarly, Huang et al. (2023) explore the use of LLMs to generate Control Flow Graphs (CFGs) for Java, highlighting the potential of LLMs in understanding code structure, a key aspect of our work on code execution path tracing (Huang et al., 2023).

In summary, the related works reviewed in this section collectively underscore the limitations of current LLMs in code reasoning and the need for more comprehensive evaluation methods. Our research contributes to this ongoing discourse by introducing the Benchmark CoCoNUT, which specifically addresses the models' ability to trace execution paths and understand advanced structural components, thereby advancing the development of LLMs for more reliable code reasoning.

Citations:

The rapid advancement of Large Language Models (LLMs) in code generation and understanding has prompted a surge of research aimed at evaluating and enhancing their capabilities. A significant body of work has emerged, focusing on the limitations of current benchmarks and proposing new methodologies to assess LLMs' code reasoning abilities. This section reviews related works, highlighting their contributions and connections to our research on the Benchmark CoCoNUT, which evaluates LLMs' ability to trace code execution paths and understand advanced structural components.

Several studies have critiqued existing benchmarks like HumanEval for their narrow focus and have proposed alternative evaluation methods. Allamanis et al. (2024) introduce round-trip correctness (RTC) as an unsupervised evaluation method, contrasting with traditional benchmarks by assessing semantic equivalence across diverse software domains. This approach complements our work by emphasizing the need for comprehensive evaluation methods to enhance LLMs' code reasoning abilities (Allamanis et al., 2024). Similarly, Zalmanovici et al. (2024) present LiveCodeBench, a holistic benchmark addressing issues such as data contamination and the limited scope of natural language-to-code tasks, aligning with our critique of existing benchmarks' inadequacy in assessing LLMs' understanding of code execution paths (Zalmanovici et al., 2024).

The limitations of LLMs in handling complex code semantics and execution paths are a recurring theme in the literature. Tang et al. (2023) propose SpecEval, a framework using formal program specifications to assess code comprehension, which parallels our introduction of the CoCoNUT benchmark to evaluate execution trace comprehension (Tang et al., 2023). Similarly, Jain et al. (2024) introduce EMPICA, focusing on control and data dependencies, underscoring the need for improved semantic comprehension in LLMs, a goal shared by our research (Jain et al., 2024).

Several works have explored the challenges LLMs face in understanding and generating code, particularly in complex scenarios. Dou et al. (2024) conduct an empirical study revealing LLMs' struggles with complex problems and propose a self-critique method to improve code quality, aligning with our focus on evaluating LLMs' abilities to trace code execution paths (Dou et al., 2024). Similarly, Ma et al. (2023) investigate LLMs' limitations in comprehending dynamic code semantics, supporting our findings on the models' struggles with advanced structural components like recursion and parallel processing (Ma et al., 2023).

The need for improved benchmarks and methodologies to capture the multifaceted nature of coding skills in LLMs is emphasized in several studies. Yan et al. (2023) introduce CodeScope, a multilingual and multitask benchmark addressing limitations in existing benchmarks, which complements our work by offering a broader evaluation framework for LLM capabilities in diverse programming environments (Yan et al., 2023). Similarly, Fu et al. (2023) present CodeApex, a bilingual benchmark highlighting the need for comprehensive evaluation metrics, akin to our CoCoNUT benchmark (Fu et al., 2023).

The exploration of LLMs' code reasoning abilities extends to their debugging and semantic understanding capabilities. Song et al. (2024) introduce BESTER, an algorithm enhancing LLMs' debugging capabilities through self-reflection and best-first tree search, which aligns with our findings on the models' limited ability to trace execution paths (Song et al., 2024). Additionally, Ma et al. (2023) propose the Semantic Chain-of-Thought (SeCoT) approach, integrating semantic information to improve code generation, potentially aiding in better tracing of execution paths, a focus of our research (Ma et al., 2023).

The evaluation of LLMs' code reasoning abilities is further explored through frameworks like CodeMind and CodeMMLU. Liu et al. (2024) emphasize the gap between LLMs' code generation capabilities and their understanding of code logic, a theme echoed in our research with the Benchmark CoCoNUT (Liu et al., 2024). Similarly, Nguyen Manh et al. (2024) introduce CodeMMLU, focusing on code comprehension through multiple-choice questions, which supports our emphasis on execution traceability (Nguyen Manh et al., 2024).

The challenges of compositional reasoning and tool use in LLMs are addressed by Zhuo et al. (2024) with BigCodeBench, a benchmark evaluating LLMs' ability to handle complex programming tasks involving multiple function calls. This work underscores the limitations of LLMs in understanding and executing complex code structures, similar to the challenges identified in our research with CoCoNUT (Zhuo et al., 2024). Additionally, Liang et al. (2024) introduce REPOCOD, highlighting LLMs' limitations in achieving high accuracy on complex, context-rich tasks, further emphasizing the need for improved benchmarks and models (Liang et al., 2024).

Finally, the exploration of LLMs' semantic understanding capabilities is complemented by works like Cooper and Scholak (2024), who introduce PERPLEXED, a library analyzing LLMs' difficulties in code generation tasks. This aligns with our research, as both works focus on identifying and analyzing the weaknesses of LLMs in handling complex coding structures (Cooper and Scholak, 2024). Similarly, Huang et al. (2023) explore the use of LLMs to generate Control Flow Graphs (CFGs) for Java, highlighting the potential of LLMs in understanding code structure, a key aspect of our work on code execution path tracing (Huang et al., 2023).

In summary, the related works reviewed in this section collectively underscore the limitations of current LLMs in code reasoning and the need for more comprehensive evaluation methods. Our research contributes to this ongoing discourse by introducing the Benchmark CoCoNUT, which specifically addresses the models' ability to trace execution paths and understand advanced structural components, thereby advancing the development of LLMs for more reliable code reasoning.