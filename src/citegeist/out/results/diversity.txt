### Related Work

The exploration of large language models (LLMs) in the domain of code understanding and generation has been a focal point of recent research, with various studies highlighting both the potential and limitations of these models. A significant body of work has concentrated on developing benchmarks to evaluate LLMs' code reasoning abilities beyond mere code generation. Zhao et al. (2024) introduced CodeJudge-Eval, a benchmark that assesses LLMs' capacity to judge the correctness of code solutions, emphasizing the gap between code generation and deeper code reasoning abilities. This aligns with our research, which critiques the current benchmarks like HumanEval for not adequately assessing LLMs' understanding of code execution paths and structural control flow. Similarly, Song et al. (2024) with CodeApex, and Zheng et al. (2024) have underscored the need for comprehensive evaluation methods that address the limitations of existing benchmarks, echoing our findings on the necessity for improved code reasoning capabilities in LLMs.

Another theme prevalent in the literature is the challenge LLMs face in understanding dynamic code behaviors and complex programming structures. Zheng et al. (2024) highlighted that while models like GPT-4 excel in syntax comprehension, they struggle with dynamic semantics, a limitation also identified in our study where LLMs show limited ability to trace execution paths. Liang et al. (2024) further explored these limitations by introducing BigCodeBench, which evaluates LLMs' ability to handle complex programming tasks involving multiple function calls. This is particularly relevant to our work, as both studies emphasize the need for advancements in LLMs' code reasoning and execution capabilities, especially concerning dynamic behavior understanding and structural control flow.

The issue of self-repair and debugging in LLMs has also been a subject of investigation. Riddell et al. (2024) explored the limitations of models like Code Llama and GPT-4 in debugging and repairing their own code, which resonates with our findings on LLMs' limited ability to handle complex code structures. Zhuo et al. (2024) introduced BESTER, an algorithm that enhances LLMs' debugging capabilities through self-reflection and best-first tree search, achieving state-of-the-art results in code generation benchmarks. While our research focuses on execution tracing, the improvements in debugging skills highlighted by Zhuo et al. are crucial for advancing LLMs' code reasoning and refinement abilities.

The potential biases and limitations of existing benchmarks have been critically examined in recent studies. Allamanis et al. (2024) investigated data contamination in code generation benchmarks, highlighting how overlaps between training data and evaluation benchmarks like HumanEval can inflate model performance. This finding is pertinent to our work, as it suggests that observed performance might be partly due to memorization rather than genuine understanding, aligning with our conclusion that LLMs need improvement in code reasoning. Yan et al. (2023) addressed programming language and task biases by introducing CRUXEVAL-X, a multilingual code reasoning benchmark, which complements our focus on enhancing LLMs' code reasoning abilities by proposing a comprehensive evaluation across multiple languages.

In summary, the current body of research underscores the limitations of LLMs in understanding and executing complex code structures, a challenge that our work also addresses by introducing the Benchmark CoCoNUT. While various studies have proposed new benchmarks and algorithms to enhance LLMs' code reasoning abilities, our research uniquely contributes by focusing on the models' ability to trace execution paths and handle advanced structural components like recursion and object-oriented programming. Together, these studies highlight the necessity for improved benchmarks and evaluation methods to bridge the gap between LLMs' current capabilities and human-level programming understanding.