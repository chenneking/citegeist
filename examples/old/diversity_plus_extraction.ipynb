{
 "cells": [
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from bertopic import BERTopic\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from pymilvus import MilvusClient\n",
    "import json\n",
    "from citegeist.utils import load_api_key\n",
    "from citegeist.utils import (\n",
    "    generate_summary_prompt_with_page_content,\n",
    "    generate_related_work_prompt\n",
    ")\n",
    "from citegeist.utils import AzureClient\n",
    "from citegeist.utils import (\n",
    "    get_arxiv_abstract,\n",
    "    get_arxiv_citation,\n",
    "    process_arxiv_paper_with_embeddings,\n",
    "    find_most_relevant_pages,\n",
    ")\n",
    "from citegeist.utils import (\n",
    "    extract_most_relevant_pages,\n",
    "    select_diverse_papers_with_weighted_similarity,\n",
    ")\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "topic_model = BERTopic.load(\"MaartenGr/BERTopic_ArXiv\")\n",
    "embedding_model = SentenceTransformer(\"sentence-transformers/all-mpnet-base-v2\")\n",
    "client = MilvusClient(\"./database.db\")\n",
    "prompting_client = AzureClient(\n",
    "    endpoint=os.getenv(\"AZURE_ENDPOINT\"),\n",
    "    deployment_id=os.getenv(\"AZURE_PROMPTING_MODEL\"),\n",
    "    api_key=load_api_key(os.getenv(\"KEY_LOCATION\")),\n",
    ")"
   ],
   "id": "be9dd043e695304b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "abstract = \"Large Language Models have shown impressive per- formance across a wide array of tasks involving both structured and unstructured textual data. More recently, adaptions of these models have drawn attention to their abilities to work with code across different programming languages. On this notion, different benchmarks for code generation, repair, or completion suggest that certain models have programming abilities comparable to or even surpass humans. In this work, we demonstrate that the performance on this benchmark does not translate to the innate ability of humans to appreciate the structural control flow of code. For this purpose, we extract code solutions from the Hu- manEval benchmark, which the relevant models perform very strongly on, and trace their execution path using function calls sampled from the respective test set. Using this dataset, we investigate the ability of 5 state-of-the-art LLMs to match the execution trace and find that, despite the model’s abilities to generate semantically identical code, they possess only limited ability to trace the execution path, especially for traces with increased length. We find that even the top-performing model, Gemini 1.5 Pro can only fully correctly generate the trace of 47% of HumanEval tasks. In addition, we introduce a specific subset for three key structures not, or only contained to a limited extent in Hu- manEval: Recursion, Parallel Processing, and Object Oriented Programming principles, including concepts like Inheritance and Polymorphism. Besides OOP, we show that none of the investigated models achieve an average accuracy of over 5% on the relevant traces. Aggregating these specialized parts with the ubiquitous HumanEval tasks, we present the Benchmark CoCoNUT: Code Control Flow for Navigation Understanding and Testing, which measures a models ability to trace the execu- tion of code upon relevant calls, including advanced structural components. We conclude that the current generation LLMs still need to significantly improve to enhance their code reasoning abilities. We hope our dataset can help researchers bridge this gap in the near future.\"\n",
    "embedded_abstract = embedding_model.encode(abstract)\n",
    "topic = topic_model.transform(abstract)\n",
    "topic_id = topic[0][0]\n",
    "\n",
    "res = client.search(\n",
    "    collection_name=\"abstracts\",\n",
    "    data=[embedded_abstract],\n",
    "    limit=60,\n",
    "    anns_field=\"embedding\",\n",
    "    # filter = f'topic == {topic_id}',\n",
    "    search_params={\"metric_type\": \"COSINE\", \"params\": {}},\n",
    "    output_fields=[\"embedding\"],\n",
    ")\n",
    "formatted_res = json.dumps(res, indent=4)\n",
    "print(formatted_res)\n",
    "print(len(res[0]))"
   ],
   "id": "9dd0cc64be1bb96"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-30T05:38:16.635715Z",
     "start_time": "2024-11-30T05:38:16.622751Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# we need to remove the best match because that's the same input paper (this only has to be done for papers that are already in the arxiv corpus)\n",
    "# res = res[0][1:]\n",
    "\n",
    "res = res[0]"
   ],
   "id": "3bd0e253ce79cbb0",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-30T05:38:19.083635Z",
     "start_time": "2024-11-30T05:38:19.076378Z"
    }
   },
   "cell_type": "code",
   "source": "reference = res[:10]",
   "id": "65e16dea9605785f",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-30T05:40:37.590194Z",
     "start_time": "2024-11-30T05:40:37.585657Z"
    }
   },
   "cell_type": "code",
   "source": [
    "for obj in res:\n",
    "    obj[\"embedding\"] = obj[\"entity\"][\"embedding\"]\n",
    "    obj[\"entity\"] = \"\""
   ],
   "id": "62616568271bc24a",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-30T05:41:06.731732Z",
     "start_time": "2024-11-30T05:41:06.473232Z"
    }
   },
   "cell_type": "code",
   "source": "output = select_diverse_papers_with_weighted_similarity(res, 10, 0.35)",
   "id": "3be3257b0e2bbd25",
   "outputs": [],
   "execution_count": 10
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "original_top_10 = [obj[\"id\"] for obj in res][:10]\n",
    "print(original_top_10)\n",
    "\n",
    "differences = set(output) - set(original_top_10)\n",
    "print(differences)"
   ],
   "id": "df72f0499bbe30f0"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-30T05:42:01.177251Z",
     "start_time": "2024-11-30T05:41:48.455480Z"
    }
   },
   "cell_type": "code",
   "source": [
    "paper_embeddings = []\n",
    "for paper in output:\n",
    "    arxiv_id = paper[\"id\"]  # Replace with the actual paper ID key in your JSON\n",
    "\n",
    "    print(f\"Processing paper: {arxiv_id}\")\n",
    "    result = process_arxiv_paper_with_embeddings(arxiv_id, topic_model)\n",
    "\n",
    "    if result:\n",
    "        paper_embeddings.append(result)\n",
    "        print(f\"Paper {arxiv_id}: Processed successfully.\")\n",
    "    else:\n",
    "        print(f\"Paper {arxiv_id}: No content remains after filtering.\")\n",
    "\n",
    "# Print an example: First page text and embedding of the first processed paper\n",
    "if paper_embeddings:\n",
    "    print(\"First paper, first page text:\", paper_embeddings[0][0][\"text\"])\n",
    "    print(\"First paper, first page embedding:\", paper_embeddings[0][0][\"embedding\"])\n",
    "\n",
    "relevant_pages = extract_most_relevant_pages(\n",
    "    paper_embeddings, abstract, topic_model, 60\n",
    ")"
   ],
   "id": "5855c5c296759d74",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing paper: 2408.10718\n",
      "PDF downloaded successfully: 2408.10718.pdf\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "03242b15644c4372aea6238c6c10d8b2"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Paper 2408.10718: Processed successfully.\n",
      "Processing paper: 2309.15432\n",
      "PDF downloaded successfully: 2309.15432.pdf\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "5dbf055d8a694430b31c57958db01f5b"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Paper 2309.15432: Processed successfully.\n",
      "Processing paper: 2406.15877\n",
      "PDF downloaded successfully: 2406.15877.pdf\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "27a3c7b324234d4a891efa5261c5486e"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Paper 2406.15877: Processed successfully.\n",
      "Processing paper: 2403.19114\n",
      "PDF downloaded successfully: 2403.19114.pdf\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "8a81b8630d1449b9af493f668e896457"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Paper 2403.19114: Processed successfully.\n",
      "Processing paper: 2305.12138\n",
      "PDF downloaded successfully: 2305.12138.pdf\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "f9a05a248c014c1abf4f9b6266e891da"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Paper 2305.12138: Processed successfully.\n",
      "Processing paper: 2309.01940\n",
      "PDF downloaded successfully: 2309.01940.pdf\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "019051e7caf84a259fc8bd1155f05f87"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Paper 2309.01940: Processed successfully.\n",
      "Processing paper: 2408.13001\n",
      "PDF downloaded successfully: 2408.13001.pdf\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "52e00995c3894a06a4628d230fa91d2f"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Paper 2408.13001: Processed successfully.\n",
      "Processing paper: 2403.04811\n",
      "PDF downloaded successfully: 2403.04811.pdf\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "bd9ae28f7a044c3f9f36af4b1990e667"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Paper 2403.04811: Processed successfully.\n",
      "Processing paper: 2407.19055\n",
      "PDF downloaded successfully: 2407.19055.pdf\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "19c6da30aaf3490eb593566682942373"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Paper 2407.19055: Processed successfully.\n",
      "Processing paper: 2306.09896\n",
      "PDF downloaded successfully: 2306.09896.pdf\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "2dadb5981c8c47ab997df00d31062133"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Paper 2306.09896: Processed successfully.\n",
      "First paper, first page text: CodeJudge-Eval: Can Large Language Models be Good Judges in\n",
      "Code Understanding?\n",
      "♢Yuwei Zhao* , ♠Ziyang Luo* , ♡Yuchen Tian , ♠Hongzhan Lin\n",
      "♣Weixiang Yan , ♢Annan Li , ♠Jing Ma†\n",
      "♠Hong Kong Baptist University, ♢Beihang University\n",
      "♡University of Tokyo, ♣Vaneval.AI\n",
      "{yuweizhao,liannan}@buaa.edu.cn\n",
      "{cszyluo,majing}@comp.hkbu.edu.hk\n",
      "Abstract\n",
      "Recent advancements in large language models\n",
      "(LLMs) have showcased impressive code gener-\n",
      "ation capabilities, primarily evaluated through\n",
      "language-to-code benchmarks. However, these\n",
      "benchmarks may not fully capture a model’s\n",
      "code understanding abilities.\n",
      "We introduce\n",
      "CodeJudge-Eval (CJ-Eval), a novel bench-\n",
      "mark designed to assess LLMs’ code under-\n",
      "standing abilities from the perspective of code\n",
      "judging rather than code generation. CJ-Eval\n",
      "challenges models to determine the correctness\n",
      "of provided code solutions, encompassing var-\n",
      "ious error types and compilation issues. By\n",
      "leveraging a diverse set of problems and a fine-\n",
      "grained judging system, CJ-Eval addresses\n",
      "the limitations of traditional benchmarks, in-\n",
      "cluding the potential memorization of solu-\n",
      "tions.\n",
      "Evaluation of 12 well-known LLMs\n",
      "on CJ-Eval reveals that even state-of-the-art\n",
      "models struggle, highlighting the benchmark’s\n",
      "ability to probe deeper into models’ code un-\n",
      "derstanding abilities. Our codes and bench-\n",
      "mark are available at https://github.com/\n",
      "CodeLLM-Research/CodeJudge-Eval.\n",
      "1\n",
      "Introduction\n",
      "Recently, powerful large language models (LLMs)\n",
      "such as GPT-4o (OpenAI, 2023), Gemini (Anil\n",
      "et al., 2023), and Claude (Anthropic, 2023) have\n",
      "demonstrated impressive code generation capabili-\n",
      "ties. These models are being used to develop tools\n",
      "that assist in software development (Hong et al.,\n",
      "2024; Yang et al., 2024). The primary method\n",
      "the community uses to evaluate the coding abili-\n",
      "ties of these LLMs is based on popular language-\n",
      "to-code benchmarks, such as HumanEval (Chen\n",
      "et al., 2021), APPS (Hendrycks et al., 2021) and\n",
      "MBPP (Austin et al., 2021), where LLMs are\n",
      "tasked with generating code based on task descrip-\n",
      "tions.\n",
      "If the generated code can pass the pre-\n",
      "*Equal Contribution. Ziyang Luo is the project lead.\n",
      "†Corresponding Author.\n",
      "Problem 1:\n",
      "    Anton has the integer 𝑥. He is interested what positive integer, which doesn't exceed 𝑥, has the \n",
      "maximum sum of digits. If there are several such integers, determine the biggest of them.\n",
      "    Input: 1 ≤𝑥≤1018\n",
      "    Output: Print the positive integer which doesn't exceed 𝑥 and has the maximum sum of digits\n",
      "    Examples:\n",
      " \n",
      " \n",
      "Input: 100 \n",
      "Output: 99 \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "Input: 521 \n",
      "Output: 499\n",
      "Judge the following code for Problem 1.\n",
      "def max_sum_digits(n):\n",
      "   n = str(n)\n",
      "   if len(n) == 1:\n",
      "       return int(n)\n",
      "   if '9' * len(n) <= n:\n",
      "       return int('9' * len(n))\n",
      "   for i in range(len(n)):\n",
      "       if n[i] != '9':\n",
      "          return int(n[:i] + str(int(n[i]) - 1) + \n",
      "'9' * (len(n) - i - 1))\n",
      "n = int(input().strip())\n",
      "print(max_sum_digits(n))\n",
      "Sure! Here’s a possible solution:\n",
      "x = input()\n",
      "best = x\n",
      "for i in range(len(x), 0, -1):\n",
      "    t = str(int(x[:i])-1) + '9' * (len(x)-i)\n",
      "    t = t.lstrip(\"0\")\n",
      "    if sum(map(int, t)) > sum(map(int, best)):\n",
      "        best = t\n",
      "print(best)\n",
      "Correct Code\n",
      "I think the code is correct.\n",
      "Ground truth label:\n",
      "Wrong Answer\n",
      "Wrong Judgement\n",
      "Correct Generation for\n",
      "Code Generation Task\n",
      "Wrong Judgement for \n",
      "Code Judging Task\n",
      "Generate a correct code solution \n",
      "for Problem 1.\n",
      "Task: Code \n",
      "Judging (Ours)\n",
      "Task: Code \n",
      "Generation\n",
      "(a).\n",
      "(b).\n",
      "Running on test cases\n",
      "Figure 1: Comparing code generation with code judg-\n",
      "ing task, we observe that a model’s ability to generate\n",
      "correct code does not necessarily imply it can accurately\n",
      "judge other codes for the same problem.\n",
      "designed test cases, the LLMs are considered to\n",
      "have successfully solved the coding tasks.\n",
      "While language-to-code benchmarks have signif-\n",
      "icantly advanced the coding capabilities of LLMs,\n",
      "the assumption that a model’s ability to pass pre-\n",
      "designed test cases for a specific task equates to\n",
      "a full understanding of that task does not always\n",
      "hold true (Dou et al., 2024).\n",
      "These test cases\n",
      "may not comprehensively cover all potential in-\n",
      "puts and edge cases (Liu et al., 2023), and con-\n",
      "cerns such as data leakage can further undermine\n",
      "the reliability of such evaluations (Dong et al.,\n",
      "2024; White et al., 2024). To overcome these chal-\n",
      "lenges, we draw inspiration from modern educa-\n",
      "tional theory, which suggests that if someone can\n",
      "accurately judge the correctness of other candidate\n",
      "1\n",
      "arXiv:2408.10718v2  [cs.SE]  13 Sep 2024\n",
      "\n",
      "First paper, first page embedding: [-3.37842340e-03  7.79132247e-02 -1.31800333e-02  5.06728403e-02\n",
      " -1.07947029e-02  4.17561270e-02 -2.67486181e-02  7.88411312e-03\n",
      "  8.85283295e-03  1.88879331e-03 -3.07997651e-02 -2.19248980e-02\n",
      "  1.55756110e-03  2.46962421e-02  3.31747644e-02  1.36062978e-02\n",
      "  3.93901840e-02 -7.81936944e-02 -1.67249069e-02  9.04109143e-03\n",
      " -1.34551860e-02 -8.72508343e-03  1.85588934e-02  6.53211251e-02\n",
      "  6.83104480e-03 -4.85333316e-02  8.59226938e-03 -3.84191680e-03\n",
      " -1.80537701e-02 -2.15987880e-02 -2.18744529e-03  2.19891127e-02\n",
      " -4.40469990e-03  4.22426127e-02  2.40093414e-06 -2.18793619e-02\n",
      " -2.22836807e-02  1.03352563e-02  9.18819662e-03  8.89230296e-02\n",
      "  4.32540290e-03 -3.97987105e-02 -7.37564499e-03  2.17736214e-02\n",
      " -2.67542526e-02 -3.47567871e-02  3.54520045e-02 -3.56779695e-02\n",
      "  5.04645631e-02  6.39834553e-02  1.07850991e-02 -2.52011605e-02\n",
      "  3.08838002e-02  6.10976899e-03  1.94588080e-02 -6.89962804e-02\n",
      "  1.37504740e-02  4.96433601e-02  6.78175129e-03 -1.71722919e-02\n",
      " -2.16616038e-02  1.69405285e-02  4.41298299e-02  1.52683072e-02\n",
      " -3.01320497e-02  2.60442141e-02 -4.08925414e-02  1.35532990e-02\n",
      " -7.18847755e-03  2.79799234e-02 -2.12117434e-02 -4.36027944e-02\n",
      " -1.04094470e-04 -3.40079819e-03 -3.45787741e-02 -3.63012664e-02\n",
      " -4.11994196e-02  2.64441897e-03 -2.49350071e-02  1.09453639e-02\n",
      "  2.74114683e-02 -1.97570827e-02  4.69105877e-02  1.19271539e-02\n",
      " -3.91242169e-02  8.08020756e-02  1.02682887e-02 -6.17723074e-03\n",
      " -1.80937313e-02 -1.17091723e-02  3.55106220e-03 -3.77867147e-02\n",
      " -1.50749665e-02  7.54924640e-02 -1.12274084e-02  2.91595589e-02\n",
      " -1.61702447e-02 -1.07570961e-02  2.56885085e-02 -5.29018678e-02\n",
      "  3.69589366e-02  4.96358909e-02 -1.96958072e-02  5.91564551e-02\n",
      " -6.63747117e-02  1.14465162e-01 -2.79043335e-02 -8.83475598e-03\n",
      " -3.56054269e-02  6.55472977e-04 -1.92574393e-02 -9.99658462e-03\n",
      " -2.98691913e-02  3.92364413e-02  7.70703331e-03  2.40672231e-02\n",
      " -3.27661894e-02  2.42945924e-02 -1.72473621e-02  7.96063244e-02\n",
      " -4.28701229e-02 -3.68332900e-02 -7.26043480e-03 -9.80286207e-03\n",
      "  1.05584711e-02 -9.44759790e-03 -1.71961002e-02 -4.36676256e-02\n",
      " -1.45340767e-02 -7.09104836e-02  1.27287516e-02  1.51938880e-02\n",
      " -4.67589833e-02 -3.74937207e-02  1.25744911e-02  5.62054068e-02\n",
      "  5.35600167e-03  5.11901593e-03 -2.13090200e-02  6.89814612e-03\n",
      " -2.35119984e-02 -5.35380729e-02  5.00876084e-03 -9.56147537e-02\n",
      " -6.30216673e-02  2.81628575e-02  2.13552080e-02 -8.79515335e-02\n",
      "  8.75089876e-03 -2.50757225e-02  1.92744508e-02  3.78193967e-02\n",
      "  6.38006581e-03  1.93798374e-02 -5.15387568e-04  3.61258835e-02\n",
      " -2.20401846e-02  5.40636815e-02  3.77872251e-02 -1.36921078e-03\n",
      "  3.87780671e-03 -3.85283269e-02  5.87977022e-02  5.71363568e-02\n",
      " -1.96374003e-02 -4.22835263e-04 -2.59593688e-02  2.98024733e-02\n",
      "  4.65694778e-02 -3.12135797e-02  1.27190910e-02  4.96535338e-02\n",
      " -3.24579724e-03  6.90333359e-03 -3.96234915e-03  1.27160233e-02\n",
      " -8.28081276e-03  5.59167936e-02  5.95322214e-02 -2.36214288e-02\n",
      "  3.78867462e-02 -1.64892047e-03 -5.01811281e-02  4.24533747e-02\n",
      " -2.09389031e-02  1.72417574e-02  1.00168837e-02  1.15799550e-02\n",
      " -1.16423918e-02 -9.11804661e-02 -4.61594090e-02  1.57321747e-02\n",
      " -2.53665298e-02  1.45135801e-02  3.71407233e-02 -2.59978399e-02\n",
      " -4.85001989e-02  2.39702873e-02 -4.22650902e-03 -6.16465136e-02\n",
      " -1.96903255e-02 -7.00059980e-02  3.27459760e-02  2.15293989e-02\n",
      "  3.75130475e-02 -4.20541428e-02 -3.26687209e-02  2.62759775e-02\n",
      " -2.01909840e-02 -4.33601178e-02  7.35499188e-02  1.40587725e-02\n",
      "  4.65788692e-03 -3.55157964e-02  1.23760188e-02 -2.02339683e-02\n",
      "  2.15887949e-02  4.68177497e-02  2.41640434e-02 -5.65783354e-03\n",
      "  2.69084722e-02 -2.53146309e-02 -8.40279385e-02  1.65280048e-02\n",
      " -5.40284021e-03 -9.04500019e-03 -1.38334045e-02 -8.27073492e-03\n",
      "  3.49197499e-02 -3.31875384e-02 -4.28362750e-02  2.19475999e-02\n",
      "  1.97301190e-02 -2.38496810e-02  1.05851106e-02 -1.83385722e-02\n",
      " -4.13560644e-02 -3.15529704e-02  2.92452611e-02 -3.90526131e-02\n",
      " -3.15449275e-02  4.51676399e-02 -3.74039449e-02 -3.50412875e-02\n",
      "  6.41843230e-02 -4.35919501e-03  3.31045948e-02 -1.93352848e-02\n",
      "  4.47758026e-02 -5.54603338e-02 -1.63438674e-02  1.00547271e-02\n",
      "  6.14169873e-02  5.61537500e-03  4.49728481e-02  1.04905283e-02\n",
      "  8.56139362e-02  9.03491210e-03  2.50312816e-02 -6.99088797e-02\n",
      "  6.99499715e-03 -2.18437910e-02  3.81192043e-02 -2.14009695e-02\n",
      "  4.66689691e-02 -6.04834817e-02 -5.39888218e-02 -9.61720124e-02\n",
      "  3.00344676e-02 -3.04657575e-02  2.43409742e-02  1.60222780e-03\n",
      " -1.30920438e-02  2.35816929e-03  2.16953065e-02 -2.61312071e-02\n",
      " -1.30355358e-02 -1.32844159e-02  1.03352480e-02 -2.07223790e-03\n",
      " -1.63022764e-02 -6.56679720e-02 -3.77140865e-02  1.11073405e-02\n",
      " -3.87118049e-02  4.02680971e-02 -1.27957948e-02  9.64333303e-04\n",
      " -5.58208749e-02  9.20339599e-02 -1.96365062e-02  5.61024575e-03\n",
      " -4.09564562e-02 -4.98859473e-02  1.07909450e-02 -2.92857531e-02\n",
      "  5.27182110e-02  4.82037589e-02 -3.88747938e-02  1.07966671e-02\n",
      "  4.71043065e-02 -1.01056173e-02 -4.05964963e-02  2.35884786e-02\n",
      "  4.30525318e-02 -6.35565594e-02 -2.73605902e-02 -2.26280745e-02\n",
      "  1.32568786e-02  8.79543945e-02 -1.35920011e-02 -2.08558030e-02\n",
      " -4.39192355e-02  1.58825777e-02 -3.53811681e-02  1.99610684e-02\n",
      "  1.13089578e-02 -7.76859932e-03 -3.12635042e-02  4.55382690e-02\n",
      " -2.72603966e-02 -7.90229589e-02  5.27100600e-02 -5.41174598e-02\n",
      " -2.96743661e-02 -2.43252777e-02 -5.09890402e-03 -7.50186965e-02\n",
      "  6.64657950e-02 -2.29088450e-03  2.32099965e-02 -8.87169689e-03\n",
      " -3.20562199e-02 -1.22987125e-02  1.31450379e-02 -3.11032794e-02\n",
      " -4.26695421e-02  8.00615475e-02 -4.41605970e-02  2.62093563e-02\n",
      " -8.90035415e-04 -2.62367409e-02 -3.42791378e-02  2.71642115e-02\n",
      " -4.81382608e-02  6.80456590e-03  3.54194380e-02 -1.52517008e-02\n",
      "  2.79299822e-02  1.06998440e-02 -1.38812084e-02  3.42929251e-02\n",
      " -1.28674752e-03 -3.02149430e-02 -8.71457625e-03 -3.14798616e-02\n",
      " -2.34222375e-02 -5.96734621e-02  3.72811221e-02 -2.95891948e-02\n",
      "  3.46423164e-02  3.12312841e-02  5.85788116e-02  2.83894427e-02\n",
      " -8.38008448e-02  2.66495869e-02  4.45408039e-02 -7.12469104e-04\n",
      " -3.37921493e-02  6.74114674e-02  6.50522783e-02 -7.84727186e-02\n",
      " -3.82255651e-02  8.91706869e-02  2.36208532e-02 -6.55924082e-02\n",
      " -1.60185457e-03 -8.37369822e-03  4.52110618e-02  5.99158071e-02\n",
      "  2.95676868e-02 -6.56185076e-02 -2.77387761e-02  4.90128389e-03\n",
      "  2.23207800e-03 -1.05846012e-02 -2.85097938e-02 -1.53153930e-02\n",
      " -1.07469976e-01  3.98774706e-02  3.56324948e-02 -3.70205306e-02\n",
      " -2.95965411e-02 -2.21803430e-02 -9.29006655e-03  8.45271721e-03\n",
      " -1.25404224e-02  4.65717129e-02 -2.96851387e-03 -5.35057345e-03\n",
      "  1.42052462e-02 -1.69882346e-02  2.17561773e-03  5.99054098e-02\n",
      " -1.59709547e-02  3.67274992e-02  6.90349862e-02  2.23804172e-02\n",
      "  1.69977061e-02 -2.66282130e-02  4.65735905e-02 -3.85713205e-02\n",
      "  2.59053968e-02 -2.73572914e-02 -1.20426726e-03  2.50926986e-02\n",
      " -1.60022620e-02  4.48563993e-02  2.07401495e-02 -4.56154235e-02\n",
      "  8.50843359e-03  5.74729443e-02  7.40508586e-02 -2.65443232e-03\n",
      " -3.90933454e-02  1.25618163e-03 -2.52902154e-02 -5.34737147e-02\n",
      "  5.99674433e-02  5.52830473e-02 -7.54617015e-03 -5.40560484e-02\n",
      " -7.25452974e-02  2.81076762e-03 -4.62694429e-02 -8.65417591e-04\n",
      "  4.09075152e-03  2.26795953e-02 -1.21706426e-02 -9.47365630e-03\n",
      "  2.52377037e-02  3.54435551e-03  2.97821909e-02  1.82955358e-02\n",
      " -1.23637198e-02  4.40889318e-03 -3.49614490e-03 -2.55065635e-02\n",
      "  1.06312223e-02 -3.24337222e-02  2.53524035e-02 -6.41557351e-02\n",
      "  6.93911174e-03  2.49600057e-02 -4.20531165e-03 -1.20535363e-02\n",
      "  9.90788732e-03 -1.08540408e-01 -4.05898467e-02 -2.76797414e-02\n",
      "  8.33132491e-02  2.51646694e-02  2.34612497e-03 -2.41455622e-02\n",
      " -3.31265703e-02 -5.72600327e-02 -2.29566563e-02 -2.03902125e-02\n",
      "  1.96675546e-02  3.61822359e-02  4.07850789e-03  2.20236275e-02\n",
      "  5.74525632e-03 -4.35841903e-02  5.97455911e-03  3.23318094e-02\n",
      "  7.36439694e-03 -1.87171660e-02  2.93470770e-02  3.05748209e-02\n",
      "  4.05962467e-02 -6.34447578e-03  2.52951421e-02 -3.25446352e-02\n",
      "  1.00313872e-02  1.50063187e-02  2.67197695e-02 -3.63441147e-02\n",
      " -5.77912554e-02  2.26656757e-02  2.47200448e-02 -2.00077239e-02\n",
      "  4.35071823e-04  1.46280145e-02  1.43800331e-02  3.39840129e-02\n",
      " -5.75931603e-03  5.70085645e-02  4.05291170e-02  3.50400172e-02\n",
      "  7.97299519e-02 -2.24885773e-02 -3.53292674e-02  9.86577943e-03\n",
      "  4.46071615e-03  2.77617984e-02  9.85991582e-03  4.97705750e-02\n",
      "  2.35751085e-03  3.11052594e-02  2.25695199e-03 -1.53418053e-02\n",
      " -7.09902048e-02 -8.53912458e-02  1.31427161e-02  2.84594316e-02\n",
      "  6.01482429e-02  3.53000499e-02  3.59884165e-02 -2.48528812e-02\n",
      "  2.12284811e-02 -2.66653113e-02  2.26863883e-02  1.89110090e-03\n",
      " -4.37320620e-02  1.33225117e-02  3.33122984e-02 -5.27875638e-03\n",
      " -3.04515846e-02  9.48688537e-02  1.02948528e-02  2.31469870e-02\n",
      "  5.42652095e-03 -1.02293193e-02 -2.45939642e-02 -1.99148385e-03\n",
      "  3.65195312e-02 -1.80687774e-02 -1.34664271e-02 -1.86175257e-02\n",
      " -5.69914989e-02 -7.22378641e-02  1.34117408e-02  3.38695496e-02\n",
      "  3.44138443e-02 -1.41145838e-02 -1.36754559e-02  9.59601104e-02\n",
      "  3.31358332e-03 -1.48215536e-02 -2.02330258e-02 -4.30976413e-03\n",
      "  2.67525595e-02 -1.27561232e-02 -1.86946858e-02 -6.40349739e-33\n",
      " -2.35966668e-02 -1.34113338e-02  2.84515247e-02 -5.91360359e-03\n",
      "  1.81529056e-02  1.74580738e-02 -6.41126037e-02  5.30113513e-03\n",
      " -5.72305452e-03 -3.36044319e-02  4.50982526e-03  4.15885411e-02\n",
      " -2.13368721e-02 -1.85040701e-02  3.47650349e-02  9.28913150e-03\n",
      "  3.48830298e-02  3.33195776e-02 -4.84676249e-02  1.75500829e-02\n",
      " -2.31653266e-02  2.37139910e-02  1.98196527e-03 -3.67337372e-03\n",
      "  1.11374864e-02 -1.17294956e-02  3.32878879e-03 -5.00286790e-03\n",
      " -2.68785711e-02  5.65307885e-02 -3.25085386e-03  9.36996599e-04\n",
      "  2.47347099e-03 -1.20958826e-02 -2.46913824e-02 -4.27539758e-02\n",
      " -3.78542580e-02 -2.90768035e-02  4.88805100e-02  4.54936596e-03\n",
      "  2.24149469e-02 -7.67751411e-02  6.10286593e-02  3.74028049e-02\n",
      " -4.70240340e-02 -2.28506280e-03 -4.72272886e-03 -3.21077034e-02\n",
      "  1.05831698e-02 -4.04227488e-02 -5.21918125e-02  5.17916195e-02\n",
      " -5.99653907e-02 -4.78611467e-03  1.14201689e-02  2.46366337e-02\n",
      " -2.94615794e-02  7.05405101e-02 -1.53828720e-02 -2.22982047e-03\n",
      "  2.45491881e-02  5.23614846e-02  9.72827002e-02  4.51960228e-03\n",
      "  3.32069438e-04  1.47557370e-02  3.13482694e-02  1.18709318e-02\n",
      " -3.07487659e-02 -3.20295990e-02  3.15699242e-02  5.98684512e-02\n",
      "  2.10699104e-02  2.54254714e-02  4.06341255e-02 -2.35910667e-03\n",
      " -5.50846756e-02  1.69150122e-02  6.68863999e-03  2.70140432e-02\n",
      "  2.59426013e-02  1.90511681e-02 -5.39215505e-02 -3.73604288e-03\n",
      " -5.40943630e-02 -1.11903381e-02 -4.85864803e-02 -1.72422193e-02\n",
      "  3.87929678e-02 -1.96023006e-02 -4.91341092e-02 -3.22576724e-02\n",
      "  2.71163844e-02 -4.25015911e-02 -1.20419199e-02  4.87761572e-02\n",
      " -3.56767103e-02  2.10060421e-02 -1.46668144e-02  8.23399499e-02\n",
      " -2.80398298e-02 -3.53153348e-02 -2.09718477e-02 -1.08398004e-02\n",
      "  7.14536980e-02 -1.63987419e-03  3.20053659e-02  8.43279250e-03\n",
      " -6.87586293e-02 -1.51859624e-02 -5.75990863e-02 -2.49422230e-02\n",
      "  1.15153957e-02  1.93568878e-02  2.68100798e-02 -1.96678768e-04\n",
      "  2.63409149e-02  2.88908910e-02 -1.69975888e-02  8.73193960e-04\n",
      " -4.65216637e-02 -6.35157200e-03 -6.94855489e-03  2.75604483e-02\n",
      " -3.68855298e-02 -5.17685004e-02 -4.11441661e-02  3.11102644e-02\n",
      "  4.39688452e-02 -4.16315384e-02 -4.30964902e-02  6.01030253e-02\n",
      "  3.03033346e-07  1.60740986e-02  2.93513499e-02 -2.09534280e-02\n",
      "  1.39084365e-02  2.02580430e-02  4.04020473e-02 -5.94391301e-02\n",
      " -3.72572914e-02  1.39955264e-02  9.07628145e-03 -2.00203992e-02\n",
      "  5.24539128e-02  5.84198013e-02  1.99676398e-02 -2.19935533e-02\n",
      "  5.74541353e-02 -1.33575164e-02 -1.77374277e-02 -5.87843992e-02\n",
      "  1.13836359e-02  7.46655464e-03  7.43599758e-02 -2.09662151e-02\n",
      "  7.06032291e-03  1.67391822e-02 -4.55243066e-02  1.80932712e-02\n",
      " -6.55851606e-03  8.50272179e-03  2.89537068e-02 -5.32746948e-02\n",
      "  2.19617803e-02 -7.76171731e-03 -4.52050753e-03  2.33755205e-02\n",
      " -1.68569237e-02  2.08565388e-02  8.82226005e-02  3.37970778e-02\n",
      "  5.01944683e-02  2.65611596e-02  1.00157429e-02 -1.11888442e-02\n",
      " -1.03893630e-01  7.43325651e-02  1.38925924e-03 -1.33506013e-02\n",
      " -2.48401780e-02 -1.20169797e-03 -2.19171345e-02 -5.53392991e-03\n",
      "  2.33898265e-03  2.31403913e-02  3.68332751e-02 -1.91735141e-02\n",
      " -4.48173024e-02 -1.21308519e-02 -3.53099853e-02 -4.54367064e-02\n",
      " -3.64732521e-04 -6.48427475e-03 -5.78781925e-02 -7.48005137e-02\n",
      "  2.31552236e-02  8.59116856e-03 -4.87045683e-02  2.05467772e-02\n",
      "  3.07489623e-34  4.59175184e-02 -5.56704961e-02  4.77665849e-03\n",
      "  4.68321219e-02  6.71808496e-02 -1.95879973e-02 -3.85415964e-02\n",
      "  2.32274774e-02 -9.90503840e-03 -3.49792354e-02 -1.57260988e-02]\n"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-30T05:51:08.279847Z",
     "start_time": "2024-11-30T05:51:05.310160Z"
    }
   },
   "cell_type": "code",
   "source": [
    "abstracts = [get_arxiv_abstract(obj[\"id\"]) for obj in output]\n",
    "top_relevant_pages = find_most_relevant_pages(relevant_pages, abstracts, 10)"
   ],
   "id": "21cb4be57d5c1cc5",
   "outputs": [],
   "execution_count": 22
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-30T05:53:55.789675Z",
     "start_time": "2024-11-30T05:53:30.313631Z"
    }
   },
   "cell_type": "code",
   "source": [
    "for key, obj in top_relevant_pages.items():\n",
    "    arxiv_id = res[key][\"id\"]\n",
    "    arxiv_abstract = obj[\"abstract\"]\n",
    "    text_segments = obj[\"text\"]\n",
    "    response: str = prompting_client.get_completions(\n",
    "        generate_summary_prompt_with_page_content(\n",
    "            abstract, arxiv_abstract, text_segments\n",
    "        ),\n",
    "        os.getenv(\"AZURE_PROMPTING_MODEL_VERSION\"),\n",
    "    )\n",
    "    obj[\"summary\"] = response\n",
    "    obj[\"citation\"] = get_arxiv_citation(arxiv_id)"
   ],
   "id": "bcb71db02d4898fb",
   "outputs": [],
   "execution_count": 25
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-30T05:54:00.896811Z",
     "start_time": "2024-11-30T05:54:00.893880Z"
    }
   },
   "cell_type": "code",
   "source": [
    "data = list(top_relevant_pages.values())\n",
    "print(generate_related_work_prompt(abstract, data))"
   ],
   "id": "2eaec628dcf0724f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    I am working on a research paper, and I need a well-written \"Related Work\" section. Below I'm providing you with the abstract of the paper I'm writing and a list of summaries of related works I've identified.\n",
      "    \n",
      "    Here's the abstract of my paper:\n",
      "    \"Large Language Models have shown impressive per- formance across a wide array of tasks involving both structured and unstructured textual data. More recently, adaptions of these models have drawn attention to their abilities to work with code across different programming languages. On this notion, different benchmarks for code generation, repair, or completion suggest that certain models have programming abilities comparable to or even surpass humans. In this work, we demonstrate that the performance on this benchmark does not translate to the innate ability of humans to appreciate the structural control flow of code. For this purpose, we extract code solutions from the Hu- manEval benchmark, which the relevant models perform very strongly on, and trace their execution path using function calls sampled from the respective test set. Using this dataset, we investigate the ability of 5 state-of-the-art LLMs to match the execution trace and find that, despite the model’s abilities to generate semantically identical code, they possess only limited ability to trace the execution path, especially for traces with increased length. We find that even the top-performing model, Gemini 1.5 Pro can only fully correctly generate the trace of 47% of HumanEval tasks. In addition, we introduce a specific subset for three key structures not, or only contained to a limited extent in Hu- manEval: Recursion, Parallel Processing, and Object Oriented Programming principles, including concepts like Inheritance and Polymorphism. Besides OOP, we show that none of the investigated models achieve an average accuracy of over 5% on the relevant traces. Aggregating these specialized parts with the ubiquitous HumanEval tasks, we present the Benchmark CoCoNUT: Code Control Flow for Navigation Understanding and Testing, which measures a models ability to trace the execu- tion of code upon relevant calls, including advanced structural components. We conclude that the current generation LLMs still need to significantly improve to enhance their code reasoning abilities. We hope our dataset can help researchers bridge this gap in the near future.\"\n",
      "    \n",
      "    Here's the list of summaries of the other related works I've found:\n",
      "    \n",
      "        Paper 1:\n",
      "        Summary: The cited paper introduces CodeJudge-Eval, a benchmark designed to assess large language models' (LLMs) code understanding abilities by evaluating their capacity to judge the correctness of code solutions, rather than just generating code. This approach highlights the limitations of traditional language-to-code benchmarks, which may not fully capture a model's understanding of code, similar to the concerns raised in my work about LLMs' ability to trace execution paths. Both studies emphasize the gap between code generation capabilities and deeper code reasoning or understanding, underscoring the need for more comprehensive evaluation methods to improve LLMs' code reasoning abilities.\n",
      "        Citation: Yuwei Zhao, Ziyang Luo, Yuchen Tian, Hongzhan Lin, Weixiang Yan, Annan Li, Jing Ma (2024). CodeJudge-Eval: Can Large Language Models be Good Judges in Code Understanding?. arXiv:2408.10718. https://arxiv.org/abs/2408.10718\n",
      "        \n",
      "        Paper 2:\n",
      "        Summary: The cited paper introduces CodeApex, a bilingual benchmark designed to evaluate the programming comprehension, code generation, and code correction abilities of Large Language Models (LLMs). It highlights the need for comprehensive evaluation across multiple programming tasks and languages, addressing limitations in existing benchmarks. This aligns with my research, which also critiques current benchmarks, such as HumanEval, for not adequately assessing LLMs' understanding of code execution paths and structural control flow. Both works emphasize the gap between LLMs' current capabilities and human-level programming understanding, suggesting that further development is needed to enhance LLMs' code reasoning abilities.\n",
      "        Citation: Jialin Song, Jonathan Raiman, Bryan Catanzaro (2024). Effective Large Language Model Debugging with Best-first Tree Search. arXiv:2407.19055. https://arxiv.org/abs/2407.19055\n",
      "        \n",
      "        Paper 3:\n",
      "        Summary: The cited paper investigates the capabilities and limitations of large language models (LLMs) in software engineering, particularly focusing on their ability to understand code syntax, static behaviors, and dynamic behaviors. It highlights that while LLMs, like GPT-4, excel in syntax comprehension and static analysis, they struggle with dynamic semantics, similar to the findings in my research where LLMs show limited ability to trace execution paths. Both studies underscore the need for improved code reasoning abilities in LLMs, especially concerning dynamic behavior understanding, which aligns with my work's emphasis on enhancing LLMs' comprehension of complex code structures and execution flows.\n",
      "        Citation: Jiasheng Zheng, Boxi Cao, Zhengzhao Ma, Ruotong Pan, Hongyu Lin, Yaojie Lu, Xianpei Han, Le Sun (2024). Beyond Correctness: Benchmarking Multi-dimensional Code Generation for Large Language Models. arXiv:2407.11470. https://arxiv.org/abs/2407.11470\n",
      "        \n",
      "        Paper 4:\n",
      "        Summary: The cited paper explores the limitations of self-repair in large language models (LLMs) like Code Llama, GPT-3.5, and GPT-4, particularly in their ability to debug and repair their own code. This is relevant to my research as it highlights the challenges LLMs face in understanding and correcting code, which aligns with my findings on their limited ability to trace execution paths and handle complex code structures. Both studies underscore the gap between current LLM capabilities and human-level code reasoning, suggesting that improvements are needed for better code comprehension and debugging.\n",
      "        Citation: Martin Riddell, Ansong Ni, Arman Cohan (2024). Quantifying Contamination in Evaluating Code Generation Capabilities of Language Models. arXiv:2403.04811. https://arxiv.org/abs/2403.04811\n",
      "        \n",
      "        Paper 5:\n",
      "        Summary: The cited paper introduces BigCodeBench, a benchmark designed to evaluate the ability of large language models (LLMs) to handle complex programming tasks involving multiple function calls across diverse libraries and domains. This is relevant to my research as both works highlight the limitations of current LLMs in understanding and executing complex code structures. While my work focuses on the models' ability to trace execution paths and handle advanced structural components like recursion and object-oriented programming, the cited paper emphasizes the need for compositional reasoning and precise function call usage. Both studies underscore the necessity for further advancements in LLMs to improve their code reasoning and execution capabilities.\n",
      "        Citation: Shanchao Liang, Yiran Hu, Nan Jiang, Lin Tan (2024). Can Language Models Replace Programmers? REPOCOD Says 'Not Yet'. arXiv:2410.21647. https://arxiv.org/abs/2410.21647\n",
      "        \n",
      "        Paper 6:\n",
      "        Summary: The cited paper addresses the programming language and task biases in existing code benchmarks, such as HumanEval, by introducing CRUXEVAL-X, a multilingual code reasoning benchmark covering 19 programming languages. This is relevant to my research as it highlights the limitations of current benchmarks, which predominantly focus on code generation in Python, similar to the HumanEval benchmark used in my study. While my work emphasizes the need for improved code reasoning capabilities in LLMs, particularly in tracing execution paths, the cited paper expands on this by proposing a comprehensive benchmark for evaluating code reasoning across multiple languages, thus complementing my focus on enhancing LLMs' code reasoning abilities.\n",
      "        Citation: Weixiang Yan, Haitian Liu, Yunkun Wang, Yunzhe Li, Qian Chen, Wen Wang, Tingyu Lin, Weishan Zhao, Li Zhu, Hari Sundaram, Shuiguang Deng (2023). CodeScope: An Execution-based Multilingual Multitask Multidimensional Benchmark for Evaluating LLMs on Code Understanding and Generation. arXiv:2311.08588. https://arxiv.org/abs/2311.08588\n",
      "        \n",
      "        Paper 7:\n",
      "        Summary: The cited paper introduces a novel algorithm, BESTER, which enhances the debugging capabilities of Large Language Models (LLMs) through self-reflection and best-first tree search, achieving state-of-the-art results in code generation benchmarks. This work is relevant to my research as it addresses the limitations of LLMs in handling complex programming tasks, similar to the challenges identified in my study regarding LLMs' ability to trace execution paths in code. While my research highlights the models' struggles with understanding structural control flow, the cited paper focuses on improving LLMs' debugging skills, which is a crucial aspect of code reasoning and refinement. Both studies underscore the need for advancements in LLMs' code reasoning abilities, albeit from different perspectives: execution tracing and debugging.\n",
      "        Citation: Terry Yue Zhuo, Minh Chien Vu, Jenny Chim, Han Hu, Wenhao Yu, Ratnadira Widyasari, Imam Nur Bani Yusuf, Haolan Zhan, Junda He, Indraneil Paul, Simon Brunner, Chen Gong, Thong Hoang, Armel Randy Zebaze, Xiaoheng Hong, Wen-Ding Li, Jean Kaddour, Ming Xu, Zhihan Zhang, Prateek Yadav, Naman Jain, Alex Gu, Zhoujun Cheng, Jiawei Liu, Qian Liu, Zijian Wang, David Lo, Binyuan Hui, Niklas Muennighoff, Daniel Fried, Xiaoning Du, Harm de Vries, Leandro Von Werra (2024). BigCodeBench: Benchmarking Code Generation with Diverse Function Calls and Complex Instructions. arXiv:2406.15877. https://arxiv.org/abs/2406.15877\n",
      "        \n",
      "        Paper 8:\n",
      "        Summary: The cited paper investigates data contamination in code generation benchmarks, highlighting how overlaps between training data and evaluation benchmarks like HumanEval can inflate model performance. This is relevant to my work as it underscores the potential limitations of current benchmarks in assessing true code reasoning abilities of LLMs. While my research focuses on the models' ability to trace execution paths and understand structural code components, the cited paper's findings suggest that observed performance might be partly due to memorization rather than genuine understanding, aligning with my conclusion that LLMs need improvement in code reasoning.\n",
      "        Citation: Miltiadis Allamanis, Sheena Panthaplackel, Pengcheng Yin (2024). Unsupervised Evaluation of Code LLMs with Round-Trip Correctness. arXiv:2402.08699. https://arxiv.org/abs/2402.08699\n",
      "        \n",
      "        Paper 9:\n",
      "        Summary: The cited paper introduces EvoEval, a benchmark suite designed to address the limitations of existing code generation benchmarks, such as HumanEval, by evolving them into diverse domains to provide a comprehensive evaluation of LLM coding abilities. This is relevant to my research as both works highlight the inadequacies of current benchmarks in assessing the true capabilities of LLMs, particularly in understanding and tracing code execution paths. While my work focuses on the structural control flow and execution tracing, the cited paper emphasizes the need for benchmarks that prevent overfitting and better evaluate program synthesis abilities. Both studies underscore the necessity for improved benchmarks to enhance the evaluation of LLMs in code-related tasks.\n",
      "        Citation: Aiden Grossman, Ludger Paehler, Konstantinos Parasyris, Tal Ben-Nun, Jacob Hegna, William Moses, Jose M Monsalve Diaz, Mircea Trofin, Johannes Doerfert (2023). ComPile: A Large IR Dataset from Production Sources. arXiv:2309.15432. https://arxiv.org/abs/2309.15432\n",
      "        \n",
      "        Paper 10:\n",
      "        Summary: The cited paper, \"ComPile: A Large IR Dataset from Production Sources,\" presents a significant advancement in leveraging the LLVM compiler infrastructure to create a comprehensive dataset of intermediate representations (IR) from languages like Rust, Swift, Julia, and C/C++. This dataset aims to enhance the training of large language models and machine-learned compiler components by utilizing the structured nature of code, which contrasts with the unstructured code focus in current models. This approach aligns with my research on improving code reasoning abilities in large language models, as both works emphasize the importance of understanding code structure. While my work highlights the limitations of current models in tracing execution paths, the cited paper provides a foundational dataset that could potentially improve models' structural comprehension, thereby addressing some of the challenges identified in my research.\n",
      "        Citation: Ruiyang Xu, Jialun Cao, Yaojie Lu, Hongyu Lin, Xianpei Han, Ben He, Shing-Chi Cheung, Le Sun (2024). CRUXEval-X: A Benchmark for Multilingual Code Reasoning, Understanding and Execution. arXiv:2408.13001. https://arxiv.org/abs/2408.13001\n",
      "        \n",
      "    \n",
      "    Instructions:\n",
      "    Using all the information given above, your goal is to write a cohesive and well-structured \"Related Work\" section. \n",
      "    Draw connections between the related papers and my research and highlight similarities and differences. \n",
      "    Please also make sure to put my work into the overall context of the provided related works in a summarizing paragraph at the end. \n",
      "    If multiple related works have a common point/theme, make sure to group them and refer to them in the same paragraph. \n",
      "    When referring to content from specific papers you must also cite the respective paper properly (i.e. cite right after your direct/indirect quotes).\n",
      "    Also, make sure the related works section consists of multiple paragraphs (6 at most) which are concise, but not too concise (e.g. avoid 2-sentence paragraphs).\n",
      "    \n"
     ]
    }
   ],
   "execution_count": 26
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-30T05:54:19.239285Z",
     "start_time": "2024-11-30T05:54:09.631178Z"
    }
   },
   "cell_type": "code",
   "source": [
    "response: str = prompting_client.get_completions(\n",
    "    generate_related_work_prompt(abstract, data),\n",
    "    os.getenv(\"AZURE_PROMPTING_MODEL_VERSION\"),\n",
    ")\n",
    "print(response)"
   ],
   "id": "9b038e0d9ddb0651",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Related Work\n",
      "\n",
      "The exploration of large language models (LLMs) in the domain of code understanding and generation has been a focal point of recent research, with various studies highlighting both the potential and limitations of these models. A significant body of work has concentrated on developing benchmarks to evaluate LLMs' code reasoning abilities beyond mere code generation. Zhao et al. (2024) introduced CodeJudge-Eval, a benchmark that assesses LLMs' capacity to judge the correctness of code solutions, emphasizing the gap between code generation and deeper code reasoning abilities. This aligns with our research, which critiques the current benchmarks like HumanEval for not adequately assessing LLMs' understanding of code execution paths and structural control flow. Similarly, Song et al. (2024) with CodeApex, and Zheng et al. (2024) have underscored the need for comprehensive evaluation methods that address the limitations of existing benchmarks, echoing our findings on the necessity for improved code reasoning capabilities in LLMs.\n",
      "\n",
      "Another theme prevalent in the literature is the challenge LLMs face in understanding dynamic code behaviors and complex programming structures. Zheng et al. (2024) highlighted that while models like GPT-4 excel in syntax comprehension, they struggle with dynamic semantics, a limitation also identified in our study where LLMs show limited ability to trace execution paths. Liang et al. (2024) further explored these limitations by introducing BigCodeBench, which evaluates LLMs' ability to handle complex programming tasks involving multiple function calls. This is particularly relevant to our work, as both studies emphasize the need for advancements in LLMs' code reasoning and execution capabilities, especially concerning dynamic behavior understanding and structural control flow.\n",
      "\n",
      "The issue of self-repair and debugging in LLMs has also been a subject of investigation. Riddell et al. (2024) explored the limitations of models like Code Llama and GPT-4 in debugging and repairing their own code, which resonates with our findings on LLMs' limited ability to handle complex code structures. Zhuo et al. (2024) introduced BESTER, an algorithm that enhances LLMs' debugging capabilities through self-reflection and best-first tree search, achieving state-of-the-art results in code generation benchmarks. While our research focuses on execution tracing, the improvements in debugging skills highlighted by Zhuo et al. are crucial for advancing LLMs' code reasoning and refinement abilities.\n",
      "\n",
      "The potential biases and limitations of existing benchmarks have been critically examined in recent studies. Allamanis et al. (2024) investigated data contamination in code generation benchmarks, highlighting how overlaps between training data and evaluation benchmarks like HumanEval can inflate model performance. This finding is pertinent to our work, as it suggests that observed performance might be partly due to memorization rather than genuine understanding, aligning with our conclusion that LLMs need improvement in code reasoning. Yan et al. (2023) addressed programming language and task biases by introducing CRUXEVAL-X, a multilingual code reasoning benchmark, which complements our focus on enhancing LLMs' code reasoning abilities by proposing a comprehensive evaluation across multiple languages.\n",
      "\n",
      "In summary, the current body of research underscores the limitations of LLMs in understanding and executing complex code structures, a challenge that our work also addresses by introducing the Benchmark CoCoNUT. While various studies have proposed new benchmarks and algorithms to enhance LLMs' code reasoning abilities, our research uniquely contributes by focusing on the models' ability to trace execution paths and handle advanced structural components like recursion and object-oriented programming. Together, these studies highlight the necessity for improved benchmarks and evaluation methods to bridge the gap between LLMs' current capabilities and human-level programming understanding.\n"
     ]
    }
   ],
   "execution_count": 27
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "print([obj[\"citation\"] for obj in top_relevant_pages.values()])",
   "id": "e30e2389b2700929"
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  }
 },
 "nbformat": 5,
 "nbformat_minor": 9
}
