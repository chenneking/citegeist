longlist_length,longlist_ratings,shortlist_length,shortlist_ratings,rating_source_ours,rating_ours_gpt,related_works,citations
30,8|7|8|6|7|7|8|7|7|7|8|8|7|7|8|7|8|8|8|7|5|7|7|7|7|8|6|8|8|8,10,7|7|7|7|7|8|6|8|5|8,source,source,"In recent years, the capabilities of Large Language Models (LLMs) in code generation and understanding have been extensively explored, with various benchmarks and methodologies developed to evaluate their performance. A significant body of work has focused on enhancing the debugging and reasoning abilities of LLMs, which aligns closely with our research on the limitations of these models in tracing execution paths and understanding complex code structures. For instance, the BESTER algorithm, introduced by Jialin Song et al., employs self-reflection and best-first tree search to improve LLMs' debugging capabilities, achieving state-of-the-art results in code generation benchmarks (Jialin Song et al., 2024). This approach complements our findings by addressing the challenges LLMs face in understanding code control flow, suggesting that improved debugging skills could enhance their reasoning abilities.

Several studies have introduced new benchmarks to evaluate LLMs' performance on complex programming tasks, highlighting the limitations of existing benchmarks like HumanEval. Terry Yue Zhuo et al. introduced BigCodeBench, which focuses on diverse function calls and compositional reasoning, underscoring the need for advancements in LLMs to handle complex instructions with precision (Terry Yue Zhuo et al., 2024). Similarly, Jiasheng Zheng et al. proposed the RACE benchmark, which evaluates LLMs on multiple dimensions of code quality, including readability and maintainability, critiquing correctness-centric benchmarks for not fully capturing LLMs' nuanced capabilities (Jiasheng Zheng et al., 2024). These works resonate with our research, which critiques current benchmarks for not adequately assessing LLMs' abilities to trace execution paths and handle advanced structural components.

The exploration of alternative evaluation methods and frameworks is another area of active research. Miltiadis Allamanis et al. introduced round-trip correctness (RTC) as a method for unsupervised evaluation of code synthesis and editing, providing a broader framework for assessing semantic equivalence in code generation (Miltiadis Allamanis et al., 2024). This approach aligns with our focus on evaluating LLMs' ability to trace execution paths, offering insights into improving code reasoning capabilities. Additionally, Lingyue Fu et al. developed CodeApex, a bilingual benchmark for evaluating programming comprehension and code correction abilities, emphasizing the need for comprehensive evaluation frameworks (Lingyue Fu et al., 2023). Both studies highlight the necessity for more robust evaluation metrics to enhance LLMs' performance in real-world coding tasks.

The limitations of LLMs in handling complex code structures and generating accurate code have been further explored in empirical studies. Shihan Dou et al. conducted an extensive study on the challenges LLMs face in code generation, introducing a taxonomy of bugs and proposing a real-world benchmark to better evaluate LLM performance (Shihan Dou et al., 2024). This study complements our work by emphasizing the need for improved benchmarks and methodologies to enhance LLMs' code reasoning abilities. Similarly, Zhijie Wang et al. developed a taxonomy of code generation errors based on semantic and syntactic characteristics, providing insights into the types of errors LLMs make and informing the limitations in their code reasoning abilities (Zhijie Wang et al., 2024).

Finally, the importance of structured code data and multilingual evaluation environments has been highlighted in recent research. Aiden Grossman et al. leveraged the LLVM compiler infrastructure to create a dataset of intermediate representations (IR) from multiple programming languages, addressing the structural understanding of code, which is critical for handling complex code structures like recursion and parallel processing (Aiden Grossman et al., 2023). Weixiang Yan et al. introduced CodeScope, a benchmark for evaluating LLMs on code understanding and generation tasks across 43 programming languages, emphasizing execution-based metrics (Weixiang Yan et al., 2023). These studies align with our goal of improving LLMs' code reasoning capabilities, particularly in advanced structural components, and underscore the necessity for LLMs to enhance their understanding of code control flow.","Miltiadis Allamanis, Sheena Panthaplackel, Pengcheng Yin (2024). Unsupervised Evaluation of Code LLMs with Round-Trip Correctness. arXiv:2402.08699. https://arxiv.org/abs/2402.08699
Shihan Dou, Haoxiang Jia, Shenxi Wu, Huiyuan Zheng, Weikang Zhou, Muling Wu, Mingxu Chai, Jessica Fan, Caishuang Huang, Yunbo Tao, Yan Liu, Enyu Zhou, Ming Zhang, Yuhao Zhou, Yueming Wu, Rui Zheng, Ming Wen, Rongxiang Weng, Jingang Wang, Xunliang Cai, Tao Gui, Xipeng Qiu, Qi Zhang, Xuanjing Huang (2024). What's Wrong with Your Code Generated by Large Language Models? An Extensive Study. arXiv:2407.06153. https://arxiv.org/abs/2407.06153
Jiasheng Zheng, Boxi Cao, Zhengzhao Ma, Ruotong Pan, Hongyu Lin, Yaojie Lu, Xianpei Han, Le Sun (2024). Beyond Correctness: Benchmarking Multi-dimensional Code Generation for Large Language Models. arXiv:2407.11470. https://arxiv.org/abs/2407.11470
Aiden Grossman, Ludger Paehler, Konstantinos Parasyris, Tal Ben-Nun, Jacob Hegna, William Moses, Jose M Monsalve Diaz, Mircea Trofin, Johannes Doerfert (2023). ComPile: A Large IR Dataset from Production Sources. arXiv:2309.15432. https://arxiv.org/abs/2309.15432
Jialin Song, Jonathan Raiman, Bryan Catanzaro (2024). Effective Large Language Model Debugging with Best-first Tree Search. arXiv:2407.19055. https://arxiv.org/abs/2407.19055
Weixiang Yan, Haitian Liu, Yunkun Wang, Yunzhe Li, Qian Chen, Wen Wang, Tingyu Lin, Weishan Zhao, Li Zhu, Hari Sundaram, Shuiguang Deng (2023). CodeScope: An Execution-based Multilingual Multitask Multidimensional Benchmark for Evaluating LLMs on Code Understanding and Generation. arXiv:2311.08588. https://arxiv.org/abs/2311.08588
Terry Yue Zhuo, Minh Chien Vu, Jenny Chim, Han Hu, Wenhao Yu, Ratnadira Widyasari, Imam Nur Bani Yusuf, Haolan Zhan, Junda He, Indraneil Paul, Simon Brunner, Chen Gong, Thong Hoang, Armel Randy Zebaze, Xiaoheng Hong, Wen-Ding Li, Jean Kaddour, Ming Xu, Zhihan Zhang, Prateek Yadav, Naman Jain, Alex Gu, Zhoujun Cheng, Jiawei Liu, Qian Liu, Zijian Wang, David Lo, Binyuan Hui, Niklas Muennighoff, Daniel Fried, Xiaoning Du, Harm de Vries, Leandro Von Werra (2024). BigCodeBench: Benchmarking Code Generation with Diverse Function Calls and Complex Instructions. arXiv:2406.15877. https://arxiv.org/abs/2406.15877
Lingyue Fu, Huacan Chai, Shuang Luo, Kounianhua Du, Weiming Zhang, Longteng Fan, Jiayi Lei, Renting Rui, Jianghao Lin, Yuchen Fang, Yifan Liu, Jingkuan Wang, Siyuan Qi, Kangning Zhang, Weinan Zhang, Yong Yu (2023). CodeApex: A Bilingual Programming Evaluation Benchmark for Large Language Models. arXiv:2309.01940. https://arxiv.org/abs/2309.01940
Zhijie Wang, Zijie Zhou, Da Song, Yuheng Huang, Shengmai Chen, Lei Ma, Tianyi Zhang (2024). Where Do Large Language Models Fail When Generating Code?. arXiv:2406.08731. https://arxiv.org/abs/2406.08731"
30,8|8|8|5|5|7|3|8|7|4|7|7|6|3|5|7|7|7|7|3|4|7|5|3|6|3|7|6|3|7,10,4|8|7|8|3|3|5|7|5|7,target,source,"The increasing demand for privacy in machine learning (ML) has led to a diverse array of research efforts aimed at safeguarding both data and model parameters in distributed environments. A significant body of work has focused on ensuring data privacy during ML processes. For instance, Drynx is a decentralized system that employs zero-knowledge proofs, homomorphic encryption, and differential privacy to maintain data confidentiality and computation correctness in distributed datasets (Froelicher et al., 2019). Similarly, TorMentor emphasizes data privacy through brokered learning and differential privacy, providing a complementary approach to my research, which focuses on model parameter privacy using zkSNARKs (Fung et al., 2018). These works highlight the importance of privacy in distributed ML, albeit with a primary focus on data rather than model parameters.

In contrast, other research efforts have concentrated on the privacy and verifiability of model parameters during inference, aligning more closely with the objectives of my work. The survey by Xing et al. (2023) provides a comprehensive overview of zero-knowledge proof-based verifiable machine learning (ZKP-VML) technologies, addressing trustworthiness issues in outsourced model computations. This aligns with my research's focus on ensuring privacy and verifiability in distributed ML inference using zkSNARKs. Additionally, the work by Kang et al. (2022) explores the use of ZK-SNARKs for verifying ML model inference non-interactively, specifically for ImageNet-scale models, which parallels my approach in leveraging zkSNARKs to enhance trust and security in distributed ML systems.

Several studies have explored innovative methodologies to achieve privacy and verifiability in ML inference. VeriSplit, for example, offers a framework for offloading ML inferences to locally available devices, addressing privacy and integrity concerns through masking techniques and a commitment-based verification protocol (Zhang et al., 2024). This approach, while differing in technical implementation, shares the goal of enhancing trust and security in distributed ML inference with my research. Similarly, ZkAudit enables trustless audits of ML models by allowing model providers to keep their model weights and data secret while permitting third-party verification through zero-knowledge proofs (Waiwitlikhit et al., 2024). Both VeriSplit and ZkAudit underscore the potential of cryptographic techniques in creating secure and verifiable ML inference frameworks.

The exploration of privacy-preserving mechanisms in hierarchical and decentralized ML setups further complements my research. The privateMDI protocol, designed for hierarchical setups, focuses on reducing communication overhead and ensuring privacy for both data and model parameters through additive secret sharing and linearly homomorphic encryption (Dehkordi et al., 2024). This work aligns with my approach by addressing privacy concerns in distributed ML inference, particularly in hierarchical models. Additionally, Shatter employs virtual nodes to disseminate model chunks, preventing attackers from accessing full models and obscuring the identity of the original node, thereby enhancing privacy in decentralized learning (Biswas et al., 2024). These studies highlight the diverse strategies employed to maintain privacy in distributed ML environments.

Finally, the integration of hardware-based solutions with cryptographic techniques presents a promising avenue for enhancing ML privacy and verifiability. LAMINATOR, for instance, utilizes Trusted Execution Environments (TEEs) to provide efficient and scalable attestations for ML properties, addressing concerns of verifiability and trust (Duddu et al., 2024). This approach complements my research by demonstrating the potential of combining hardware-assisted attestations with cryptographic methods like zkSNARKs to achieve secure and reliable ML deployments. Collectively, these related works underscore the multifaceted nature of privacy and verifiability challenges in ML, highlighting the need for continued innovation and collaboration across different methodologies and application contexts.","Han Zhang, Zifan Wang, Mihir Dhamankar, Matt Fredrikson, Yuvraj Agarwal (2024). VeriSplit: Secure and Practical Offloading of Machine Learning Inferences across IoT Devices. arXiv:2406.00586. https://arxiv.org/abs/2406.00586
Suppakit Waiwitlikhit, Ion Stoica, Yi Sun, Tatsunori Hashimoto, Daniel Kang (2024). Trustless Audits without Revealing Data or Models. arXiv:2404.04500. https://arxiv.org/abs/2404.04500
Vasisht Duddu, Oskari Järvinen, Lachlan J Gunn, N Asokan (2024). Laminator: Verifiable ML Property Cards using Hardware-assisted Attestations. arXiv:2406.17548. https://arxiv.org/abs/2406.17548
Clement Fung, Jamie Koerner, Stewart Grant, Ivan Beschastnikh (2018). Dancing in the Dark: Private Multi-Party Machine Learning in an Untrusted Setting. arXiv:1811.09712. https://arxiv.org/abs/1811.09712
Tianwei Zhang, Zecheng He, Ruby B. Lee (2018). Privacy-preserving Machine Learning through Data Obfuscation. arXiv:1807.01860. https://arxiv.org/abs/1807.01860
Sayan Biswas, Mathieu Even, Anne-Marie Kermarrec, Laurent Massoulie, Rafael Pires, Rishi Sharma, Martijn de Vos (2024). Noiseless Privacy-Preserving Decentralized Learning. arXiv:2404.09536. https://arxiv.org/abs/2404.09536
Zhibo Xing, Zijian Zhang, Jiamou Liu, Ziang Zhang, Meng Li, Liehuang Zhu, Giovanni Russello (2023). Zero-knowledge Proof Meets Machine Learning in Verifiability: A Survey. arXiv:2310.14848. https://arxiv.org/abs/2310.14848
David Froelicher, Juan R. Troncoso-Pastoriza, Joao Sa Sousa, Jean-Pierre Hubaux (2019). Drynx: Decentralized, Secure, Verifiable System for Statistical Queries and Machine Learning on Distributed Datasets. arXiv:1902.03785. https://arxiv.org/abs/1902.03785
Daniel Kang, Tatsunori Hashimoto, Ion Stoica, Yi Sun (2022). Scaling up Trustless DNN Inference with Zero-Knowledge Proofs. arXiv:2210.08674. https://arxiv.org/abs/2210.08674"
30,9|8|8|8|8|8|8|8|8|8|7|9|8|7|5|7|6|5|8|8|7|7|8|8|5|7|6|6|7|7,10,8|8|8|8|8|8|7|5|9|8,source,source,"The exploration of spiking neural networks (SNNs) as energy-efficient alternatives to traditional artificial neural networks (ANNs) in natural language processing (NLP) has gained significant traction in recent years. This research direction is motivated by the high energy consumption of conventional Transformer architectures, which are widely used in NLP tasks. Several studies have sought to address this challenge by developing SNN-based models that maintain competitive performance while significantly reducing energy usage. For instance, SpikeLM introduces a fully spike-driven language model that enhances spike encoding to bridge the performance gap between SNNs and ANNs in language modeling tasks (Xing et al., 2024). Similarly, SpikeGPT modifies the Transformer block to reduce computational complexity, demonstrating the potential of SNNs to achieve energy efficiency in language generation (Zhu et al., 2023). These works align with our research on SpikeDecoder, a spike-based Transformer decoder model, by emphasizing the energy efficiency of SNNs in NLP applications.

The challenge of encoding text into spike trains for SNNs is a critical aspect of developing energy-efficient NLP models. Knipper et al. (2024) explore various encoding methods, including a novel deterministic rate-coding technique, to improve performance in sentiment analysis tasks. This study highlights the importance of effective text encoding for SNNs, which is directly relevant to our work on SpikeDecoder, where we investigate different embedding methods to project text data into spike-range. Additionally, the work by Lv et al. (2024) on spiking convolutional neural networks for text classification introduces a ""conversion + fine-tuning"" method, demonstrating the potential of SNNs to achieve comparable performance to deep neural networks with reduced energy consumption. These studies provide valuable insights into encoding techniques and training methods that are crucial for the development of energy-efficient spike-based models.

The conversion of ANN models to SNNs is another area of active research, with several studies focusing on maintaining accuracy and performance in the transition. SpikeZIP-TF presents a novel ANN-to-SNN conversion method that achieves equivalence between ANN and SNN without accuracy degradation, specifically for Transformer-based architectures (You et al., 2024). This approach informs our exploration of spike-compatible normalization techniques and residual connections in SpikeDecoder. Similarly, BrainTransformers highlight the development of SNN-compatible Transformer components and a three-stage training approach, addressing challenges similar to those in our work, such as the conversion of ANN models to SNNs and the exploration of synaptic plasticity (Tang & Zhu, 2024). These studies underscore the potential for energy-efficient alternatives in NLP by leveraging SNNs.

The potential of SNNs to reduce energy consumption while maintaining performance parity with ANNs is further demonstrated in the work by Stanojevic et al. (2023), which leverages time-to-first-spike (TTFS) coding for image classification tasks. This study highlights the importance of robust gradient descent methods in overcoming training challenges in SNNs, which is particularly relevant to our work on SpikeDecoder. Additionally, Yao et al. (2024) present a general training framework for SNNs that enhances feature learning and activation efficiency, providing insights into optimizing neuron states and spike feature learning. These approaches offer valuable strategies for enhancing the spike-based components of our model, contributing to the development of a low-power Transformer decoder for NLP.

Finally, the application of SNNs in language tasks is further explored in the work on SpikeBERT, which employs a two-stage knowledge distillation method to transfer knowledge from BERT to SNNs, achieving competitive performance with significantly reduced energy consumption (Lv et al., 2023). This study, along with the comprehensive review by Zhou et al. (2024) on training high-performance deep SNNs, highlights the potential of SNNs to bridge the performance gap with traditional ANNs while maintaining energy efficiency. Both works focus on leveraging the energy-efficient properties of SNNs while addressing training challenges and performance optimization, aligning with our research on developing SpikeDecoder, a spike-based Transformer model for NLP. These studies collectively underscore the promise of SNNs as a viable solution for energy-efficient NLP applications, providing a foundation for future advancements in this field.","Changze Lv, Jianhan Xu, Xiaoqing Zheng (2024). Spiking Convolutional Neural Networks for Text Classification. arXiv:2406.19230. https://arxiv.org/abs/2406.19230
Rui-Jie Zhu, Qihang Zhao, Guoqi Li, Jason K. Eshraghian (2023). SpikeGPT: Generative Pre-trained Language Model with Spiking Neural Networks. arXiv:2302.13939. https://arxiv.org/abs/2302.13939
Ana Stanojevic, Stanisław Woźniak, Guillaume Bellec, Giovanni Cherubini, Angeliki Pantazi, Wulfram Gerstner (2023). High-performance deep spiking neural networks with 0.3 spikes per neuron. arXiv:2306.08744. https://arxiv.org/abs/2306.08744
Changze Lv, Tianlong Li, Jianhan Xu, Chenxi Gu, Zixuan Ling, Cenyuan Zhang, Xiaoqing Zheng, Xuanjing Huang (2023). SpikeBERT: A Language Spikformer Learned from BERT with Knowledge Distillation. arXiv:2308.15122. https://arxiv.org/abs/2308.15122
Chenlin Zhou, Han Zhang, Liutao Yu, Yumin Ye, Zhaokun Zhou, Liwei Huang, Zhengyu Ma, Xiaopeng Fan, Huihui Zhou, Yonghong Tian (2024). Direct Training High-Performance Deep Spiking Neural Networks: A Review of Theories and Methods. arXiv:2405.04289. https://arxiv.org/abs/2405.04289
Zhengzheng Tang, Eva Zhu (2024). BrainTransformers: SNN-LLM. arXiv:2410.14687. https://arxiv.org/abs/2410.14687
Kang You, Zekai Xu, Chen Nie, Zhijie Deng, Qinghai Guo, Xiang Wang, Zhezhi He (2024). SpikeZIP-TF: Conversion is All You Need for Transformer-based SNN. arXiv:2406.03470. https://arxiv.org/abs/2406.03470
Xingrun Xing, Zheng Zhang, Ziyi Ni, Shitao Xiao, Yiming Ju, Siqi Fan, Yequan Wang, Jiajun Zhang, Guoqi Li (2024). SpikeLM: Towards General Spike-Driven Language Modeling via Elastic Bi-Spiking Mechanisms. arXiv:2406.03287. https://arxiv.org/abs/2406.03287
Yunpeng Yao, Man Wu, Zheng Chen, Renyuan Zhang (2024). Training a General Spiking Neural Network with Improved Efficiency and Minimum Latency. arXiv:2401.10843. https://arxiv.org/abs/2401.10843"
30,8|8|8|8|8|8|8|8|8|8|8|8|8|8|8|7|8|8|8|8|7|8|7|8|8|8|6|8|7|4,10,8|8|8|7|8|8|8|8|7|8,source,source,"The field of Multimodal Large Language Models (MLLMs) has seen significant advancements, particularly in the realm of visual understanding. A comprehensive survey by Heqing Zou et al. (2024) highlights the unique challenges posed by long video understanding, emphasizing the need for models to handle fine-grained spatiotemporal details and long-term dependencies. This aligns with our research focus on enhancing video understanding through data augmentation and instruction diversity. While Zou et al. review existing methodologies and challenges, our study proposes a novel approach, T2Vid, to improve training efficiency and performance by synthesizing video-like samples, addressing some of the limitations identified in their survey.

Several studies have explored the integration of video-specific information into LLM-based frameworks to enhance long-video understanding. Kanchana Ranasinghe et al. (2024) achieve state-of-the-art performance by leveraging object-centric modalities and natural language fusion, which complements our approach of enriching instruction diversity through data augmentation. Similarly, the work by Zijia Zhao et al. (2024) introduces VideoNIAH, a synthetic video generation framework that enhances the evaluation of video understanding in MLLMs by decoupling video content from query-responses. While our research emphasizes efficient training schemes, Zhao et al. provide a complementary evaluation framework that isolates specific skills, offering a more targeted assessment of video understanding capabilities.

The challenges of long video comprehension are further addressed by Weihan Wang et al. (2024) through LVBench, a benchmark specifically designed for long video understanding. This work is relevant to our research as it highlights the complexities of long video comprehension, which we aim to tackle by enhancing instruction diversity and training efficiency. In parallel, Xiangyu Zeng et al. (2024) introduce TimeSuite, a framework that incorporates grounding supervision to improve long video understanding capabilities. While our work focuses on data augmentation, TimeSuite addresses temporal grounding challenges through architectural and dataset enhancements, both contributing to advancing MLLMs' capabilities in video understanding.

Innovative data processing techniques are also explored by Bo He et al. (2024) with the introduction of a Memory-Augmented Large Multimodal Model (MA-LMM) designed for efficient long-term video understanding. This approach complements our exploration of fine-tuning pre-trained image-LLMs for video data, particularly in enhancing temporal understanding capabilities. Similarly, Yang Jin et al. (2024) present Video-LaVIT, a framework that decomposes videos into keyframes and temporal motions for efficient integration with LLMs. Both studies aim to improve video comprehension and generation, with our research focusing on data augmentation and instruction diversity.

Lastly, the challenge of handling lengthy contexts in MLLMs is addressed by Jianing Zhou et al. (2024) through a hybrid transformer-MAMBA model that efficiently manages long contexts, enhancing inference efficiency. This is relevant to our work as it highlights the importance of efficient context handling, aligning with our focus on improving learning efficiency in video data. Additionally, Yuxuan Wang et al. (2024) introduce the Temporal Grounding Bridge (TGB) framework, which enhances temporal capabilities by efficiently grounding temporal features and extending context window sizes. While our research emphasizes data augmentation with T2Vid, the TGB framework offers complementary strategies for advancing video understanding in MLLMs, both aiming to overcome inefficiencies in processing long video sequences.","Heqing Zou, Tianze Luo, Guiyang Xie, Victor, Zhang, Fengmao Lv, Guangcong Wang, Junyang Chen, Zhuochen Wang, Hansheng Zhang, Huaijian Zhang (2024). From Seconds to Hours: Reviewing MultiModal Large Language Models on Comprehensive Long Video Understanding. arXiv:2409.18938. https://arxiv.org/abs/2409.18938
Jianing Zhou, Han Li, Shuai Zhang, Ning Xie, Ruijie Wang, Xiaohan Nie, Sheng Liu, Lingyun Wang (2024). Multimodal Instruction Tuning with Hybrid State Space Models. arXiv:2411.08840. https://arxiv.org/abs/2411.08840
Bo He, Hengduo Li, Young Kyun Jang, Menglin Jia, Xuefei Cao, Ashish Shah, Abhinav Shrivastava, Ser-Nam Lim (2024). MA-LMM: Memory-Augmented Large Multimodal Model for Long-Term Video Understanding. arXiv:2404.05726. https://arxiv.org/abs/2404.05726
Weihan Wang, Zehai He, Wenyi Hong, Yean Cheng, Xiaohan Zhang, Ji Qi, Xiaotao Gu, Shiyu Huang, Bin Xu, Yuxiao Dong, Ming Ding, Jie Tang (2024). LVBench: An Extreme Long Video Understanding Benchmark. arXiv:2406.08035. https://arxiv.org/abs/2406.08035
Kanchana Ranasinghe, Xiang Li, Kumara Kahatapitiya, Michael S. Ryoo (2024). Understanding Long Videos with Multimodal Language Models. arXiv:2403.16998. https://arxiv.org/abs/2403.16998
Zijia Zhao, Haoyu Lu, Yuqi Huo, Yifan Du, Tongtian Yue, Longteng Guo, Bingning Wang, Weipeng Chen, Jing Liu (2024). Needle In A Video Haystack: A Scalable Synthetic Evaluator for Video MLLMs. arXiv:2406.09367. https://arxiv.org/abs/2406.09367
Yuxuan Wang, Yueqian Wang, Pengfei Wu, Jianxin Liang, Dongyan Zhao, Yang Liu, Zilong Zheng (2024). Efficient Temporal Extrapolation of Multimodal Large Language Models with Temporal Grounding Bridge. arXiv:2402.16050. https://arxiv.org/abs/2402.16050
Xiangyu Zeng, Kunchang Li, Chenting Wang, Xinhao Li, Tianxiang Jiang, Ziang Yan, Songze Li, Yansong Shi, Zhengrong Yue, Yi Wang, Yali Wang, Yu Qiao, Limin Wang (2024). TimeSuite: Improving MLLMs for Long Video Understanding via Grounded Tuning. arXiv:2410.19702. https://arxiv.org/abs/2410.19702
Yang Jin, Zhicheng Sun, Kun Xu, Kun Xu, Liwei Chen, Hao Jiang, Quzhe Huang, Chengru Song, Yuliang Liu, Di Zhang, Yang Song, Kun Gai, Yadong Mu (2024). Video-LaVIT: Unified Video-Language Pre-training with Decoupled Visual-Motional Tokenization. arXiv:2402.03161. https://arxiv.org/abs/2402.03161"
30,8|6|5|7|8|7|7|5|7|7|7|7|7|7|3|8|7|8|7|6|7|6|8|8|3|7|7|7|7|8,10,7|7|6|7|7|7|7|6|8|5,source,source,"In recent years, there has been a growing interest in enhancing the reasoning capabilities of Large Language Models (LLMs) through innovative training methodologies and frameworks. A significant body of work has focused on structured reasoning processes, which align closely with the objectives of our Reverse-Enhanced Thinking (REVTHINK) framework. For instance, Quiet-STaR and Reasoning Paths Optimization (RPO) both emphasize the importance of structured reasoning paths to improve model performance. Quiet-STaR enhances reasoning by generating rationales for each token, similar to the reverse thinking approach in REVTHINK, but without task-specific fine-tuning (Zelikman et al., 2024). RPO, on the other hand, explores diverse reasoning paths and provides contrastive feedback to optimize reasoning paths, preventing divergence into errors (Chia et al., 2024). Both methods, like REVTHINK, aim to enhance reasoning capabilities without relying heavily on large-scale human annotations, focusing on multi-step reasoning tasks.

Another theme in the literature is the integration of knowledge into model parameters to improve reasoning robustness and generalization. RECKONING and ThoughtSource exemplify this approach. RECKONING employs a bi-level learning algorithm to encode contextual knowledge directly into model parameters, thereby enhancing robustness against distractors and generalization to longer reasoning chains (Chen et al., 2023). ThoughtSource, a meta-dataset and software library, enhances chain-of-thought reasoning by providing diverse training data and facilitating empirical evaluations (Ott et al., 2023). While ThoughtSource focuses on CoT prompting, REVTHINK extends this by incorporating reverse thinking to further enhance reasoning performance and generalization.

The distillation of reasoning abilities from larger to smaller models is another area of interest, as seen in the works of Li et al. (2023) and Adarsh et al. (2024). Li et al. explore the distillation of reasoning abilities by leveraging both positive and negative samples, highlighting the value of incorrect reasoning chains. This approach aligns with REVTHINK's goal of enhancing reasoning in LLMs through innovative training methodologies. Similarly, SIKeD addresses the challenge of strategy bias in traditional distillation by iteratively teaching smaller models to use multiple strategies (Adarsh et al., 2024). Both methods emphasize the importance of leveraging large language models to transfer complex reasoning skills to smaller models, albeit through different mechanisms.

Self-improvement and self-reinforcement methodologies have also been explored to enhance reasoning abilities in LLMs. ReGenesis and the work by Tong et al. (2024) exemplify this approach. ReGenesis enhances reasoning abilities by self-synthesizing reasoning paths without additional supervision, addressing the limitations of existing methods like STaR in generalizing to out-of-domain tasks (Peng et al., 2024). Tong et al. introduce a self-reinforcement methodology to enhance reasoning abilities with minimal human supervision, aligning with REVTHINK's focus on structured forward-backward reasoning (Tong et al., 2024). Both methods aim to improve reasoning performance and generalization, highlighting the potential for integrating or comparing these methodologies with REVTHINK.

Finally, the importance of diverse reasoning capabilities is underscored by works such as REFINER and BRAINTEASER. REFINER enhances reasoning by generating intermediate steps and using a critic model to provide structured feedback, iteratively refining these steps (Paul et al., 2023). This approach is similar to REVTHINK in that both aim to improve reasoning capabilities through structured processes. BRAINTEASER, on the other hand, introduces a benchmark designed to evaluate lateral thinking in language models, contrasting with the vertical thinking focus of existing tasks (Jiang et al., 2023). While REVTHINK focuses on reverse thinking to enhance reasoning consistency, BRAINTEASER emphasizes creative problem-solving, highlighting the need for diverse reasoning capabilities in LLMs. Both approaches contribute to advancing the understanding and development of reasoning in AI models.","Yifan Jiang, Filip Ilievski, Kaixin Ma, Zhivar Sourati (2023). BRAINTEASER: Lateral Thinking Puzzles for Large Language Models. arXiv:2310.05057. https://arxiv.org/abs/2310.05057
Eric Zelikman, Georges Harik, Yijia Shao, Varuna Jayasiri, Nick Haber, Noah D. Goodman (2024). Quiet-STaR: Language Models Can Teach Themselves to Think Before Speaking. arXiv:2403.09629. https://arxiv.org/abs/2403.09629
Simon Ott, Konstantin Hebenstreit, Valentin Liévin, Christoffer Egeberg Hother, Milad Moradi, Maximilian Mayrhauser, Robert Praas, Ole Winther, Matthias Samwald (2023). ThoughtSource: A central hub for large language model reasoning data. arXiv:2301.11596. https://arxiv.org/abs/2301.11596
Yongqi Tong, Sizhe Wang, Dawei Li, Yifan Wang, Simeng Han, Zi Lin, Chengsong Huang, Jiaxin Huang, Jingbo Shang (2024). Optimizing Language Model's Reasoning Abilities with Weak Supervision. arXiv:2405.04086. https://arxiv.org/abs/2405.04086
Debjit Paul, Mete Ismayilzada, Maxime Peyrard, Beatriz Borges, Antoine Bosselut, Robert West, Boi Faltings (2023). REFINER: Reasoning Feedback on Intermediate Representations. arXiv:2304.01904. https://arxiv.org/abs/2304.01904
Shivam Adarsh, Kumar Shridhar, Caglar Gulcehre, Nicholas Monath, Mrinmaya Sachan (2024). SIKeD: Self-guided Iterative Knowledge Distillation for mathematical reasoning. arXiv:2410.18574. https://arxiv.org/abs/2410.18574
Yiwei Li, Peiwen Yuan, Shaoxiong Feng, Boyuan Pan, Bin Sun, Xinglin Wang, Heda Wang, Kan Li (2023). Turning Dust into Gold: Distilling Complex Reasoning Capabilities from LLMs by Leveraging Negative Data. arXiv:2312.12832. https://arxiv.org/abs/2312.12832
Xiangyu Peng, Congying Xia, Xinyi Yang, Caiming Xiong, Chien-Sheng Wu, Chen Xing (2024). ReGenesis: LLMs can Grow into Reasoning Generalists via Self-Improvement. arXiv:2410.02108. https://arxiv.org/abs/2410.02108
Zeming Chen, Gail Weiss, Eric Mitchell, Asli Celikyilmaz, Antoine Bosselut (2023). RECKONING: Reasoning through Dynamic Knowledge Encoding. arXiv:2305.06349. https://arxiv.org/abs/2305.06349"
30,7|7|7|7|5|5|5|7|4|7|7|3|8|7|6|7|5|8|7|7|3|5|7|7|5|7|7|7|7|3,10,7|7|4|7|7|7|7|6|5|7,source,source,"The exploration of commitment power in strategic communication has been a focal point in recent research, with various studies examining different mechanisms to enhance sender credibility and influence outcomes. A significant body of work has focused on Bayesian persuasion models, where the sender's ability to commit to a strategy is crucial. Zhang et al. (2023) introduce the concept of ex-post individually rational (ex-post IR) Bayesian persuasion, which ensures sender credibility by guaranteeing that the sender is never worse off by truthfully following the protocol. This approach aligns with our investigation of money-burning tactics as a means to enhance commitment power, as both studies explore the sender's optimal utility under different commitment constraints (Zhang et al., 2023). Similarly, Su and Subramanian (2022) highlight the impact of commitment order in Bayesian persuasion games, emphasizing how sequential commitments can alter equilibrium payoffs and strategies. This underscores the importance of commitment power in strategic communication settings, complementing our focus on robust Bayesian persuasion in Web 3.0 contexts (Su & Subramanian, 2022).

The strategic use of information in communication is another theme explored in related works. Arieli et al. (2023) present a two-stage Bayesian persuasion model where a third-party platform controls the information available to the sender, aiming to maximize user utility through optimal information disclosure. This model is relevant to our research as it addresses the enhancement of sender strategies in persuasion contexts, with an emphasis on platform-mediated information control and reputation management (Arieli et al., 2023). In contrast, Lin and Liu (2022) introduce a new notion of credibility in Bayesian persuasion, focusing on the sender's inability to profit from undetectable deviations in message distribution. While their work shows that state-independent payoffs lead to credible disclosure policies, our research demonstrates that money-burning can improve the sender's payoff in such scenarios, contributing to the understanding of strategic communication (Lin & Liu, 2022).

The dynamics of commitment and information transmission are further explored in studies examining competitive environments and communication constraints. Gradwohl et al. (2020) investigate a Bayesian persuasion model with multiple senders, where the receiver captures all informational surplus when senders are uncertain about each other's preferences. This contrasts with our focus on a single sender using money-burning tactics to enhance commitment power, thereby improving the sender's payoff (Gradwohl et al., 2020). Aybas and Turkel (2019) examine the impact of limited message capacity on an expert's persuasive ability, using a geometric approach to analyze the sender's payoff and the value of additional signals. Their research complements our work by exploring how communication constraints, such as message coarseness, affect the sender's ability to commit and persuade (Aybas & Turkel, 2019).

In dynamic settings, the complexity of designing signaling mechanisms is addressed by Iyer et al. (2023), who investigate a dynamic Bayesian persuasion framework where a long-lived sender influences short-lived agents with endogenous beliefs shaped by the history of a Markov process. This work complements our research by highlighting the importance of robustness in information design, similar to the commitment challenges in our model (Iyer et al., 2023). Che et al. (2020) examine a dynamic model of Bayesian persuasion where information generation and processing are costly, and neither the sender nor the receiver can commit to future actions. While their study explores the collapse of persuasion in a Markov perfect equilibrium, our research demonstrates how money-burning can improve the sender's payoff in scenarios where commitment is valuable (Che et al., 2020).

Finally, the computational aspects of persuasion are explored by Dughmi and Xu (2015), who examine the computational complexity of the Bayesian persuasion model, focusing on the sender's optimization task in various input models. This foundational understanding of sender-receiver dynamics is crucial for designing communication mechanisms that incorporate money-burning tactics for commitment power, particularly in contexts like Web 3.0 communities (Dughmi & Xu, 2015). Salcedo (2019) explores a cheap-talk model where the sender can use private messages to persuade only a subset of the audience, enhancing credibility by selectively speaking truthfully. This approach aligns with our research on communication mechanisms, as both studies investigate how strategic communication can optimize sender outcomes (Salcedo, 2019). Together, these works provide a comprehensive backdrop for our exploration of money-burning tactics in mediated communication, highlighting the diverse strategies and challenges in enhancing commitment power.","Bruno Salcedo (2019). Persuading part of an audience. arXiv:1903.00129. https://arxiv.org/abs/1903.00129
Shih-Tang Su, Vijay G. Subramanian (2022). Order of Commitments in Bayesian Persuasion with Partial-informed Senders. arXiv:2202.06479. https://arxiv.org/abs/2202.06479
Jiahao Zhang, Shuran Zheng, Renato Paes Leme, Zhiwei Steven Wu (2023). Ex-post Individually Rational Bayesian Persuasion. arXiv:2312.04973. https://arxiv.org/abs/2312.04973
Shaddin Dughmi, Haifeng Xu (2015). Algorithmic Bayesian Persuasion. arXiv:1503.05988. https://arxiv.org/abs/1503.05988
Krishnamurthy Iyer, Haifeng Xu, You Zu (2023). Markov Persuasion Processes with Endogenous Agent Beliefs. arXiv:2307.03181. https://arxiv.org/abs/2307.03181
Xiao Lin, Ce Liu (2022). Credible Persuasion. arXiv:2205.03495. https://arxiv.org/abs/2205.03495
Yeon-Koo Che, Kyungmin Kim, Konrad Mierendorff (2020). Keeping the Listener Engaged: a Dynamic Model of Bayesian Persuasion. arXiv:2003.07338. https://arxiv.org/abs/2003.07338
Itai Arieli, Omer Madmon, Moshe Tennenholtz (2023). Reputation-based Persuasion Platforms. arXiv:2305.16694. https://arxiv.org/abs/2305.16694
Ronen Gradwohl, Niklas Hahn, Martin Hoefer, Rann Smorodinsky (2020). Reaping the Informational Surplus in Bayesian Persuasion. arXiv:2006.02048. https://arxiv.org/abs/2006.02048"
30,3|6|7|3|7|4|2|5|8|8|5|7|7|3|3|2|6|6|7|8|2|3|3|7|3|5|3|2|2|3,10,6|8|3|5|2|8|7|5|2|2,target,source,"The study of Condorcet extensions and their susceptibility to paradoxes has been a significant area of research in social choice theory. A central theme in this domain is the exploration of the vulnerabilities and limitations of Condorcet-consistent voting rules, particularly in the context of variable-electorate paradoxes such as the reinforcement and no-show paradoxes. Dominik Peters (2017) investigates the manipulability of Condorcet-consistent voting rules through preference reversals, establishing that these rules can be manipulated when there are at least four alternatives. This complements our research by highlighting the broader challenges in designing robust Condorcet extensions, aligning with our findings on the limitations of these voting rules in the face of paradoxes (Peters, 2017).

The robustness of specific voting rules, such as Minimax, against paradoxical outcomes is another focal point in the literature. M. S. Krishnamoorthy and M. Raghavachari (2005) provide a comprehensive analysis of Condorcet voting schemes, emphasizing the unique characteristics of Minimax voting, including its immunity to spoilers and positive involvement. Their exploration of axioms such as homogeneity and block preservation supports the suitability of Minimax for three-candidate elections, reinforcing our findings on its robustness against variable-electorate paradoxes (Krishnamoorthy & Raghavachari, 2005). Similarly, Wesley H. Holliday and Eric Pacuit (2023) extend May's Theorem to three alternatives, highlighting the Minimax method's role in mitigating spoiler effects and avoiding the strong no-show paradox, which underscores the relevance of Minimax as a robust solution in our analysis (Holliday & Pacuit, 2023).

The intersection of Condorcet-consistency and participation criteria is another critical area of investigation. Wesley H. Holliday and Eric Pacuit (2020) explore the no-show paradox, leveraging SAT solving to establish optimal bounds for the number of voters required for these paradoxes to manifest. Their computational perspective complements our research by reinforcing the significance of maximin refinements in achieving immunity to such issues in smaller electorates (Holliday & Pacuit, 2020). Felix Brandt, Christian Geist, and Dominik Peters (2016) further examine the compatibility of Condorcet-consistent voting rules with the participation criterion, demonstrating the challenges of maintaining desirable voting properties in larger electorates, which parallels our investigation into the susceptibility of Condorcet extensions to paradoxes in three-candidate elections (Brandt, Geist, & Peters, 2016).

The complexities and paradoxes in voting systems, such as the no-show paradox and spoiler effects, are also addressed in the work of Wesley H. Holliday et al. (2022), who highlight the tension between expansion consistency and resoluteness in voting methods. Their use of SAT solving and formal verification to prove impossibility theorems complements our investigation into the axiomatic characterizations of maximin and its refinements, offering insights into the limitations and potential of different voting rules (Holliday et al., 2022). Additionally, Ross Hyman et al. (2023) explore the strategic behavior of candidates in elections, focusing on the existence of Nash equilibria in candidacy games under various voting rules, including Condorcet-consistent rules. This underscores the complexities and paradoxes associated with Condorcet extensions, reinforcing the conditions under which certain voting rules, like maximin, can avoid these issues in three-candidate elections (Hyman et al., 2023).

Finally, the broader context of voting rule paradoxes is explored in the works of Luis Sánchez-Fernández and Jesús A. Fisteus (2017) and Piotr Skowron et al. (2016). Sánchez-Fernández and Fisteus examine monotonicity axioms in approval-based multi-winner voting rules, addressing challenges and paradoxes in voting systems, albeit in different contexts. Their insights on axiomatic incompatibilities could inform the understanding of similar issues in Condorcet extensions (Sánchez-Fernández & Fisteus, 2017). Skowron et al. provide an axiomatic framework for understanding how committee scoring rules can be strategy-proof and non-dictatorial, highlighting the challenges of ensuring strategy-proofness in voting rules, a theme central to both our research on Condorcet extensions and their examination of committee selection rules (Skowron et al., 2016). These studies collectively underscore the importance of axiomatic characterizations in addressing strategic manipulation and paradoxical outcomes in both single-winner and multiwinner settings.","M. S. Krishnamoorthy, M. Raghavachari (2005). Condorcet Winner Probabilities - A Statistical Perspective. arXiv:math/0511140. https://arxiv.org/abs/math/0511140
Wesley H. Holliday, Chase Norman, Eric Pacuit, Saam Zahedian (2022). Impossibility theorems involving weakenings of expansion consistency and resoluteness in voting. arXiv:2208.06907. https://arxiv.org/abs/2208.06907
Piotr Skowron, Piotr Faliszewski, Arkadii Slinko (2016). Axiomatic Characterization of Committee Scoring Rules. arXiv:1604.01529. https://arxiv.org/abs/1604.01529
Felix Brandt, Christian Geist, Dominik Peters (2016). Optimal Bounds for the No-Show Paradox via SAT Solving. arXiv:1602.08063. https://arxiv.org/abs/1602.08063
Ross Hyman, Deb Otis, Seamus Allen, Greg Dennis (2023). A Majority Rule Philosophy for Instant Runoff Voting. arXiv:2308.08430. https://arxiv.org/abs/2308.08430
Wesley H. Holliday, Eric Pacuit (2023). An extension of May's Theorem to three alternatives: axiomatizing Minimax voting. arXiv:2312.14256. https://arxiv.org/abs/2312.14256
Wesley H. Holliday, Eric Pacuit (2020). Axioms for Defeat in Democratic Elections. arXiv:2008.08451. https://arxiv.org/abs/2008.08451
Luis Sánchez-Fernández, Jesús A. Fisteus (2017). Monotonicity axioms in approval-based multi-winner voting rules. arXiv:1710.04246. https://arxiv.org/abs/1710.04246
Dominik Peters (2017). Condorcet's Principle and the Preference Reversal Paradox. arXiv:1707.08760. https://arxiv.org/abs/1707.08760
Dominik Peters (2021). Proportionality and Strategyproofness in Multiwinner Elections. arXiv:2104.08594. https://arxiv.org/abs/2104.08594"
30,7|7|7|8|7|7|8|7|8|7|3|6|5|8|3|7|6|7|7|5|5|7|6|7|5|7|8|5|5|7,10,7|8|8|7|7|7|6|7|8|7,target,source,"In recent years, the study of chaotic dynamics in coupled map lattices (CMLs) has garnered significant attention, particularly in understanding the interplay between microscopic chaos and macroscopic transport phenomena. The present research on a lattice of coupled cat maps, which examines the ballistic spread of perturbations and the resulting diffusive transport in phase space, aligns with several key studies in this domain. Notably, the work by Moyano et al. (2006) explores a system of globally coupled standard maps, focusing on the scaling behavior of the largest Lyapunov exponent and the presence of metastable states. This study provides valuable insights into the dynamics of coupled chaotic systems, particularly in understanding how long-range interactions can lead to weak chaos and influence the ergodic properties of the system, complementing our investigation into diffusion and chaos-induced fluctuations in coupled cat maps.

The role of Lyapunov exponents in characterizing chaotic dynamics and transport properties is a recurring theme in the literature. Hu and Rosenhaus (2022) delve into the chaotic dynamics of a one-dimensional lattice of coupled maps with diffusive coupling, emphasizing the significance of covariant Lyapunov vectors (CLVs) and varying diffusion strength. Their findings on the ergodic properties and diffusive transport in phase space resonate with our study, as both works underscore the importance of chaos in determining transport properties. Similarly, Torcini and Lepri (1996) investigate the propagation of localized perturbations in chaotic coupled map lattices with long-range couplings, highlighting the exponential spread of perturbations and the role of Lyapunov exponents. These studies collectively underscore the critical role of Lyapunov exponents in understanding the dynamics of perturbation spread in chaotic systems, albeit with different coupling mechanisms and propagation characteristics.

The exploration of strange non-chaotic attractors (SNAs) and their influence on chaotic dynamics is another area of interest. Muruganandam and Senthilvelan (2021) examine the dynamics of CMLs with quasiperiodically forced nonlinear maps, focusing on the role of SNAs in the spatial spread of out-of-time-ordered correlators (OTOCs). Their use of finite-time Lyapunov exponents and instantaneous speed to map various dynamical regimes complements our research on the ergodic properties and diffusive transport in phase space of a lattice of coupled cat maps. Barbish and Paul (2023) further investigate the dynamics of CMLs, emphasizing the impact of SNAs on the spread of perturbations. Both studies highlight the connection between local perturbations and global chaotic behavior, providing a broader context for analyzing chaotic transport phenomena.

The transition from weak to strong chaos and its impact on diffusion is another critical aspect explored in the literature. Ginelli et al. (2001) investigate the dynamics of coupled symplectic McMillan maps, revealing subdiffusive behavior in disordered Hamiltonian lattices. Their work parallels the chaotic diffusion observed in our study of coupled cat maps, offering a stochastic model perspective on how diffusion emerges from microscopic chaos. Similarly, Mulansky et al. (2011) explore chaos in one-dimensional nonlinear Hamiltonian lattices with weakly coupled oscillators, revealing that the measure of chaos is proportional to the coupling strength and lattice length. These studies provide insights into the interplay between chaos and diffusion, complementing our findings on the ballistic spread of perturbations and diffusive transport.

Finally, the influence of noise and specific coupling configurations on chaotic dynamics and diffusion is addressed in several studies. Baroni et al. (2000) explore the dynamics of spatially extended systems under the influence of noise, leading to phenomena like stochastic synchronization and phase transitions. Their insights into the role of Lyapunov exponents and noise in suppressing information propagation complement our research on the ergodic properties and diffusive transport in coupled cat maps. Moges et al. (2021) investigate the diffusion and chaos properties of single and coupled standard maps, focusing on anomalous diffusion induced by accelerator modes. Their exploration of how specific configurations of coupling can suppress anomalous transport and lead to normal diffusion aligns with our study, highlighting the connection between microscopic chaos and macroscopic diffusion behavior. Collectively, these studies provide a comprehensive framework for understanding the complex dynamics of coupled chaotic systems, offering valuable perspectives that enhance our understanding of diffusion and transport phenomena in phase space driven by microscopic chaos.","Lucia Baroni, Roberto Livi, Alessandro Torcini (2000). Transition to Stochastic Synchronization in Spatially Extended Systems. arXiv:nlin/0003010. https://arxiv.org/abs/nlin/0003010
F. Ginelli, R. Livi, A. Politi (2001). Emergence of chaotic behaviour in linearly stable systems. arXiv:nlin/0102005. https://arxiv.org/abs/nlin/0102005
Mario Mulansky, Karsten Ahnert, Arkady Pikovsky, Dima Shepelyansky (2011). Strong and weak chaos in weakly nonintegrable many-body Hamiltonian systems. arXiv:1103.2634. https://arxiv.org/abs/1103.2634
Xu-Yao Hu, Vladimir Rosenhaus (2022). Correlation functions in linear chaotic maps. arXiv:2204.13655. https://arxiv.org/abs/2204.13655
Alessandro Torcini, Stefano Lepri (1996). Disturbance propagation in chaotic extended systems with long-range coupling. arXiv:chao-dyn/9609003. https://arxiv.org/abs/chao-dyn/9609003
Johnathon Barbish, Mark Paul (2023). Using Covariant Lyapunov Vectors to Quantify High Dimensional Chaos with a Conservation Law. arXiv:2303.13977. https://arxiv.org/abs/2303.13977
P. Muruganandam, M. Senthilvelan (2021). Manifestation of strange nonchaotic attractors in extended systems: A study through out-of-time-ordered correlators. arXiv:2109.07412. https://arxiv.org/abs/2109.07412"
30,7|8|8|8|8|8|8|8|8|8|8|7|8|8|8|8|8|7|8|7|7|8|8|8|8|7|8|7|8|7,10,7|8|8|8|7|7|7|8|7|8,tie,source,"In recent years, the challenge of accurately modeling chaotic dynamical systems has led to a surge in research exploring the integration of machine learning and data assimilation techniques. A common theme across several studies is the focus on improving model predictions without relying on detailed noise statistics, which aligns closely with the objectives of our research. For instance, Ribera et al. (2021) and Carlson et al. (2021) both address the issue of model selection and parameter estimation in chaotic systems, specifically the Lorenz system, using innovative approaches that do not depend on complete state variable measurements. Ribera et al. employ variational annealing combined with sparse optimization to identify governing equations from incomplete data, while Carlson et al. utilize a feedback control-based method to dynamically learn parameters from partial observations. These studies complement our work by highlighting alternative strategies for handling hidden variables and parameter estimation in chaotic systems, underscoring the broader goal of enhancing model accuracy and understanding in complex dynamical systems.

Another significant area of research involves the use of hybrid modeling approaches that combine machine learning with data assimilation to address unknown dynamics in physical systems. Pawar et al. (2021) and Brajard et al. (2020) exemplify this trend by integrating neural networks with data assimilation techniques to improve prediction accuracy in chaotic systems. Pawar et al. employ a recurrent neural network, specifically an LSTM, to model hidden physics and correct predictions, while Brajard et al. use an ensemble Kalman filter alongside a neural network to create a surrogate model for the Lorenz 96 model. Both studies demonstrate the potential of hybrid models to enhance forecast accuracy without relying on known noise statistics, paralleling our approach of using topological data analysis to optimize model predictions.

The exploration of data-driven methods for reconstructing and forecasting chaotic systems is further advanced by Özalp et al. (2023) and Young and Graham (2022), who focus on inferring hidden variables and stability properties from partial observations. Özalp et al. utilize LSTM networks to handle partial data, while Young and Graham emphasize the use of deep artificial neural networks to learn delay coordinate mappings. These approaches align with our research on data assimilation for dynamical systems, as both works address the challenges of modeling chaotic dynamics with limited data. Our novel algorithm, which leverages topological data analysis, offers a complementary perspective by optimizing model predictions without relying on noise statistics.

The integration of machine learning and data assimilation is also explored by Gottwald and Reich (2020) and Frerix et al. (2021), who introduce innovative methods to enhance forecasting capabilities in chaotic systems. Gottwald and Reich propose RAFDA, a method combining random feature maps with ensemble Kalman filter data assimilation, while Frerix et al. focus on variational data assimilation with a learned inverse observation operator. Both studies aim to improve forecast skill and probabilistic forecasting without detailed noise statistics, aligning with our work's focus on optimizing model parameters through incoming observations. These methodologies highlight the potential of combining machine learning techniques with data assimilation to address challenges in forecasting with noisy data and unknown system dynamics.

Finally, the emphasis on learning invariant measures and dynamics in chaotic systems is exemplified by Schiff et al. (2024) and Chakraborty et al. (2024). Schiff et al. introduce the DySLIM framework, which focuses on learning invariant measures to improve long-term statistical accuracy and point-wise tracking, while Chakraborty et al. propose a novel training approach for Neural Ordinary Differential Equations to effectively learn chaotic dynamical systems. Both studies emphasize the importance of enhancing model stability and accuracy, offering complementary strategies to our approach of minimizing topological differences between measurements and predictions. These works collectively underscore the diverse methodologies being developed to tackle the complexities of chaotic systems, contributing to the ongoing advancement of data-driven modeling in dynamical systems.","Suraj Pawar, Omer San, Adil Rasheed, Ionel M. Navon (2021). A nonintrusive hybrid neural-physics modeling of incomplete dynamical systems: Lorenz equations. arXiv:2104.00114. https://arxiv.org/abs/2104.00114
Yair Schiff, Zhong Yi Wan, Jeffrey B. Parker, Stephan Hoyer, Volodymyr Kuleshov, Fei Sha, Leonardo Zepeda-Núñez (2024). DySLIM: Dynamics Stable Learning by Invariant Measure for Chaotic Systems. arXiv:2402.04467. https://arxiv.org/abs/2402.04467
Thomas Frerix, Dmitrii Kochkov, Jamie A. Smith, Daniel Cremers, Michael P. Brenner, Stephan Hoyer (2021). Variational Data Assimilation with a Learned Inverse Observation Operator. arXiv:2102.11192. https://arxiv.org/abs/2102.11192
Dibyajyoti Chakraborty, Seung Whan Chung, Troy Arcomano, Romit Maulik (2024). Divide And Conquer: Learning Chaotic Dynamical Systems With Multistep Penalty Neural Ordinary Differential Equations. arXiv:2407.00568. https://arxiv.org/abs/2407.00568
H. Ribera, S. Shirman, A. V. Nguyen, N. M. Mangan (2021). Model selection of chaotic systems from data with hidden variables using sparse data assimilation. arXiv:2105.10068. https://arxiv.org/abs/2105.10068
Elise Özalp, Georgios Margazoglou, Luca Magri (2023). Reconstruction, forecasting, and stability of chaotic dynamics from partial data. arXiv:2305.15111. https://arxiv.org/abs/2305.15111
Elizabeth Carlson, Joshua Hudson, Adam Larios, Vincent R. Martinez, Eunice Ng, Jared P. Whitehead (2021). Dynamically learning the parameters of a chaotic system using partial observations. arXiv:2108.08354. https://arxiv.org/abs/2108.08354
Julien Brajard, Alberto Carassi, Marc Bocquet, Laurent Bertino (2020). Combining data assimilation and machine learning to emulate a dynamical model from sparse and noisy observations: a case study with the Lorenz 96 model. arXiv:2001.01520. https://arxiv.org/abs/2001.01520"
30,5|7|4|3|7|3|7|6|6|2|3|3|2|7|6|6|6|7|3|2|2|6|5|7|3|7|7|3|2|3,10,2|3|3|2|7|7|3|2|7|4,source,source,"In recent years, the Age of Information (AoI) has emerged as a critical metric for evaluating the freshness of information in various systems, particularly those involving sequential processing steps. The work by Soysal and Ulukus provides a foundational analysis of AoI in G/G/1/1 systems under different service disciplines, offering insights into age expressions and bounds that are pertinent to my research on series server setups. Their exploration of preemption and blocking models parallels my investigation into wasted power due to preemptions in series setups, and their findings on optimal interarrival and service times can inform my optimization problem for determining optimal service rates under power constraints (Han Dong et al., 2021).

The trade-off between computation and communication in power-efficient systems is another area of significant interest, as explored by Ismail Akturk and Ulya R. Karpuzcu. Their work models a tandem computation-transmission queue with power constraints, akin to the series server setup in my research, where updates require multiple processing steps. Both studies address the challenge of minimizing AoI while adhering to power budgets, underscoring the importance of efficient resource management in modern systems (Ismail Akturk and Ulya R. Karpuzcu, 2017). Similarly, the investigation by Parisa Rafiee et al. into AoI in edge computing systems highlights the age-delay tradeoff, which aligns with my exploration of the age-power tradeoff in parallel and series server setups. Both works focus on optimizing system performance under constraints, with an emphasis on power efficiency (Parisa Rafiee et al., 2020).

Energy efficiency and power management are recurring themes in the literature, as evidenced by the work of Ege Orkun Gamgam et al. and Balaji Subramaniam and Wu-chun Feng. Gamgam et al. focus on energy proportionality and power management in scale-out workloads, providing insights into power-saving techniques that could enhance energy efficiency in systems with multiple processing steps (Ege Orkun Gamgam et al., 2023). Subramaniam and Feng's exploration of power provisioning techniques and their impact on latency and energy efficiency offers valuable insights for my optimization problem, which seeks to determine optimal service rates under a power budget (Balaji Subramaniam and Wu-chun Feng, 2015). Both studies highlight the importance of managing power consumption to achieve optimal system performance.

The concept of leveraging latency slack to improve system performance and resource management is explored in the work of Yibei Ling and Jie Mi, as well as Balajee Vamanan et al. Ling and Mi introduce TimeTrader, a method to reduce energy consumption in datacenters by exploiting latency slack, which is relevant to my research on the age-power trade-off (Yibei Ling and Jie Mi, 2010). Vamanan et al. also focus on optimizing energy consumption in datacenters by exploiting latency slack in sub-critical replies, addressing the challenge of maintaining performance while reducing resource usage (Balajee Vamanan et al., 2015). Both studies provide insights into strategies for managing processing steps in systems with power constraints.

Finally, the exploration of energy minimization in data-intensive applications by Melih Bastopcu and Sennur Ulukus, as well as Ajay Badita et al., offers a broader context for my research. Bastopcu and Ulukus model applications as (n, k) fork-join systems, exploring the trade-off between energy consumption and response time, which is relevant to my focus on optimizing service rates to manage AoI (Melih Bastopcu and Sennur Ulukus, 2019). Badita et al. investigate probabilistic speed selection for server cores to balance performance and energy consumption, highlighting the importance of efficient power usage in maintaining system performance (Ajay Badita et al., 2021). Both studies emphasize the significance of optimizing service rates under power constraints to enhance system efficiency.","Han Dong, Sanjay Arora, Yara Awad, Tommy Unger, Orran Krieger, Jonathan Appavoo (2021). Slowing Down for Performance and Energy: An OS-Centric Study in Network Driven Workloads. arXiv:2112.07010. https://arxiv.org/abs/2112.07010
Melih Bastopcu, Sennur Ulukus (2019). Age of Information for Updates with Distortion. arXiv:1904.10444. https://arxiv.org/abs/1904.10444
Ajay Badita, Rooji Jinan, Balajee Vamanan, Parimal Parag (2021). Modeling Performance and Energy trade-offs in Online Data-Intensive Applications. arXiv:2108.08199. https://arxiv.org/abs/2108.08199
Balajee Vamanan, Hamza Bin Sohail, Jahangir Hasan, T. N. Vijaykumar (2015). TimeTrader: Exploiting Latency Tail to Save Datacenter Energy for On-line Data-Intensive Applications. arXiv:1503.05338. https://arxiv.org/abs/1503.05338
Balaji Subramaniam, Wu-chun Feng (2015). On the Energy Proportionality of Scale-Out Workloads. arXiv:1501.02729. https://arxiv.org/abs/1501.02729
Ege Orkun Gamgam, Nail Akar, Sennur Ulukus (2023). Minimizing Age of Information with Generate at Will Status Updates and Age-Agnostic Cyclic Scheduling. arXiv:2311.18791. https://arxiv.org/abs/2311.18791
Yibei Ling, Jie Mi (2010). An Optimal Trade-off between Content Freshness and Refresh Cost. arXiv:1008.0441. https://arxiv.org/abs/1008.0441
Ismail Akturk, Ulya R. Karpuzcu (2017). Trading Computation for Communication: A Taxonomy. arXiv:1709.06555. https://arxiv.org/abs/1709.06555
Parisa Rafiee, Peng Zou, Omur Ozel, Suresh Subramaniam (2020). Maintaining Information Freshness in Power-Efficient Status Update Systems. arXiv:2003.13577. https://arxiv.org/abs/2003.13577"
30,8|7|8|7|8|8|7|9|8|7|8|6|8|8|8|7|7|8|8|8|7|8|8|8|8|8|8|8|6|6,10,8|7|9|8|8|8|6|8|8|7,target,source,"The study of rank-metric codes has been enriched by various approaches that emphasize both algebraic and geometric perspectives, which are crucial for understanding and distinguishing these codes. A significant contribution to this field is the geometric approach to rank-metric codes, which introduces a simplified definition for generalized rank weights and provides a comprehensive classification of constant rank weight codes. This aligns with our research focus on geometric invariants, as both works underscore the importance of geometric perspectives in differentiating rank-metric codes, particularly Gabidulin codes from random ones (Randrianarisoa, 2019). Similarly, the exploration of minimal codes in the sum-rank metric highlights the role of geometric perspectives and invariants, paralleling our work on geometric invariants for linear rank-metric codes (Borello & Zullo, 2023).

The algebraic structures within rank-metric codes have been further explored through the extension of symmetric rank-metric codes to higher-degree polynomials, introducing essential-rank-metric codes as a generalization of symmetric Gabidulin codes. This work is relevant to our research as it provides a framework for comparing these codes to known families, similar to our focus on geometric invariants (Bik & Neri, 2023). Additionally, the introduction of tensor rank as a coding theoretic parameter, and the definition of minimal tensor rank (MTR) codes, complements our investigation into the geometric properties of rank-metric codes by providing a framework that aligns with our focus on using invariants to differentiate Gabidulin codes from random ones (Byrne et al., 2019).

Invariants play a crucial role in distinguishing between different families of rank-metric codes. The exploration of sequences of dimensions generated by field automorphisms as criteria for code equivalence provides a framework closely related to our research. This approach complements our introduction of a novel geometric invariant inspired by the Schur product, offering an alternative method to differentiate Gabidulin codes from random ones (Neri et al., 2019). Similarly, the study of generalized weights as algebraic invariants for codes, focusing on anticodes in both Hamming and rank metrics, provides a framework for distinguishing rank-metric codes through algebraic invariants, akin to our approach using geometric invariants (Ravagnani, 2014).

The foundational understanding of rank-metric codes is further supported by comprehensive introductions to their mathematical theory, covering essential topics such as the definition of rank metric, code equivalence, and generalized weights. This foundational work aligns with our research focus on constructing new families of rank-metric codes and distinguishing them using novel invariants (Gorla, 2019). The exploration of generalized Subspace Subcodes in the rank-metric, which provides an algorithm for constructing generator and parity-check matrices, also aligns with our focus on distinguishing rank-metric codes using geometric invariants. This work underscores the importance of developing new code families to address structural vulnerabilities in existing rank-metric codes, particularly in the context of post-quantum cryptography applications (Ndiaye et al., 2023).

Finally, the characterization of the generator matrix in standard form for generalized Gabidulin codes, introducing the concept of q-Cauchy matrices, offers a new criterion for identifying these codes, which is crucial for distinguishing them from random ones. This exploration of structured matrices aligns with our focus on novel algebraic structures in rank-metric codes (Neri, 2018). Additionally, the geometric characterization of sum-rank metric codes extends the understanding of code structures through geometric invariants, paralleling our exploration of distinguishing Gabidulin codes using geometric perspectives (Neri et al., 2021). Together, these works contribute to the broader goal of identifying and characterizing distinct algebraic structures within rank-metric codes, enhancing the understanding of their structural properties.","Arthur Bik, Alessandro Neri (2023). Higher-degree symmetric rank-metric codes. arXiv:2303.06745. https://arxiv.org/abs/2303.06745
Eimear Byrne, Alessandro Neri, Alberto Ravagnani, John Sheekey (2019). Tensor Representation of Rank-Metric Codes. arXiv:1904.05227. https://arxiv.org/abs/1904.05227
Martino Borello, Ferdinando Zullo (2023). Geometric dual and sum-rank minimal codes. arXiv:2303.07288. https://arxiv.org/abs/2303.07288
Alessandro Neri, Sven Puchinger, Anna-Lena Horlemann-Trautmann (2019). Equivalence and Characterizations of Linear Rank-Metric Codes Based on Invariants. arXiv:1911.13059. https://arxiv.org/abs/1911.13059
Alessandro Neri, Paolo Santonastaso, Ferdinando Zullo (2021). The geometry of one-weight codes in the sum-rank metric. arXiv:2112.04989. https://arxiv.org/abs/2112.04989
Alessandro Neri (2018). Systematic encoders for generalized Gabidulin codes and the $q$-analogue of Cauchy matrices. arXiv:1805.06706. https://arxiv.org/abs/1805.06706
Ousmane Ndiaye, Peter Arnaud Kidoudou, Hervé Tale Kalachi (2023). Generalized Subspace Subcodes in the Rank Metric. arXiv:2301.12523. https://arxiv.org/abs/2301.12523"
