from bertopic import BERTopic
from sentence_transformers import SentenceTransformer
from pymilvus import MilvusClient
from citegeist.utils.helpers import load_api_key
from citegeist.utils.prompts import (
    generate_summary_prompt,
    generate_related_work_prompt
)
from citegeist.utils.azure_client import AzureClient
from citegeist.utils.citations import get_arxiv_abstract, get_arxiv_citation
import time
import random

AZURE_ENDPOINT = "cai-project"
AZURE_PROMPTING_MODEL = "gpt-4o"
AZURE_PROMPTING_MODEL_VERSION = "2024-08-01-preview"
KEY_LOCATION = "carl_config.json"

topic_model = BERTopic.load("MaartenGr/BERTopic_ArXiv")
embedding_model = SentenceTransformer("sentence-transformers/all-mpnet-base-v2")
client = MilvusClient("./database.db")
prompting_client = AzureClient(
    endpoint=AZURE_ENDPOINT,
    deployment_id=AZURE_PROMPTING_MODEL,
    api_key=load_api_key(KEY_LOCATION),
)

import google.generativeai as genai
import json


def load_gemini_key():
    with open("Gemini_Key.json", "r") as file:
        data = json.load(file)
        return data["secret_key"]


genai.configure(api_key=load_gemini_key())


def prompt_gemini_2(
    model_name: str, prompt: str, temperature: float = 0.0, max_tokens: int = 4096
):
    """Prompt the Gemini model via Google Vertex AI."""
    model = genai.GenerativeModel(model_name)

    generation_config = {
        "temperature": temperature,
    }

    response = model.generate_content(prompt, generation_config=generation_config)

    return response.text


def exponential_backoff_retry(func, retries=10, backoff_factor=2, max_wait=120):
    """Retry with exponential backoff in case of rate limit error (429)."""
    wait = 1
    for attempt in range(retries):
        try:
            return func()
        except Exception as e:
            if "429" in str(e) or "529" in str(e):
                print(
                    f"Rate limit exceeded. Attempt {attempt + 1} of {retries}. Retrying in {wait} seconds."
                )
                time.sleep(wait + random.uniform(0, 1))  # Adding jitter
                wait = min(wait * backoff_factor, max_wait)
            else:
                raise e
    print("Exceeded maximum retries due to rate limit.")
    raise Exception("Exceeded maximum retries due to rate limit.")


def prompt_gemini_with_backoff(model_name: str, prompt: str):
    """Wrap Gemini prompt with exponential backoff in case of rate limit."""

    def call_gemini():
        return prompt_gemini_2(model_name, prompt)

    return exponential_backoff_retry(call_gemini)


abstract = "Large Language Models have shown impressive per- formance across a wide array of tasks involving both structured and unstructured textual data. More recently, adaptions of these models have drawn attention to their abilities to work with code across different programming languages. On this notion, different benchmarks for code generation, repair, or completion suggest that certain models have programming abilities comparable to or even surpass humans. In this work, we demonstrate that the performance on this benchmark does not translate to the innate ability of humans to appreciate the structural control flow of code. For this purpose, we extract code solutions from the Hu- manEval benchmark, which the relevant models perform very strongly on, and trace their execution path using function calls sampled from the respective test set. Using this dataset, we investigate the ability of 5 state-of-the-art LLMs to match the execution trace and find that, despite the modelâ€™s abilities to generate semantically identical code, they possess only limited ability to trace the execution path, especially for traces with increased length. We find that even the top-performing model, Gemini 1.5 Pro can only fully correctly generate the trace of 47% of HumanEval tasks. In addition, we introduce a specific subset for three key structures not, or only contained to a limited extent in Hu- manEval: Recursion, Parallel Processing, and Object Oriented Programming principles, including concepts like Inheritance and Polymorphism. Besides OOP, we show that none of the investigated models achieve an average accuracy of over 5% on the relevant traces. Aggregating these specialized parts with the ubiquitous HumanEval tasks, we present the Benchmark CoCoNUT: Code Control Flow for Navigation Understanding and Testing, which measures a models ability to trace the execu- tion of code upon relevant calls, including advanced structural components. We conclude that the current generation LLMs still need to significantly improve to enhance their code reasoning abilities. We hope our dataset can help researchers bridge this gap in the near future."
embedded_abstract = embedding_model.encode(abstract)
topic = topic_model.transform(abstract)
topic_id = topic[0][0]

res = client.search(
    collection_name="abstracts",
    data=[embedded_abstract],
    limit=10,
    # filter = f'topic == {topic_id}',
    search_params={"metric_type": "COSINE", "params": {}},
    # output_fields = []
)
formatted_res = json.dumps(res, indent=4)
print(formatted_res)

for obj in res:
    arxiv_id = obj["id"]
    arxiv_abstract = get_arxiv_abstract(arxiv_id)
    response: str = prompting_client.get_completions(
        generate_summary_prompt(abstract, arxiv_abstract), AZURE_PROMPTING_MODEL_VERSION
    )
    obj["summary"] = response
    obj["citation"] = get_arxiv_citation(arxiv_id)
res = res[0]

for obj in res:
    arxiv_id = obj["id"]
    arxiv_abstract = get_arxiv_abstract(arxiv_id)
    response: str = prompting_client.get_completions(
        generate_summary_prompt(abstract, arxiv_abstract), AZURE_PROMPTING_MODEL_VERSION
    )
    obj["summary"] = response
    obj["citation"] = get_arxiv_citation(arxiv_id)

response: str = prompting_client.get_completions(
    generate_related_work_prompt(abstract, res), AZURE_PROMPTING_MODEL_VERSION
)
print(response)
