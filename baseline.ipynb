{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-11-28T01:50:01.503905Z",
     "start_time": "2024-11-28T01:49:52.601203Z"
    }
   },
   "source": [
    "from bertopic import BERTopic\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from pymilvus import MilvusClient\n",
    "import json\n",
    "from utils.helpers import load_api_key, generate_summary_prompt, generate_related_work_prompt\n",
    "from utils.azure_client import AzureClient\n",
    "from utils.citations import get_arxiv_abstract, get_arxiv_citation\n",
    "\n",
    "AZURE_ENDPOINT = \"cai-project\"\n",
    "AZURE_PROMPTING_MODEL = \"gpt-4o\"\n",
    "AZURE_PROMPTING_MODEL_VERSION = \"2024-08-01-preview\"\n",
    "KEY_LOCATION = \"carl_config.json\"\n",
    "\n",
    "topic_model = BERTopic.load(\"MaartenGr/BERTopic_ArXiv\")\n",
    "embedding_model = SentenceTransformer('sentence-transformers/all-mpnet-base-v2')\n",
    "client = MilvusClient('./database.db')\n",
    "prompting_client = AzureClient(\n",
    "    endpoint=AZURE_ENDPOINT,\n",
    "    deployment_id=AZURE_PROMPTING_MODEL,\n",
    "    api_key=load_api_key(KEY_LOCATION)\n",
    ")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ruiyang Xu, Jialun Cao, Yaojie Lu, Hongyu Lin, Xianpei Han, Ben He, Shing-Chi Cheung, Le Sun (2024). CRUXEval-X: A Benchmark for Multilingual Code Reasoning, Understanding and Execution. arXiv:2408.13001. https://arxiv.org/abs/2408.13001\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-28T01:51:31.303221Z",
     "start_time": "2024-11-28T01:50:08.396994Z"
    }
   },
   "cell_type": "code",
   "source": [
    "abstract = 'Large Language Models have shown impressive per- formance across a wide array of tasks involving both structured and unstructured textual data. More recently, adaptions of these models have drawn attention to their abilities to work with code across different programming languages. On this notion, different benchmarks for code generation, repair, or completion suggest that certain models have programming abilities comparable to or even surpass humans. In this work, we demonstrate that the performance on this benchmark does not translate to the innate ability of humans to appreciate the structural control flow of code. For this purpose, we extract code solutions from the Hu- manEval benchmark, which the relevant models perform very strongly on, and trace their execution path using function calls sampled from the respective test set. Using this dataset, we investigate the ability of 5 state-of-the-art LLMs to match the execution trace and find that, despite the modelâ€™s abilities to generate semantically identical code, they possess only limited ability to trace the execution path, especially for traces with increased length. We find that even the top-performing model, Gemini 1.5 Pro can only fully correctly generate the trace of 47% of HumanEval tasks. In addition, we introduce a specific subset for three key structures not, or only contained to a limited extent in Hu- manEval: Recursion, Parallel Processing, and Object Oriented Programming principles, including concepts like Inheritance and Polymorphism. Besides OOP, we show that none of the investigated models achieve an average accuracy of over 5% on the relevant traces. Aggregating these specialized parts with the ubiquitous HumanEval tasks, we present the Benchmark CoCoNUT: Code Control Flow for Navigation Understanding and Testing, which measures a models ability to trace the execu- tion of code upon relevant calls, including advanced structural components. We conclude that the current generation LLMs still need to significantly improve to enhance their code reasoning abilities. We hope our dataset can help researchers bridge this gap in the near future.'\n",
    "embedded_abstract = embedding_model.encode(abstract)\n",
    "topic = topic_model.transform(abstract)\n",
    "topic_id = topic[0][0]\n",
    "\n",
    "res = client.search(\n",
    "    collection_name=\"abstracts\",\n",
    "    data = [embedded_abstract],\n",
    "    limit = 10,\n",
    "    # filter = f'topic == {topic_id}',\n",
    "    search_params = {\n",
    "        \"metric_type\": \"COSINE\",\n",
    "        \"params\": {}\n",
    "    },\n",
    "    # output_fields = []\n",
    ")\n",
    "formatted_res = json.dumps(res, indent=4)\n",
    "print(formatted_res)"
   ],
   "id": "12f31531c99f743a",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "2e0ec31d6c2f48f0a4d1479f61d0c47b"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-27 20:50:08,818 - BERTopic - Predicting topic assignments through cosine similarity of topic and document embeddings.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\n",
      "    [\n",
      "        {\n",
      "            \"id\": \"2408.10718\",\n",
      "            \"distance\": 0.8272675275802612,\n",
      "            \"entity\": {}\n",
      "        },\n",
      "        {\n",
      "            \"id\": \"2408.13001\",\n",
      "            \"distance\": 0.8230882883071899,\n",
      "            \"entity\": {}\n",
      "        },\n",
      "        {\n",
      "            \"id\": \"2410.21647\",\n",
      "            \"distance\": 0.812175989151001,\n",
      "            \"entity\": {}\n",
      "        },\n",
      "        {\n",
      "            \"id\": \"2309.15432\",\n",
      "            \"distance\": 0.8043964505195618,\n",
      "            \"entity\": {}\n",
      "        },\n",
      "        {\n",
      "            \"id\": \"2407.11470\",\n",
      "            \"distance\": 0.803741455078125,\n",
      "            \"entity\": {}\n",
      "        },\n",
      "        {\n",
      "            \"id\": \"2407.19055\",\n",
      "            \"distance\": 0.8031219840049744,\n",
      "            \"entity\": {}\n",
      "        },\n",
      "        {\n",
      "            \"id\": \"2311.08588\",\n",
      "            \"distance\": 0.8028398752212524,\n",
      "            \"entity\": {}\n",
      "        },\n",
      "        {\n",
      "            \"id\": \"2402.08699\",\n",
      "            \"distance\": 0.8003471493721008,\n",
      "            \"entity\": {}\n",
      "        },\n",
      "        {\n",
      "            \"id\": \"2406.15877\",\n",
      "            \"distance\": 0.8000816106796265,\n",
      "            \"entity\": {}\n",
      "        },\n",
      "        {\n",
      "            \"id\": \"2403.04811\",\n",
      "            \"distance\": 0.7978211641311646,\n",
      "            \"entity\": {}\n",
      "        }\n",
      "    ]\n",
      "]\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-28T01:52:32.030131Z",
     "start_time": "2024-11-28T01:52:32.017690Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# we need to remove the best match because that's the same input paper (this only has to be done for papers that are already in the arxiv corpus)\n",
    "# res = res[0][1:]\n",
    "\n",
    "res = res[0]"
   ],
   "id": "ada7380cc8fb5a6e",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-28T01:59:49.273452Z",
     "start_time": "2024-11-28T01:52:36.852144Z"
    }
   },
   "cell_type": "code",
   "source": [
    "for obj in res:\n",
    "    arxiv_id = obj['id']\n",
    "    arxiv_abstract = get_arxiv_abstract(arxiv_id)\n",
    "    response: str = prompting_client.get_completions(generate_summary_prompt(abstract, arxiv_abstract), AZURE_PROMPTING_MODEL_VERSION)\n",
    "    obj['summary'] = response\n",
    "    obj['citation'] = get_arxiv_citation(arxiv_id)"
   ],
   "id": "c2f3c9cbf90465c8",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Rate limit exceeded. Attempt 1 of 10. Retrying in 1 seconds.\n",
      "WARNING:root:Rate limit exceeded. Attempt 2 of 10. Retrying in 2 seconds.\n",
      "WARNING:root:Rate limit exceeded. Attempt 3 of 10. Retrying in 4 seconds.\n",
      "WARNING:root:Rate limit exceeded. Attempt 4 of 10. Retrying in 8 seconds.\n",
      "WARNING:root:Rate limit exceeded. Attempt 5 of 10. Retrying in 16 seconds.\n",
      "WARNING:root:Rate limit exceeded. Attempt 6 of 10. Retrying in 32 seconds.\n",
      "WARNING:root:Rate limit exceeded. Attempt 1 of 10. Retrying in 1 seconds.\n",
      "WARNING:root:Rate limit exceeded. Attempt 2 of 10. Retrying in 2 seconds.\n",
      "WARNING:root:Rate limit exceeded. Attempt 3 of 10. Retrying in 4 seconds.\n",
      "WARNING:root:Rate limit exceeded. Attempt 4 of 10. Retrying in 8 seconds.\n",
      "WARNING:root:Rate limit exceeded. Attempt 5 of 10. Retrying in 16 seconds.\n",
      "WARNING:root:Rate limit exceeded. Attempt 6 of 10. Retrying in 32 seconds.\n",
      "WARNING:root:Rate limit exceeded. Attempt 1 of 10. Retrying in 1 seconds.\n",
      "WARNING:root:Rate limit exceeded. Attempt 2 of 10. Retrying in 2 seconds.\n",
      "WARNING:root:Rate limit exceeded. Attempt 3 of 10. Retrying in 4 seconds.\n",
      "WARNING:root:Rate limit exceeded. Attempt 4 of 10. Retrying in 8 seconds.\n",
      "WARNING:root:Rate limit exceeded. Attempt 5 of 10. Retrying in 16 seconds.\n",
      "WARNING:root:Rate limit exceeded. Attempt 6 of 10. Retrying in 32 seconds.\n",
      "WARNING:root:Rate limit exceeded. Attempt 1 of 10. Retrying in 1 seconds.\n",
      "WARNING:root:Rate limit exceeded. Attempt 2 of 10. Retrying in 2 seconds.\n",
      "WARNING:root:Rate limit exceeded. Attempt 3 of 10. Retrying in 4 seconds.\n",
      "WARNING:root:Rate limit exceeded. Attempt 4 of 10. Retrying in 8 seconds.\n",
      "WARNING:root:Rate limit exceeded. Attempt 5 of 10. Retrying in 16 seconds.\n",
      "WARNING:root:Rate limit exceeded. Attempt 6 of 10. Retrying in 32 seconds.\n",
      "WARNING:root:Rate limit exceeded. Attempt 1 of 10. Retrying in 1 seconds.\n",
      "WARNING:root:Rate limit exceeded. Attempt 2 of 10. Retrying in 2 seconds.\n",
      "WARNING:root:Rate limit exceeded. Attempt 3 of 10. Retrying in 4 seconds.\n",
      "WARNING:root:Rate limit exceeded. Attempt 4 of 10. Retrying in 8 seconds.\n",
      "WARNING:root:Rate limit exceeded. Attempt 5 of 10. Retrying in 16 seconds.\n",
      "WARNING:root:Rate limit exceeded. Attempt 6 of 10. Retrying in 32 seconds.\n",
      "WARNING:root:Rate limit exceeded. Attempt 1 of 10. Retrying in 1 seconds.\n",
      "WARNING:root:Rate limit exceeded. Attempt 2 of 10. Retrying in 2 seconds.\n",
      "WARNING:root:Rate limit exceeded. Attempt 3 of 10. Retrying in 4 seconds.\n",
      "WARNING:root:Rate limit exceeded. Attempt 4 of 10. Retrying in 8 seconds.\n",
      "WARNING:root:Rate limit exceeded. Attempt 5 of 10. Retrying in 16 seconds.\n",
      "WARNING:root:Rate limit exceeded. Attempt 6 of 10. Retrying in 32 seconds.\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-28T02:01:29.807750Z",
     "start_time": "2024-11-28T02:01:29.781450Z"
    }
   },
   "cell_type": "code",
   "source": "generate_related_work_prompt(abstract, res)",
   "id": "8e913230b74b0b30",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n    I am working on a research paper, and I need a well-written \"Related Work\" section. Below, I provide:\\n    The abstract of my paper:\\n    \"Large Language Models have shown impressive per- formance across a wide array of tasks involving both structured and unstructured textual data. More recently, adaptions of these models have drawn attention to their abilities to work with code across different programming languages. On this notion, different benchmarks for code generation, repair, or completion suggest that certain models have programming abilities comparable to or even surpass humans. In this work, we demonstrate that the performance on this benchmark does not translate to the innate ability of humans to appreciate the structural control flow of code. For this purpose, we extract code solutions from the Hu- manEval benchmark, which the relevant models perform very strongly on, and trace their execution path using function calls sampled from the respective test set. Using this dataset, we investigate the ability of 5 state-of-the-art LLMs to match the execution trace and find that, despite the modelâ€™s abilities to generate semantically identical code, they possess only limited ability to trace the execution path, especially for traces with increased length. We find that even the top-performing model, Gemini 1.5 Pro can only fully correctly generate the trace of 47% of HumanEval tasks. In addition, we introduce a specific subset for three key structures not, or only contained to a limited extent in Hu- manEval: Recursion, Parallel Processing, and Object Oriented Programming principles, including concepts like Inheritance and Polymorphism. Besides OOP, we show that none of the investigated models achieve an average accuracy of over 5% on the relevant traces. Aggregating these specialized parts with the ubiquitous HumanEval tasks, we present the Benchmark CoCoNUT: Code Control Flow for Navigation Understanding and Testing, which measures a models ability to trace the execu- tion of code upon relevant calls, including advanced structural components. We conclude that the current generation LLMs still need to significantly improve to enhance their code reasoning abilities. We hope our dataset can help researchers bridge this gap in the near future.\"\\n    \\n        \\n        Paper 1:\\n        Summary: The cited paper introduces CodeJudge-Eval, a benchmark that evaluates large language models\\' code understanding abilities by assessing their capability to judge the correctness of code solutions, rather than just generating code. This approach complements my research, which highlights the limitations of current LLMs in tracing execution paths and understanding structural control flow in code. Both works underscore the need for improved benchmarks that go beyond code generation to assess deeper code reasoning and understanding abilities in LLMs.\\n        Citation: Yuwei Zhao, Ziyang Luo, Yuchen Tian, Hongzhan Lin, Weixiang Yan, Annan Li, Jing Ma (2024). CodeJudge-Eval: Can Large Language Models be Good Judges in Code Understanding?. arXiv:2408.10718. https://arxiv.org/abs/2408.10718\\n        \\n        \\n        Paper 2:\\n        Summary: The cited paper highlights the programming language and task biases in existing code benchmarks, such as HumanEval, which predominantly focus on Python and code generation capabilities. This aligns with our research, which also critiques the limitations of current benchmarks, specifically in evaluating code reasoning and structural control flow understanding. While the cited work introduces CRUXEVAL-X to address multi-lingual and reasoning biases, our research emphasizes the need for improved code reasoning abilities in LLMs, particularly in tracing execution paths and handling advanced structural components. Both studies underscore the necessity for more comprehensive benchmarks to enhance LLMs\\' coding capabilities.\\n        Citation: Ruiyang Xu, Jialun Cao, Yaojie Lu, Hongyu Lin, Xianpei Han, Ben He, Shing-Chi Cheung, Le Sun (2024). CRUXEval-X: A Benchmark for Multilingual Code Reasoning, Understanding and Execution. arXiv:2408.13001. https://arxiv.org/abs/2408.13001\\n        \\n        \\n        Paper 3:\\n        Summary: The cited paper introduces REPOCOD, a benchmark designed to evaluate the real-world code completion capabilities of large language models (LLMs), highlighting their limitations in achieving high accuracy in complex, context-rich coding tasks. This aligns with my research, which also identifies the shortcomings of LLMs in understanding and tracing code execution paths, particularly in advanced structural components. Both studies underscore the need for improved LLMs to enhance code reasoning and performance in realistic software development scenarios.\\n        Citation: Shanchao Liang, Yiran Hu, Nan Jiang, Lin Tan (2024). Can Language Models Replace Programmers? REPOCOD Says \\'Not Yet\\'. arXiv:2410.21647. https://arxiv.org/abs/2410.21647\\n        \\n        \\n        Paper 4:\\n        Summary: The cited paper highlights the importance of leveraging structured data inherent in programming languages, such as LLVM IR, to enhance the performance of machine learning models in code-related tasks. This aligns with my research, which underscores the limitations of current large language models in understanding code execution paths, particularly when advanced structural components are involved. By utilizing structured representations, as suggested in the cited work, there is potential to improve the code reasoning abilities of models, addressing the gaps identified in my study.\\n        Citation: Aiden Grossman, Ludger Paehler, Konstantinos Parasyris, Tal Ben-Nun, Jacob Hegna, William Moses, Jose M Monsalve Diaz, Mircea Trofin, Johannes Doerfert (2023). ComPile: A Large IR Dataset from Production Sources. arXiv:2309.15432. https://arxiv.org/abs/2309.15432\\n        \\n        \\n        Paper 5:\\n        Summary: The cited paper introduces the RACE benchmark, which evaluates LLM-generated code across multiple dimensions such as Readability, Maintainability, Correctness, and Efficiency, highlighting the limitations of correctness-centric benchmarks. This aligns with our research, which also critiques current benchmarks by demonstrating that LLMs struggle with tracing execution paths, especially in complex code structures. Both works emphasize the need for more comprehensive evaluations to improve LLMs\\' real-world coding capabilities.\\n        Citation: Jiasheng Zheng, Boxi Cao, Zhengzhao Ma, Ruotong Pan, Hongyu Lin, Yaojie Lu, Xianpei Han, Le Sun (2024). Beyond Correctness: Benchmarking Multi-dimensional Code Generation for Large Language Models. arXiv:2407.11470. https://arxiv.org/abs/2407.11470\\n        \\n        \\n        Paper 6:\\n        Summary: The cited paper highlights the limitations of Large Language Models (LLMs) in handling complex code generation tasks, particularly in debugging, which is crucial for iterative code refinement. It introduces a novel algorithm, BESTER, that enhances LLMs\\' debugging capabilities through self-reflection and search, achieving state-of-the-art results. This relates to my research by underscoring the gap in LLMs\\' ability to understand and trace code execution paths, as my work demonstrates their limited performance in appreciating structural control flow, especially in complex scenarios like recursion and object-oriented programming. Both studies emphasize the need for improved code reasoning abilities in LLMs.\\n        Citation: Jialin Song, Jonathan Raiman, Bryan Catanzaro (2024). Effective Large Language Model Debugging with Best-first Tree Search. arXiv:2407.19055. https://arxiv.org/abs/2407.19055\\n        \\n        \\n        Paper 7:\\n        Summary: The cited paper introduces CodeScope, a comprehensive benchmark designed to evaluate the code understanding and generation capabilities of Large Language Models (LLMs) across 43 programming languages and eight coding tasks, addressing limitations in existing benchmarks by focusing on multilingual and multitask environments. This relates to my research by highlighting the need for more robust evaluation metrics, as my work also identifies gaps in current benchmarks, particularly in assessing LLMs\\' abilities to trace execution paths and handle advanced code structures. Both studies emphasize the importance of improving LLMs\\' code reasoning abilities to better align with real-world software development needs.\\n        Citation: Weixiang Yan, Haitian Liu, Yunkun Wang, Yunzhe Li, Qian Chen, Wen Wang, Tingyu Lin, Weishan Zhao, Li Zhu, Hari Sundaram, Shuiguang Deng (2023). CodeScope: An Execution-based Multilingual Multitask Multidimensional Benchmark for Evaluating LLMs on Code Understanding and Generation. arXiv:2311.08588. https://arxiv.org/abs/2311.08588\\n        \\n        \\n        Paper 8:\\n        Summary: The cited paper introduces round-trip correctness (RTC) as an alternative evaluation method for code large language models (LLMs), allowing for assessment across a broader spectrum of real-world software domains without costly human curation. This approach contrasts with traditional benchmarks like HumanEval, which are limited in scope. In relation to my work, while RTC expands the evaluation to diverse domains, my research focuses on the models\\' ability to trace execution paths, revealing limitations in their code reasoning abilities, particularly in complex structures. Both works highlight the need for improved evaluation methods to better understand and enhance LLMs\\' programming capabilities.\\n        Citation: Miltiadis Allamanis, Sheena Panthaplackel, Pengcheng Yin (2024). Unsupervised Evaluation of Code LLMs with Round-Trip Correctness. arXiv:2402.08699. https://arxiv.org/abs/2402.08699\\n        \\n        \\n        Paper 9:\\n        Summary: The cited paper introduces BigCodeBench, a benchmark designed to evaluate the ability of Large Language Models (LLMs) to perform complex tasks by invoking multiple function calls across various libraries and domains, highlighting the current limitations of LLMs in compositional reasoning and precise function call usage. This aligns with my research, which also identifies the limitations of LLMs in understanding and tracing code execution paths, particularly in complex structures like recursion and object-oriented programming. Both studies underscore the need for advancements in LLMs\\' code reasoning capabilities to better mimic human-like understanding and execution of programming tasks.\\n        Citation: Terry Yue Zhuo, Minh Chien Vu, Jenny Chim, Han Hu, Wenhao Yu, Ratnadira Widyasari, Imam Nur Bani Yusuf, Haolan Zhan, Junda He, Indraneil Paul, Simon Brunner, Chen Gong, Thong Hoang, Armel Randy Zebaze, Xiaoheng Hong, Wen-Ding Li, Jean Kaddour, Ming Xu, Zhihan Zhang, Prateek Yadav, Naman Jain, Alex Gu, Zhoujun Cheng, Jiawei Liu, Qian Liu, Zijian Wang, David Lo, Binyuan Hui, Niklas Muennighoff, Daniel Fried, Xiaoning Du, Harm de Vries, Leandro Von Werra (2024). BigCodeBench: Benchmarking Code Generation with Diverse Function Calls and Complex Instructions. arXiv:2406.15877. https://arxiv.org/abs/2406.15877\\n        \\n        \\n        Paper 10:\\n        Summary: The cited paper investigates data contamination in code generation benchmarks, highlighting the overlap between these benchmarks and pretraining data, which can lead to inflated model performance. This is relevant to our work as it underscores the need for robust evaluation metrics, like our proposed Benchmark CoCoNUT, to accurately assess LLMs\\' code reasoning abilities beyond mere memorization of training data. By addressing potential contamination, both studies aim to enhance the reliability and validity of LLMs in programming contexts.\\n        Citation: Martin Riddell, Ansong Ni, Arman Cohan (2024). Quantifying Contamination in Evaluating Code Generation Capabilities of Language Models. arXiv:2403.04811. https://arxiv.org/abs/2403.04811\\n        \\n    \\n    Instructions:\\n    Using the above information:\\n    Write a cohesive and well-structured \"Related Work\" section that integrates the provided summaries and citations.\\n    Make meaningful connections between the related papers and my research, highlighting similarities, differences, and how the related work contextualizes my study.\\n    Ensure the text flows logically, grouped into thematic paragraphs as needed.\\n    Use the provided citations where relevant to indicate references to the papers.\\n    '"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-28T02:02:13.073232Z",
     "start_time": "2024-11-28T02:01:58.088401Z"
    }
   },
   "cell_type": "code",
   "source": [
    "response: str = prompting_client.get_completions(generate_related_work_prompt(abstract, res), AZURE_PROMPTING_MODEL_VERSION)\n",
    "print(response)"
   ],
   "id": "88f8849d1ab1726f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## Related Work\n",
      "\n",
      "The exploration of large language models (LLMs) in the domain of code understanding and generation has garnered significant attention, with various benchmarks and studies highlighting both their potential and limitations. This section reviews relevant literature, situating our research within the broader context of LLM evaluation and code reasoning capabilities.\n",
      "\n",
      "A number of recent studies have focused on the evaluation of LLMs beyond mere code generation, emphasizing the need for deeper code reasoning and understanding. Zhao et al. (2024) introduced CodeJudge-Eval, a benchmark that assesses LLMs' ability to judge the correctness of code solutions, rather than just generating them. This approach complements our research, which highlights the limitations of LLMs in tracing execution paths and understanding structural control flow in code, underscoring the need for improved benchmarks that assess deeper code reasoning abilities (Zhao et al., 2024).\n",
      "\n",
      "The limitations of existing benchmarks, such as HumanEval, which predominantly focus on Python and code generation capabilities, have been critiqued by Xu et al. (2024). Their work introduces CRUXEVAL-X to address multi-lingual and reasoning biases, aligning with our research's critique of current benchmarks' inadequacies in evaluating code reasoning and structural control flow understanding. While CRUXEVAL-X broadens the scope to include multilingual and reasoning biases, our study emphasizes the need for improved code reasoning abilities in LLMs, particularly in tracing execution paths and handling advanced structural components (Xu et al., 2024).\n",
      "\n",
      "Liang et al. (2024) introduced REPOCOD, a benchmark designed to evaluate the real-world code completion capabilities of LLMs, highlighting their limitations in achieving high accuracy in complex, context-rich coding tasks. This aligns with our findings on the shortcomings of LLMs in understanding and tracing code execution paths, particularly in advanced structural components, further emphasizing the need for improved LLMs to enhance code reasoning and performance in realistic software development scenarios (Liang et al., 2024).\n",
      "\n",
      "The potential of leveraging structured data inherent in programming languages to enhance LLM performance in code-related tasks is explored by Grossman et al. (2023). Their work suggests utilizing structured representations, such as LLVM IR, to improve code reasoning abilities, addressing gaps identified in our study regarding LLMs' limitations in understanding code execution paths, especially with advanced structural components (Grossman et al., 2023).\n",
      "\n",
      "Zheng et al. (2024) introduced the RACE benchmark, which evaluates LLM-generated code across multiple dimensions, such as Readability, Maintainability, Correctness, and Efficiency. This approach highlights the limitations of correctness-centric benchmarks, aligning with our research's critique of current benchmarks by demonstrating that LLMs struggle with tracing execution paths, especially in complex code structures (Zheng et al., 2024).\n",
      "\n",
      "Song et al. (2024) highlight the limitations of LLMs in handling complex code generation tasks, particularly in debugging, which is crucial for iterative code refinement. Their introduction of the BESTER algorithm, which enhances LLMs' debugging capabilities, relates to our research by underscoring the gap in LLMs' ability to understand and trace code execution paths, as demonstrated by their limited performance in appreciating structural control flow (Song et al., 2024).\n",
      "\n",
      "Yan et al. (2023) introduced CodeScope, a comprehensive benchmark designed to evaluate LLMs' code understanding and generation capabilities across 43 programming languages and eight coding tasks. This work highlights the need for more robust evaluation metrics, as our research also identifies gaps in current benchmarks, particularly in assessing LLMs' abilities to trace execution paths and handle advanced code structures (Yan et al., 2023).\n",
      "\n",
      "Allamanis et al. (2024) propose round-trip correctness (RTC) as an alternative evaluation method for code LLMs, allowing for assessment across a broader spectrum of real-world software domains. While RTC expands the evaluation to diverse domains, our research focuses on the models' ability to trace execution paths, revealing limitations in their code reasoning abilities, particularly in complex structures (Allamanis et al., 2024).\n",
      "\n",
      "Zhuo et al. (2024) introduced BigCodeBench, a benchmark designed to evaluate LLMs' ability to perform complex tasks by invoking multiple function calls across various libraries and domains. This aligns with our research, which also identifies the limitations of LLMs in understanding and tracing code execution paths, particularly in complex structures like recursion and object-oriented programming (Zhuo et al., 2024).\n",
      "\n",
      "Finally, Riddell et al. (2024) investigate data contamination in code generation benchmarks, highlighting the overlap between these benchmarks and pretraining data, which can lead to inflated model performance. This is relevant to our work as it underscores the need for robust evaluation metrics, like our proposed Benchmark CoCoNUT, to accurately assess LLMs' code reasoning abilities beyond mere memorization of training data (Riddell et al., 2024).\n",
      "\n",
      "In summary, the reviewed literature collectively emphasizes the need for more comprehensive and robust benchmarks to evaluate LLMs' code reasoning capabilities. Our research contributes to this discourse by introducing the Benchmark CoCoNUT, which specifically addresses the limitations of current benchmarks in assessing LLMs' abilities to trace execution paths and handle advanced structural components.\n"
     ]
    }
   ],
   "execution_count": 13
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
