{
 "cells": [
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from bertopic import BERTopic\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from pymilvus import MilvusClient\n",
    "import json\n",
    "from citegeist.utils.helpers import load_api_key\n",
    "from citegeist.utils.prompts import (\n",
    "    generate_summary_prompt_with_page_content,\n",
    "    generate_related_work_prompt\n",
    ")\n",
    "from citegeist.utils.azure_client import AzureClient\n",
    "from citegeist.utils.citations import (\n",
    "    get_arxiv_abstract,\n",
    "    get_arxiv_citation,\n",
    "    process_arxiv_paper_with_embeddings,\n",
    "    find_most_relevant_pages,\n",
    ")\n",
    "from citegeist.utils.long_to_short import extract_most_relevant_pages\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "topic_model = BERTopic.load(\"MaartenGr/BERTopic_ArXiv\")\n",
    "embedding_model = SentenceTransformer(\"sentence-transformers/all-mpnet-base-v2\")\n",
    "client = MilvusClient(\"./database.db\")\n",
    "prompting_client = AzureClient(\n",
    "    endpoint=os.getenv(\"AZURE_ENDPOINT\"),\n",
    "    deployment_id=os.getenv(\"AZURE_PROMPTING_MODEL\"),\n",
    "    api_key=load_api_key(os.getenv(\"KEY_LOCATION\")),\n",
    ")"
   ],
   "id": "2c779a1eec2cbe0f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "abstract = \"Large Language Models have shown impressive per- formance across a wide array of tasks involving both structured and unstructured textual data. More recently, adaptions of these models have drawn attention to their abilities to work with code across different programming languages. On this notion, different benchmarks for code generation, repair, or completion suggest that certain models have programming abilities comparable to or even surpass humans. In this work, we demonstrate that the performance on this benchmark does not translate to the innate ability of humans to appreciate the structural control flow of code. For this purpose, we extract code solutions from the Hu- manEval benchmark, which the relevant models perform very strongly on, and trace their execution path using function calls sampled from the respective test set. Using this dataset, we investigate the ability of 5 state-of-the-art LLMs to match the execution trace and find that, despite the model’s abilities to generate semantically identical code, they possess only limited ability to trace the execution path, especially for traces with increased length. We find that even the top-performing model, Gemini 1.5 Pro can only fully correctly generate the trace of 47% of HumanEval tasks. In addition, we introduce a specific subset for three key structures not, or only contained to a limited extent in Hu- manEval: Recursion, Parallel Processing, and Object Oriented Programming principles, including concepts like Inheritance and Polymorphism. Besides OOP, we show that none of the investigated models achieve an average accuracy of over 5% on the relevant traces. Aggregating these specialized parts with the ubiquitous HumanEval tasks, we present the Benchmark CoCoNUT: Code Control Flow for Navigation Understanding and Testing, which measures a models ability to trace the execu- tion of code upon relevant calls, including advanced structural components. We conclude that the current generation LLMs still need to significantly improve to enhance their code reasoning abilities. We hope our dataset can help researchers bridge this gap in the near future.\"\n",
    "embedded_abstract = embedding_model.encode(abstract)\n",
    "topic = topic_model.transform(abstract)\n",
    "topic_id = topic[0][0]\n",
    "\n",
    "res = client.search(\n",
    "    collection_name=\"abstracts\",\n",
    "    data=[embedded_abstract],\n",
    "    limit=30,\n",
    "    anns_field=\"embedding\",\n",
    "    # filter = f'topic == {topic_id}',\n",
    "    search_params={\"metric_type\": \"COSINE\", \"params\": {}},\n",
    "    # output_fields = []\n",
    ")\n",
    "formatted_res = json.dumps(res, indent=4)\n",
    "print(formatted_res)\n",
    "print(len(res[0]))"
   ],
   "id": "e41de9643f9fac23"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-29T05:28:00.736263Z",
     "start_time": "2024-11-29T05:28:00.731736Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# we need to remove the best match because that's the same input paper (this only has to be done for papers that are already in the arxiv corpus)\n",
    "# res = res[0][1:]\n",
    "\n",
    "res = res[0]"
   ],
   "id": "ada7380cc8fb5a6e",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-29T05:28:55.529173Z",
     "start_time": "2024-11-29T05:28:10.614761Z"
    }
   },
   "cell_type": "code",
   "source": [
    "paper_embeddings = []\n",
    "for paper in res:\n",
    "    arxiv_id = paper[\"id\"]  # Replace with the actual paper ID key in your JSON\n",
    "\n",
    "    print(f\"Processing paper: {arxiv_id}\")\n",
    "    result = process_arxiv_paper_with_embeddings(arxiv_id, topic_model)\n",
    "\n",
    "    if result:\n",
    "        paper_embeddings.append(result)\n",
    "        print(f\"Paper {arxiv_id}: Processed successfully.\")\n",
    "    else:\n",
    "        print(f\"Paper {arxiv_id}: No content remains after filtering.\")\n",
    "\n",
    "# Print an example: First page text and embedding of the first processed paper\n",
    "if paper_embeddings:\n",
    "    print(\"First paper, first page text:\", paper_embeddings[0][0][\"text\"])\n",
    "    print(\"First paper, first page embedding:\", paper_embeddings[0][0][\"embedding\"])\n",
    "\n",
    "relevant_pages = extract_most_relevant_pages(\n",
    "    paper_embeddings, abstract, topic_model, 60\n",
    ")"
   ],
   "id": "6070f2fb6f8c4af2",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing paper: 2408.10718\n",
      "PDF downloaded successfully: 2408.10718.pdf\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "ff12b7e34d3e4f73b711814874a2f579"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Paper 2408.10718: Processed successfully.\n",
      "Processing paper: 2408.13001\n",
      "PDF downloaded successfully: 2408.13001.pdf\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "be2ae0013a9d4a39ae007d3629196e5f"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Paper 2408.13001: Processed successfully.\n",
      "Processing paper: 2410.21647\n",
      "PDF downloaded successfully: 2410.21647.pdf\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "5690c897c2d140a3b8374155f274707d"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Paper 2410.21647: Processed successfully.\n",
      "Processing paper: 2309.15432\n",
      "PDF downloaded successfully: 2309.15432.pdf\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "a2a824e3f098478396f7e07a8187903b"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Paper 2309.15432: Processed successfully.\n",
      "Processing paper: 2407.11470\n",
      "PDF downloaded successfully: 2407.11470.pdf\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "44efde4a450d41c3bd383c33f3324503"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Paper 2407.11470: Processed successfully.\n",
      "Processing paper: 2407.19055\n",
      "PDF downloaded successfully: 2407.19055.pdf\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "6908acdf424b453abe85353c05bd2708"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Paper 2407.19055: Processed successfully.\n",
      "Processing paper: 2311.08588\n",
      "PDF downloaded successfully: 2311.08588.pdf\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Batches:   0%|          | 0/2 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "94def34338b64e659714ebf4458ebbd0"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Paper 2311.08588: Processed successfully.\n",
      "Processing paper: 2402.08699\n",
      "PDF downloaded successfully: 2402.08699.pdf\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "2c0bbc2f9a5141f3b7507809a1fa835f"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Paper 2402.08699: Processed successfully.\n",
      "Processing paper: 2406.15877\n",
      "PDF downloaded successfully: 2406.15877.pdf\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "9ea245c387f848518a089f0b5e40b7dc"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Paper 2406.15877: Processed successfully.\n",
      "Processing paper: 2403.04811\n",
      "PDF downloaded successfully: 2403.04811.pdf\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "881e02722222478eba021f4841824506"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Paper 2403.04811: Processed successfully.\n",
      "Processing paper: 2403.16437\n",
      "PDF downloaded successfully: 2403.16437.pdf\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "d0ef081e5ee048d79067aa68fcc92511"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Paper 2403.16437: Processed successfully.\n",
      "Processing paper: 2305.15507\n",
      "PDF downloaded successfully: 2305.15507.pdf\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "37e05663f6254e54817ec96d833212a3"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Paper 2305.15507: Processed successfully.\n",
      "Processing paper: 2310.08992\n",
      "PDF downloaded successfully: 2310.08992.pdf\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "50e0d4b89a3540a9b0c11e285e7092e4"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Paper 2310.08992: Processed successfully.\n",
      "Processing paper: 2403.13583\n",
      "PDF downloaded successfully: 2403.13583.pdf\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "fde8495a8d654fb2ba7a518d44314799"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Paper 2403.13583: Processed successfully.\n",
      "Processing paper: 2305.12138\n",
      "PDF downloaded successfully: 2305.12138.pdf\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "e6179d3c8f4d4794a67f03181a5fae9a"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Paper 2305.12138: Processed successfully.\n",
      "Processing paper: 2305.04087\n",
      "PDF downloaded successfully: 2305.04087.pdf\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "562db403fbda4ba0add7256d2944909a"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Paper 2305.04087: Processed successfully.\n",
      "Processing paper: 2406.08731\n",
      "PDF downloaded successfully: 2406.08731.pdf\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "21e9ff65149442d986f7041fa3222928"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Paper 2406.08731: Processed successfully.\n",
      "Processing paper: 2403.19114\n",
      "PDF downloaded successfully: 2403.19114.pdf\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "845976f1bc114e45b83c61ade5e15230"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Paper 2403.19114: Processed successfully.\n",
      "Processing paper: 2407.06153\n",
      "PDF downloaded successfully: 2407.06153.pdf\n",
      "MuPDF error: syntax error: could not parse color space (311 0 R)\n",
      "\n",
      "MuPDF error: syntax error: could not parse color space (633 0 R)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "bf3f672888c3440ea6535812a72e92a2"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Paper 2407.06153: Processed successfully.\n",
      "Processing paper: 2405.00253\n",
      "PDF downloaded successfully: 2405.00253.pdf\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "8e59ae47d01940fa8fc8411ca4d17eb5"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Paper 2405.00253: Processed successfully.\n",
      "Processing paper: 2410.13187\n",
      "PDF downloaded successfully: 2410.13187.pdf\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "87d786bfdafb488c84b4d0c6b54c5226"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Paper 2410.13187: Processed successfully.\n",
      "Processing paper: 2306.09896\n",
      "PDF downloaded successfully: 2306.09896.pdf\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "8cbfe4628c664773b4f7cd9b551f827f"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Paper 2306.09896: Processed successfully.\n",
      "Processing paper: 2309.01940\n",
      "PDF downloaded successfully: 2309.01940.pdf\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "ebb8225ddac54333a1e50831a0efe1ca"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Paper 2309.01940: Processed successfully.\n",
      "Processing paper: 2408.15658\n",
      "PDF downloaded successfully: 2408.15658.pdf\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "3a7b3a5791fe45bf9f8427abe1d36471"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Paper 2408.15658: Processed successfully.\n",
      "Processing paper: 2307.13383\n",
      "PDF downloaded successfully: 2307.13383.pdf\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "094c109ed1c644d2888456c33fd05f83"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Paper 2307.13383: Processed successfully.\n",
      "Processing paper: 2402.09664\n",
      "PDF downloaded successfully: 2402.09664.pdf\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "c078886aadee4ed081dc7a3eef46fa99"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Paper 2402.09664: Processed successfully.\n",
      "Processing paper: 2311.09635\n",
      "PDF downloaded successfully: 2311.09635.pdf\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "3a543022da3f4269837ac787e97e6807"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Paper 2311.09635: Processed successfully.\n",
      "Processing paper: 2405.04520\n",
      "PDF downloaded successfully: 2405.04520.pdf\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "23b078fb19024f2ea4c4d8e2c0122655"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Paper 2405.04520: Processed successfully.\n",
      "Processing paper: 2410.01999\n",
      "PDF downloaded successfully: 2410.01999.pdf\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Batches:   0%|          | 0/2 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "2f51996351be4e3db3e801b738960e7a"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Paper 2410.01999: Processed successfully.\n",
      "Processing paper: 2405.11430\n",
      "PDF downloaded successfully: 2405.11430.pdf\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "1700468923b84d96bce596bcf1f9e49d"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Paper 2405.11430: Processed successfully.\n",
      "First paper, first page text: CodeJudge-Eval: Can Large Language Models be Good Judges in\n",
      "Code Understanding?\n",
      "♢Yuwei Zhao* , ♠Ziyang Luo* , ♡Yuchen Tian , ♠Hongzhan Lin\n",
      "♣Weixiang Yan , ♢Annan Li , ♠Jing Ma†\n",
      "♠Hong Kong Baptist University, ♢Beihang University\n",
      "♡University of Tokyo, ♣Vaneval.AI\n",
      "{yuweizhao,liannan}@buaa.edu.cn\n",
      "{cszyluo,majing}@comp.hkbu.edu.hk\n",
      "Abstract\n",
      "Recent advancements in large language models\n",
      "(LLMs) have showcased impressive code gener-\n",
      "ation capabilities, primarily evaluated through\n",
      "language-to-code benchmarks. However, these\n",
      "benchmarks may not fully capture a model’s\n",
      "code understanding abilities.\n",
      "We introduce\n",
      "CodeJudge-Eval (CJ-Eval), a novel bench-\n",
      "mark designed to assess LLMs’ code under-\n",
      "standing abilities from the perspective of code\n",
      "judging rather than code generation. CJ-Eval\n",
      "challenges models to determine the correctness\n",
      "of provided code solutions, encompassing var-\n",
      "ious error types and compilation issues. By\n",
      "leveraging a diverse set of problems and a fine-\n",
      "grained judging system, CJ-Eval addresses\n",
      "the limitations of traditional benchmarks, in-\n",
      "cluding the potential memorization of solu-\n",
      "tions.\n",
      "Evaluation of 12 well-known LLMs\n",
      "on CJ-Eval reveals that even state-of-the-art\n",
      "models struggle, highlighting the benchmark’s\n",
      "ability to probe deeper into models’ code un-\n",
      "derstanding abilities. Our codes and bench-\n",
      "mark are available at https://github.com/\n",
      "CodeLLM-Research/CodeJudge-Eval.\n",
      "1\n",
      "Introduction\n",
      "Recently, powerful large language models (LLMs)\n",
      "such as GPT-4o (OpenAI, 2023), Gemini (Anil\n",
      "et al., 2023), and Claude (Anthropic, 2023) have\n",
      "demonstrated impressive code generation capabili-\n",
      "ties. These models are being used to develop tools\n",
      "that assist in software development (Hong et al.,\n",
      "2024; Yang et al., 2024). The primary method\n",
      "the community uses to evaluate the coding abili-\n",
      "ties of these LLMs is based on popular language-\n",
      "to-code benchmarks, such as HumanEval (Chen\n",
      "et al., 2021), APPS (Hendrycks et al., 2021) and\n",
      "MBPP (Austin et al., 2021), where LLMs are\n",
      "tasked with generating code based on task descrip-\n",
      "tions.\n",
      "If the generated code can pass the pre-\n",
      "*Equal Contribution. Ziyang Luo is the project lead.\n",
      "†Corresponding Author.\n",
      "Problem 1:\n",
      "    Anton has the integer 𝑥. He is interested what positive integer, which doesn't exceed 𝑥, has the \n",
      "maximum sum of digits. If there are several such integers, determine the biggest of them.\n",
      "    Input: 1 ≤𝑥≤1018\n",
      "    Output: Print the positive integer which doesn't exceed 𝑥 and has the maximum sum of digits\n",
      "    Examples:\n",
      " \n",
      " \n",
      "Input: 100 \n",
      "Output: 99 \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "Input: 521 \n",
      "Output: 499\n",
      "Judge the following code for Problem 1.\n",
      "def max_sum_digits(n):\n",
      "   n = str(n)\n",
      "   if len(n) == 1:\n",
      "       return int(n)\n",
      "   if '9' * len(n) <= n:\n",
      "       return int('9' * len(n))\n",
      "   for i in range(len(n)):\n",
      "       if n[i] != '9':\n",
      "          return int(n[:i] + str(int(n[i]) - 1) + \n",
      "'9' * (len(n) - i - 1))\n",
      "n = int(input().strip())\n",
      "print(max_sum_digits(n))\n",
      "Sure! Here’s a possible solution:\n",
      "x = input()\n",
      "best = x\n",
      "for i in range(len(x), 0, -1):\n",
      "    t = str(int(x[:i])-1) + '9' * (len(x)-i)\n",
      "    t = t.lstrip(\"0\")\n",
      "    if sum(map(int, t)) > sum(map(int, best)):\n",
      "        best = t\n",
      "print(best)\n",
      "Correct Code\n",
      "I think the code is correct.\n",
      "Ground truth label:\n",
      "Wrong Answer\n",
      "Wrong Judgement\n",
      "Correct Generation for\n",
      "Code Generation Task\n",
      "Wrong Judgement for \n",
      "Code Judging Task\n",
      "Generate a correct code solution \n",
      "for Problem 1.\n",
      "Task: Code \n",
      "Judging (Ours)\n",
      "Task: Code \n",
      "Generation\n",
      "(a).\n",
      "(b).\n",
      "Running on test cases\n",
      "Figure 1: Comparing code generation with code judg-\n",
      "ing task, we observe that a model’s ability to generate\n",
      "correct code does not necessarily imply it can accurately\n",
      "judge other codes for the same problem.\n",
      "designed test cases, the LLMs are considered to\n",
      "have successfully solved the coding tasks.\n",
      "While language-to-code benchmarks have signif-\n",
      "icantly advanced the coding capabilities of LLMs,\n",
      "the assumption that a model’s ability to pass pre-\n",
      "designed test cases for a specific task equates to\n",
      "a full understanding of that task does not always\n",
      "hold true (Dou et al., 2024).\n",
      "These test cases\n",
      "may not comprehensively cover all potential in-\n",
      "puts and edge cases (Liu et al., 2023), and con-\n",
      "cerns such as data leakage can further undermine\n",
      "the reliability of such evaluations (Dong et al.,\n",
      "2024; White et al., 2024). To overcome these chal-\n",
      "lenges, we draw inspiration from modern educa-\n",
      "tional theory, which suggests that if someone can\n",
      "accurately judge the correctness of other candidate\n",
      "1\n",
      "arXiv:2408.10718v2  [cs.SE]  13 Sep 2024\n",
      "\n",
      "First paper, first page embedding: [-3.37842340e-03  7.79132247e-02 -1.31800333e-02  5.06728403e-02\n",
      " -1.07947029e-02  4.17561270e-02 -2.67486181e-02  7.88411312e-03\n",
      "  8.85283295e-03  1.88879331e-03 -3.07997651e-02 -2.19248980e-02\n",
      "  1.55756110e-03  2.46962421e-02  3.31747644e-02  1.36062978e-02\n",
      "  3.93901840e-02 -7.81936944e-02 -1.67249069e-02  9.04109143e-03\n",
      " -1.34551860e-02 -8.72508343e-03  1.85588934e-02  6.53211251e-02\n",
      "  6.83104480e-03 -4.85333316e-02  8.59226938e-03 -3.84191680e-03\n",
      " -1.80537701e-02 -2.15987880e-02 -2.18744529e-03  2.19891127e-02\n",
      " -4.40469990e-03  4.22426127e-02  2.40093414e-06 -2.18793619e-02\n",
      " -2.22836807e-02  1.03352563e-02  9.18819662e-03  8.89230296e-02\n",
      "  4.32540290e-03 -3.97987105e-02 -7.37564499e-03  2.17736214e-02\n",
      " -2.67542526e-02 -3.47567871e-02  3.54520045e-02 -3.56779695e-02\n",
      "  5.04645631e-02  6.39834553e-02  1.07850991e-02 -2.52011605e-02\n",
      "  3.08838002e-02  6.10976899e-03  1.94588080e-02 -6.89962804e-02\n",
      "  1.37504740e-02  4.96433601e-02  6.78175129e-03 -1.71722919e-02\n",
      " -2.16616038e-02  1.69405285e-02  4.41298299e-02  1.52683072e-02\n",
      " -3.01320497e-02  2.60442141e-02 -4.08925414e-02  1.35532990e-02\n",
      " -7.18847755e-03  2.79799234e-02 -2.12117434e-02 -4.36027944e-02\n",
      " -1.04094470e-04 -3.40079819e-03 -3.45787741e-02 -3.63012664e-02\n",
      " -4.11994196e-02  2.64441897e-03 -2.49350071e-02  1.09453639e-02\n",
      "  2.74114683e-02 -1.97570827e-02  4.69105877e-02  1.19271539e-02\n",
      " -3.91242169e-02  8.08020756e-02  1.02682887e-02 -6.17723074e-03\n",
      " -1.80937313e-02 -1.17091723e-02  3.55106220e-03 -3.77867147e-02\n",
      " -1.50749665e-02  7.54924640e-02 -1.12274084e-02  2.91595589e-02\n",
      " -1.61702447e-02 -1.07570961e-02  2.56885085e-02 -5.29018678e-02\n",
      "  3.69589366e-02  4.96358909e-02 -1.96958072e-02  5.91564551e-02\n",
      " -6.63747117e-02  1.14465162e-01 -2.79043335e-02 -8.83475598e-03\n",
      " -3.56054269e-02  6.55472977e-04 -1.92574393e-02 -9.99658462e-03\n",
      " -2.98691913e-02  3.92364413e-02  7.70703331e-03  2.40672231e-02\n",
      " -3.27661894e-02  2.42945924e-02 -1.72473621e-02  7.96063244e-02\n",
      " -4.28701229e-02 -3.68332900e-02 -7.26043480e-03 -9.80286207e-03\n",
      "  1.05584711e-02 -9.44759790e-03 -1.71961002e-02 -4.36676256e-02\n",
      " -1.45340767e-02 -7.09104836e-02  1.27287516e-02  1.51938880e-02\n",
      " -4.67589833e-02 -3.74937207e-02  1.25744911e-02  5.62054068e-02\n",
      "  5.35600167e-03  5.11901593e-03 -2.13090200e-02  6.89814612e-03\n",
      " -2.35119984e-02 -5.35380729e-02  5.00876084e-03 -9.56147537e-02\n",
      " -6.30216673e-02  2.81628575e-02  2.13552080e-02 -8.79515335e-02\n",
      "  8.75089876e-03 -2.50757225e-02  1.92744508e-02  3.78193967e-02\n",
      "  6.38006581e-03  1.93798374e-02 -5.15387568e-04  3.61258835e-02\n",
      " -2.20401846e-02  5.40636815e-02  3.77872251e-02 -1.36921078e-03\n",
      "  3.87780671e-03 -3.85283269e-02  5.87977022e-02  5.71363568e-02\n",
      " -1.96374003e-02 -4.22835263e-04 -2.59593688e-02  2.98024733e-02\n",
      "  4.65694778e-02 -3.12135797e-02  1.27190910e-02  4.96535338e-02\n",
      " -3.24579724e-03  6.90333359e-03 -3.96234915e-03  1.27160233e-02\n",
      " -8.28081276e-03  5.59167936e-02  5.95322214e-02 -2.36214288e-02\n",
      "  3.78867462e-02 -1.64892047e-03 -5.01811281e-02  4.24533747e-02\n",
      " -2.09389031e-02  1.72417574e-02  1.00168837e-02  1.15799550e-02\n",
      " -1.16423918e-02 -9.11804661e-02 -4.61594090e-02  1.57321747e-02\n",
      " -2.53665298e-02  1.45135801e-02  3.71407233e-02 -2.59978399e-02\n",
      " -4.85001989e-02  2.39702873e-02 -4.22650902e-03 -6.16465136e-02\n",
      " -1.96903255e-02 -7.00059980e-02  3.27459760e-02  2.15293989e-02\n",
      "  3.75130475e-02 -4.20541428e-02 -3.26687209e-02  2.62759775e-02\n",
      " -2.01909840e-02 -4.33601178e-02  7.35499188e-02  1.40587725e-02\n",
      "  4.65788692e-03 -3.55157964e-02  1.23760188e-02 -2.02339683e-02\n",
      "  2.15887949e-02  4.68177497e-02  2.41640434e-02 -5.65783354e-03\n",
      "  2.69084722e-02 -2.53146309e-02 -8.40279385e-02  1.65280048e-02\n",
      " -5.40284021e-03 -9.04500019e-03 -1.38334045e-02 -8.27073492e-03\n",
      "  3.49197499e-02 -3.31875384e-02 -4.28362750e-02  2.19475999e-02\n",
      "  1.97301190e-02 -2.38496810e-02  1.05851106e-02 -1.83385722e-02\n",
      " -4.13560644e-02 -3.15529704e-02  2.92452611e-02 -3.90526131e-02\n",
      " -3.15449275e-02  4.51676399e-02 -3.74039449e-02 -3.50412875e-02\n",
      "  6.41843230e-02 -4.35919501e-03  3.31045948e-02 -1.93352848e-02\n",
      "  4.47758026e-02 -5.54603338e-02 -1.63438674e-02  1.00547271e-02\n",
      "  6.14169873e-02  5.61537500e-03  4.49728481e-02  1.04905283e-02\n",
      "  8.56139362e-02  9.03491210e-03  2.50312816e-02 -6.99088797e-02\n",
      "  6.99499715e-03 -2.18437910e-02  3.81192043e-02 -2.14009695e-02\n",
      "  4.66689691e-02 -6.04834817e-02 -5.39888218e-02 -9.61720124e-02\n",
      "  3.00344676e-02 -3.04657575e-02  2.43409742e-02  1.60222780e-03\n",
      " -1.30920438e-02  2.35816929e-03  2.16953065e-02 -2.61312071e-02\n",
      " -1.30355358e-02 -1.32844159e-02  1.03352480e-02 -2.07223790e-03\n",
      " -1.63022764e-02 -6.56679720e-02 -3.77140865e-02  1.11073405e-02\n",
      " -3.87118049e-02  4.02680971e-02 -1.27957948e-02  9.64333303e-04\n",
      " -5.58208749e-02  9.20339599e-02 -1.96365062e-02  5.61024575e-03\n",
      " -4.09564562e-02 -4.98859473e-02  1.07909450e-02 -2.92857531e-02\n",
      "  5.27182110e-02  4.82037589e-02 -3.88747938e-02  1.07966671e-02\n",
      "  4.71043065e-02 -1.01056173e-02 -4.05964963e-02  2.35884786e-02\n",
      "  4.30525318e-02 -6.35565594e-02 -2.73605902e-02 -2.26280745e-02\n",
      "  1.32568786e-02  8.79543945e-02 -1.35920011e-02 -2.08558030e-02\n",
      " -4.39192355e-02  1.58825777e-02 -3.53811681e-02  1.99610684e-02\n",
      "  1.13089578e-02 -7.76859932e-03 -3.12635042e-02  4.55382690e-02\n",
      " -2.72603966e-02 -7.90229589e-02  5.27100600e-02 -5.41174598e-02\n",
      " -2.96743661e-02 -2.43252777e-02 -5.09890402e-03 -7.50186965e-02\n",
      "  6.64657950e-02 -2.29088450e-03  2.32099965e-02 -8.87169689e-03\n",
      " -3.20562199e-02 -1.22987125e-02  1.31450379e-02 -3.11032794e-02\n",
      " -4.26695421e-02  8.00615475e-02 -4.41605970e-02  2.62093563e-02\n",
      " -8.90035415e-04 -2.62367409e-02 -3.42791378e-02  2.71642115e-02\n",
      " -4.81382608e-02  6.80456590e-03  3.54194380e-02 -1.52517008e-02\n",
      "  2.79299822e-02  1.06998440e-02 -1.38812084e-02  3.42929251e-02\n",
      " -1.28674752e-03 -3.02149430e-02 -8.71457625e-03 -3.14798616e-02\n",
      " -2.34222375e-02 -5.96734621e-02  3.72811221e-02 -2.95891948e-02\n",
      "  3.46423164e-02  3.12312841e-02  5.85788116e-02  2.83894427e-02\n",
      " -8.38008448e-02  2.66495869e-02  4.45408039e-02 -7.12469104e-04\n",
      " -3.37921493e-02  6.74114674e-02  6.50522783e-02 -7.84727186e-02\n",
      " -3.82255651e-02  8.91706869e-02  2.36208532e-02 -6.55924082e-02\n",
      " -1.60185457e-03 -8.37369822e-03  4.52110618e-02  5.99158071e-02\n",
      "  2.95676868e-02 -6.56185076e-02 -2.77387761e-02  4.90128389e-03\n",
      "  2.23207800e-03 -1.05846012e-02 -2.85097938e-02 -1.53153930e-02\n",
      " -1.07469976e-01  3.98774706e-02  3.56324948e-02 -3.70205306e-02\n",
      " -2.95965411e-02 -2.21803430e-02 -9.29006655e-03  8.45271721e-03\n",
      " -1.25404224e-02  4.65717129e-02 -2.96851387e-03 -5.35057345e-03\n",
      "  1.42052462e-02 -1.69882346e-02  2.17561773e-03  5.99054098e-02\n",
      " -1.59709547e-02  3.67274992e-02  6.90349862e-02  2.23804172e-02\n",
      "  1.69977061e-02 -2.66282130e-02  4.65735905e-02 -3.85713205e-02\n",
      "  2.59053968e-02 -2.73572914e-02 -1.20426726e-03  2.50926986e-02\n",
      " -1.60022620e-02  4.48563993e-02  2.07401495e-02 -4.56154235e-02\n",
      "  8.50843359e-03  5.74729443e-02  7.40508586e-02 -2.65443232e-03\n",
      " -3.90933454e-02  1.25618163e-03 -2.52902154e-02 -5.34737147e-02\n",
      "  5.99674433e-02  5.52830473e-02 -7.54617015e-03 -5.40560484e-02\n",
      " -7.25452974e-02  2.81076762e-03 -4.62694429e-02 -8.65417591e-04\n",
      "  4.09075152e-03  2.26795953e-02 -1.21706426e-02 -9.47365630e-03\n",
      "  2.52377037e-02  3.54435551e-03  2.97821909e-02  1.82955358e-02\n",
      " -1.23637198e-02  4.40889318e-03 -3.49614490e-03 -2.55065635e-02\n",
      "  1.06312223e-02 -3.24337222e-02  2.53524035e-02 -6.41557351e-02\n",
      "  6.93911174e-03  2.49600057e-02 -4.20531165e-03 -1.20535363e-02\n",
      "  9.90788732e-03 -1.08540408e-01 -4.05898467e-02 -2.76797414e-02\n",
      "  8.33132491e-02  2.51646694e-02  2.34612497e-03 -2.41455622e-02\n",
      " -3.31265703e-02 -5.72600327e-02 -2.29566563e-02 -2.03902125e-02\n",
      "  1.96675546e-02  3.61822359e-02  4.07850789e-03  2.20236275e-02\n",
      "  5.74525632e-03 -4.35841903e-02  5.97455911e-03  3.23318094e-02\n",
      "  7.36439694e-03 -1.87171660e-02  2.93470770e-02  3.05748209e-02\n",
      "  4.05962467e-02 -6.34447578e-03  2.52951421e-02 -3.25446352e-02\n",
      "  1.00313872e-02  1.50063187e-02  2.67197695e-02 -3.63441147e-02\n",
      " -5.77912554e-02  2.26656757e-02  2.47200448e-02 -2.00077239e-02\n",
      "  4.35071823e-04  1.46280145e-02  1.43800331e-02  3.39840129e-02\n",
      " -5.75931603e-03  5.70085645e-02  4.05291170e-02  3.50400172e-02\n",
      "  7.97299519e-02 -2.24885773e-02 -3.53292674e-02  9.86577943e-03\n",
      "  4.46071615e-03  2.77617984e-02  9.85991582e-03  4.97705750e-02\n",
      "  2.35751085e-03  3.11052594e-02  2.25695199e-03 -1.53418053e-02\n",
      " -7.09902048e-02 -8.53912458e-02  1.31427161e-02  2.84594316e-02\n",
      "  6.01482429e-02  3.53000499e-02  3.59884165e-02 -2.48528812e-02\n",
      "  2.12284811e-02 -2.66653113e-02  2.26863883e-02  1.89110090e-03\n",
      " -4.37320620e-02  1.33225117e-02  3.33122984e-02 -5.27875638e-03\n",
      " -3.04515846e-02  9.48688537e-02  1.02948528e-02  2.31469870e-02\n",
      "  5.42652095e-03 -1.02293193e-02 -2.45939642e-02 -1.99148385e-03\n",
      "  3.65195312e-02 -1.80687774e-02 -1.34664271e-02 -1.86175257e-02\n",
      " -5.69914989e-02 -7.22378641e-02  1.34117408e-02  3.38695496e-02\n",
      "  3.44138443e-02 -1.41145838e-02 -1.36754559e-02  9.59601104e-02\n",
      "  3.31358332e-03 -1.48215536e-02 -2.02330258e-02 -4.30976413e-03\n",
      "  2.67525595e-02 -1.27561232e-02 -1.86946858e-02 -6.40349739e-33\n",
      " -2.35966668e-02 -1.34113338e-02  2.84515247e-02 -5.91360359e-03\n",
      "  1.81529056e-02  1.74580738e-02 -6.41126037e-02  5.30113513e-03\n",
      " -5.72305452e-03 -3.36044319e-02  4.50982526e-03  4.15885411e-02\n",
      " -2.13368721e-02 -1.85040701e-02  3.47650349e-02  9.28913150e-03\n",
      "  3.48830298e-02  3.33195776e-02 -4.84676249e-02  1.75500829e-02\n",
      " -2.31653266e-02  2.37139910e-02  1.98196527e-03 -3.67337372e-03\n",
      "  1.11374864e-02 -1.17294956e-02  3.32878879e-03 -5.00286790e-03\n",
      " -2.68785711e-02  5.65307885e-02 -3.25085386e-03  9.36996599e-04\n",
      "  2.47347099e-03 -1.20958826e-02 -2.46913824e-02 -4.27539758e-02\n",
      " -3.78542580e-02 -2.90768035e-02  4.88805100e-02  4.54936596e-03\n",
      "  2.24149469e-02 -7.67751411e-02  6.10286593e-02  3.74028049e-02\n",
      " -4.70240340e-02 -2.28506280e-03 -4.72272886e-03 -3.21077034e-02\n",
      "  1.05831698e-02 -4.04227488e-02 -5.21918125e-02  5.17916195e-02\n",
      " -5.99653907e-02 -4.78611467e-03  1.14201689e-02  2.46366337e-02\n",
      " -2.94615794e-02  7.05405101e-02 -1.53828720e-02 -2.22982047e-03\n",
      "  2.45491881e-02  5.23614846e-02  9.72827002e-02  4.51960228e-03\n",
      "  3.32069438e-04  1.47557370e-02  3.13482694e-02  1.18709318e-02\n",
      " -3.07487659e-02 -3.20295990e-02  3.15699242e-02  5.98684512e-02\n",
      "  2.10699104e-02  2.54254714e-02  4.06341255e-02 -2.35910667e-03\n",
      " -5.50846756e-02  1.69150122e-02  6.68863999e-03  2.70140432e-02\n",
      "  2.59426013e-02  1.90511681e-02 -5.39215505e-02 -3.73604288e-03\n",
      " -5.40943630e-02 -1.11903381e-02 -4.85864803e-02 -1.72422193e-02\n",
      "  3.87929678e-02 -1.96023006e-02 -4.91341092e-02 -3.22576724e-02\n",
      "  2.71163844e-02 -4.25015911e-02 -1.20419199e-02  4.87761572e-02\n",
      " -3.56767103e-02  2.10060421e-02 -1.46668144e-02  8.23399499e-02\n",
      " -2.80398298e-02 -3.53153348e-02 -2.09718477e-02 -1.08398004e-02\n",
      "  7.14536980e-02 -1.63987419e-03  3.20053659e-02  8.43279250e-03\n",
      " -6.87586293e-02 -1.51859624e-02 -5.75990863e-02 -2.49422230e-02\n",
      "  1.15153957e-02  1.93568878e-02  2.68100798e-02 -1.96678768e-04\n",
      "  2.63409149e-02  2.88908910e-02 -1.69975888e-02  8.73193960e-04\n",
      " -4.65216637e-02 -6.35157200e-03 -6.94855489e-03  2.75604483e-02\n",
      " -3.68855298e-02 -5.17685004e-02 -4.11441661e-02  3.11102644e-02\n",
      "  4.39688452e-02 -4.16315384e-02 -4.30964902e-02  6.01030253e-02\n",
      "  3.03033346e-07  1.60740986e-02  2.93513499e-02 -2.09534280e-02\n",
      "  1.39084365e-02  2.02580430e-02  4.04020473e-02 -5.94391301e-02\n",
      " -3.72572914e-02  1.39955264e-02  9.07628145e-03 -2.00203992e-02\n",
      "  5.24539128e-02  5.84198013e-02  1.99676398e-02 -2.19935533e-02\n",
      "  5.74541353e-02 -1.33575164e-02 -1.77374277e-02 -5.87843992e-02\n",
      "  1.13836359e-02  7.46655464e-03  7.43599758e-02 -2.09662151e-02\n",
      "  7.06032291e-03  1.67391822e-02 -4.55243066e-02  1.80932712e-02\n",
      " -6.55851606e-03  8.50272179e-03  2.89537068e-02 -5.32746948e-02\n",
      "  2.19617803e-02 -7.76171731e-03 -4.52050753e-03  2.33755205e-02\n",
      " -1.68569237e-02  2.08565388e-02  8.82226005e-02  3.37970778e-02\n",
      "  5.01944683e-02  2.65611596e-02  1.00157429e-02 -1.11888442e-02\n",
      " -1.03893630e-01  7.43325651e-02  1.38925924e-03 -1.33506013e-02\n",
      " -2.48401780e-02 -1.20169797e-03 -2.19171345e-02 -5.53392991e-03\n",
      "  2.33898265e-03  2.31403913e-02  3.68332751e-02 -1.91735141e-02\n",
      " -4.48173024e-02 -1.21308519e-02 -3.53099853e-02 -4.54367064e-02\n",
      " -3.64732521e-04 -6.48427475e-03 -5.78781925e-02 -7.48005137e-02\n",
      "  2.31552236e-02  8.59116856e-03 -4.87045683e-02  2.05467772e-02\n",
      "  3.07489623e-34  4.59175184e-02 -5.56704961e-02  4.77665849e-03\n",
      "  4.68321219e-02  6.71808496e-02 -1.95879973e-02 -3.85415964e-02\n",
      "  2.32274774e-02 -9.90503840e-03 -3.49792354e-02 -1.57260988e-02]\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-29T05:41:23.057018Z",
     "start_time": "2024-11-29T05:41:23.051341Z"
    }
   },
   "cell_type": "code",
   "source": [
    "relevant_pages[4][\"text\"]"
   ],
   "id": "9cd23a178702a0f0",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'CodeApex: A Bilingual Programming Evaluation Benchmark for Large Language Models\\n111:3\\nand discuss their potential limitations as follows: (1) Existing benchmarks only focus on a single\\nprogramming task for LLMs within uni-lingual test cases (i.e., English), which makes the evaluation\\nincomprehensive. (2) Existing benchmarks, excluding DebugBench [50], generally lack fine-grained\\ncategorization over the test data and human-expert evaluation, which are crucial to derive deeper\\ninsights and analysis for different aspects of programming, as well as the thorough comparison\\nbetween LLMs and human-level abilities.\\nTo this end, we propose the CodeApex benchmark, which gives a comprehensive code and\\nprogramming evaluation of large language models. As shown in Table 1, CodeApex is a pioneering\\nbilingual (English and Chinese) programming benchmark over three different code-related tasks\\n(i.e., programming comprehension, code generation, and code correction) with fine-grained cate-\\ngorization, large test data scale, as well as human-expert evaluation. We have comprehensively\\nevaluated 12 different LLMs using CodeApex, including both API-based and open-source models.\\nOur work analyzes the overall code capabilities of LLMs by comparing their performance across\\ndifferent tasks. Fine-grained categorization experiments provide an analysis of LLMs across different\\nstrategies and data types. Experimental results demonstrate the varying performance of different\\nmodels in code-related tasks, with GPT models showcasing exceptional competitiveness and dis-\\ntinct advantages. Additionally, the experiment compares the performance of LLMs in bilingual and\\ndifferent prompt strategy scenarios. We also organize human testing on code comprehension and\\ncode generation tasks, comparing performance between humans and LLMs. Overall, within the\\nleaderboard of the CodeApex, there remains significant room for improvement in LLM accuracy,\\nindicating an untapped potential for LLMs in code-related tasks.\\nThe rest of the paper is organized as follows. In Section 2, we review previous work on evaluating\\nthe code-related capabilities of LLMs. We present the evaluation protocol for three programming\\ntasks in Section 3. We present and discuss the evaluation results across three tasks and multiple\\ncategorizations in Section 4. Finally, we conclude this paper and discuss future work in Section 5.\\n2\\nRELATED WORK\\n2.1\\nCode Foundation Models\\nThe implementation of programming comprehension heavily relies on the alignment of code space\\nand natural language space through encoding. Graph2Code [58] employs graph neural networks\\nto convert source code into a graph structure, capturing structural information within the code\\nand thereby enhancing the accuracy of programming comprehension. Code2Vec [2] represents the\\nAbstract Syntax Tree (AST) as token sequences along paths, enabling code transformation into\\nfixed-length vector representations and facilitating the learning of code semantics. The introduction\\nof the Transformer architecture [52] has provided novel approaches for code comprehension tasks.\\nOne prominent model in this regard is CodeT5 [53], an extension of the Text-to-Text Transfer\\nTransformer (T5) [45] specifically designed for natural language processing tasks on source code.\\nCodeBERT [25] maps both natural language and code into a shared vector space and leverages\\nattention mechanisms to capture semantic relationships between them. Furthermore, there are\\nmodels [42, 56] that focus on specific programming tasks such as code comment generation and\\nAPI documentation generation within the source code context.\\nThe task of code generation has garnered significant attention after the emergence of LLMs.\\nThese language models are pre-trained on massive text datasets, enabling them to learn rich\\nlanguage representations. General-purpose LLMs, such as GPT and Llama [46], have a certain\\nability to generate code. Some LLMs are specifically designed training schemes for the programming\\ntasks, aimed at improving their coding performance. One common approach to code generation is\\nfine-tuning existing large-scale language models. In this method, a pre-trained language model\\nJ. ACM, Vol. 37, No. 4, Article 111. Publication date: August 2018.\\n'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-29T05:29:59.143944Z",
     "start_time": "2024-11-29T05:29:50.510014Z"
    }
   },
   "cell_type": "code",
   "source": [
    "abstracts = [get_arxiv_abstract(obj[\"id\"]) for obj in res]\n",
    "top_relevant_pages = find_most_relevant_pages(relevant_pages, abstracts, 10)"
   ],
   "id": "8f653ad6a6797322",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-29T05:50:16.021277Z",
     "start_time": "2024-11-29T05:49:43.080087Z"
    }
   },
   "cell_type": "code",
   "source": [
    "for key, obj in top_relevant_pages.items():\n",
    "    arxiv_id = res[key][\"id\"]\n",
    "    arxiv_abstract = obj[\"abstract\"]\n",
    "    text_segments = obj[\"text\"]\n",
    "    response: str = prompting_client.get_completions(\n",
    "        generate_summary_prompt_with_page_content(\n",
    "            abstract, arxiv_abstract, text_segments\n",
    "        ),\n",
    "        os.getenv(\"AZURE_PROMPTING_MODEL_VERSION\"),\n",
    "    )\n",
    "    obj[\"summary\"] = response\n",
    "    obj[\"citation\"] = get_arxiv_citation(arxiv_id)"
   ],
   "id": "c2f3c9cbf90465c8",
   "outputs": [],
   "execution_count": 25
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-29T05:54:57.077476Z",
     "start_time": "2024-11-29T05:54:57.074107Z"
    }
   },
   "cell_type": "code",
   "source": [
    "data = list(top_relevant_pages.values())\n",
    "print(generate_related_work_prompt(abstract, data))"
   ],
   "id": "8e913230b74b0b30",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    I am working on a research paper, and I need a well-written \"Related Work\" section. Below I'm providing you with the abstract of the paper I'm writing and a list of summaries of related works I've identified.\n",
      "    \n",
      "    Here's the abstract of my paper:\n",
      "    \"Large Language Models have shown impressive per- formance across a wide array of tasks involving both structured and unstructured textual data. More recently, adaptions of these models have drawn attention to their abilities to work with code across different programming languages. On this notion, different benchmarks for code generation, repair, or completion suggest that certain models have programming abilities comparable to or even surpass humans. In this work, we demonstrate that the performance on this benchmark does not translate to the innate ability of humans to appreciate the structural control flow of code. For this purpose, we extract code solutions from the Hu- manEval benchmark, which the relevant models perform very strongly on, and trace their execution path using function calls sampled from the respective test set. Using this dataset, we investigate the ability of 5 state-of-the-art LLMs to match the execution trace and find that, despite the model’s abilities to generate semantically identical code, they possess only limited ability to trace the execution path, especially for traces with increased length. We find that even the top-performing model, Gemini 1.5 Pro can only fully correctly generate the trace of 47% of HumanEval tasks. In addition, we introduce a specific subset for three key structures not, or only contained to a limited extent in Hu- manEval: Recursion, Parallel Processing, and Object Oriented Programming principles, including concepts like Inheritance and Polymorphism. Besides OOP, we show that none of the investigated models achieve an average accuracy of over 5% on the relevant traces. Aggregating these specialized parts with the ubiquitous HumanEval tasks, we present the Benchmark CoCoNUT: Code Control Flow for Navigation Understanding and Testing, which measures a models ability to trace the execu- tion of code upon relevant calls, including advanced structural components. We conclude that the current generation LLMs still need to significantly improve to enhance their code reasoning abilities. We hope our dataset can help researchers bridge this gap in the near future.\"\n",
      "    \n",
      "    Here's the list of summaries of the other related works I've found:\n",
      "    \n",
      "        Paper 1:\n",
      "        Summary: The cited paper introduces CodeJudge-Eval (CJ-Eval), a benchmark that assesses large language models' (LLMs) code understanding abilities by evaluating their capacity to judge code correctness, which complements traditional language-to-code benchmarks. This aligns with my research, which highlights the limitations of current benchmarks like HumanEval in evaluating LLMs' code reasoning abilities, particularly in tracing execution paths. Both works emphasize the need for more comprehensive evaluation methods to better capture LLMs' understanding of code beyond mere generation, with the cited paper focusing on correctness and my work on execution flow. Additionally, the concept of round-trip correctness (RTC) from the cited paper offers an unsupervised evaluation approach that could potentially be adapted to assess the execution tracing capabilities explored in my research.\n",
      "        Citation: Miltiadis Allamanis, Sheena Panthaplackel, Pengcheng Yin (2024). Unsupervised Evaluation of Code LLMs with Round-Trip Correctness. arXiv:2402.08699. https://arxiv.org/abs/2402.08699\n",
      "        \n",
      "        Paper 2:\n",
      "        Summary: The cited paper highlights the limitations of existing code benchmarks like HumanEval in evaluating LLMs' real-world coding capabilities, emphasizing the need for more complex and context-rich tasks. This aligns with my research, which critiques the inadequacy of current benchmarks in assessing LLMs' understanding of code control flow. Both works underscore the necessity for more comprehensive benchmarks to better evaluate LLMs' code reasoning and execution tracing abilities. Additionally, the cited paper's focus on multi-lingual benchmarks complements my introduction of specialized subsets for advanced code structures, further illustrating the gaps in current LLM evaluations.\n",
      "        Citation: Shanchao Liang, Yiran Hu, Nan Jiang, Lin Tan (2024). Can Language Models Replace Programmers? REPOCOD Says 'Not Yet'. arXiv:2410.21647. https://arxiv.org/abs/2410.21647\n",
      "        \n",
      "        Paper 3:\n",
      "        Summary: The cited paper highlights the limitations of large language models (LLMs) in real-world code generation tasks, emphasizing that existing benchmarks like HumanEval and MBPP do not adequately represent the complexity of real-world software development. This aligns with my research, which demonstrates that LLMs, despite their strong performance on benchmarks, struggle with understanding and tracing complex code execution paths. Both studies underscore the need for more robust benchmarks and methodologies to evaluate and improve LLMs' code reasoning abilities, with the cited paper introducing REPOCOD as a new benchmark to address these challenges.\n",
      "        Citation: Shihan Dou, Haoxiang Jia, Shenxi Wu, Huiyuan Zheng, Weikang Zhou, Muling Wu, Mingxu Chai, Jessica Fan, Caishuang Huang, Yunbo Tao, Yan Liu, Enyu Zhou, Ming Zhang, Yuhao Zhou, Yueming Wu, Rui Zheng, Ming Wen, Rongxiang Weng, Jingang Wang, Xunliang Cai, Tao Gui, Xipeng Qiu, Qi Zhang, Xuanjing Huang (2024). What's Wrong with Your Code Generated by Large Language Models? An Extensive Study. arXiv:2407.06153. https://arxiv.org/abs/2407.06153\n",
      "        \n",
      "        Paper 4:\n",
      "        Summary: The cited paper introduces CodeJudge-Eval, a benchmark designed to assess large language models' (LLMs) code understanding abilities by evaluating their capacity to judge the correctness of code solutions, rather than just generating code. This approach highlights the limitations of traditional language-to-code benchmarks, which may not fully capture a model's understanding of code, similar to the concerns raised in your work about LLMs' ability to trace execution paths. Both studies emphasize the need for more comprehensive evaluation methods to truly assess LLMs' code reasoning capabilities, with your research focusing on execution trace understanding and the cited paper on code judgment accuracy.\n",
      "        Citation: Yuwei Zhao, Ziyang Luo, Yuchen Tian, Hongzhan Lin, Weixiang Yan, Annan Li, Jing Ma (2024). CodeJudge-Eval: Can Large Language Models be Good Judges in Code Understanding?. arXiv:2408.10718. https://arxiv.org/abs/2408.10718\n",
      "        \n",
      "        Paper 5:\n",
      "        Summary: The cited paper highlights the limitations of current benchmarks for evaluating large language models (LLMs) in coding tasks, emphasizing the need for multidimensional evaluation beyond correctness, such as readability, maintainability, and efficiency. This aligns with my research, which critiques the overemphasis on correctness in benchmarks like HumanEval and introduces the CoCoNUT benchmark to assess code control flow understanding. Both works underscore the necessity for more comprehensive evaluation metrics to capture the nuanced capabilities and limitations of LLMs in real-world programming scenarios. Additionally, the cited paper's focus on bilingual and fine-grained categorization complements my exploration of advanced structural components in code evaluation.\n",
      "        Citation: Lingyue Fu, Huacan Chai, Shuang Luo, Kounianhua Du, Weiming Zhang, Longteng Fan, Jiayi Lei, Renting Rui, Jianghao Lin, Yuchen Fang, Yifan Liu, Jingkuan Wang, Siyuan Qi, Kangning Zhang, Weinan Zhang, Yong Yu (2023). CodeApex: A Bilingual Programming Evaluation Benchmark for Large Language Models. arXiv:2309.01940. https://arxiv.org/abs/2309.01940\n",
      "        \n",
      "        Paper 6:\n",
      "        Summary: The cited paper introduces CodeScope, a comprehensive benchmark for evaluating LLMs on code understanding and generation across 43 programming languages and multiple tasks, addressing limitations in existing benchmarks that focus on narrow language ranges and specific tasks. This relates to my research by highlighting the need for more robust evaluation metrics for LLMs, particularly in understanding complex code structures and execution paths. While CodeScope emphasizes multilingual and multitask environments, my work focuses on the ability of LLMs to trace execution paths and understand advanced structural components, underscoring the complementary nature of both studies in advancing LLM capabilities in code reasoning and debugging.\n",
      "        Citation: Weixiang Yan, Haitian Liu, Yunkun Wang, Yunzhe Li, Qian Chen, Wen Wang, Tingyu Lin, Weishan Zhao, Li Zhu, Hari Sundaram, Shuiguang Deng (2023). CodeScope: An Execution-based Multilingual Multitask Multidimensional Benchmark for Evaluating LLMs on Code Understanding and Generation. arXiv:2311.08588. https://arxiv.org/abs/2311.08588\n",
      "        \n",
      "        Paper 7:\n",
      "        Summary: The cited paper introduces the Mostly Hard Python Problems (MHPP) dataset to address the limitations of existing benchmarks like HumanEval and MBPP in evaluating the code generation capabilities of large language models (LLMs). While your research highlights the inadequacy of current benchmarks in assessing LLMs' ability to trace code execution paths, the cited paper similarly questions the sufficiency of these benchmarks in evaluating function-level code generation. Both works emphasize the need for more comprehensive evaluation methods, with your research focusing on structural control flow understanding and the cited paper on natural language and code reasoning. The introduction of MHPP aligns with your goal of developing more robust benchmarks, such as CoCoNUT, to better assess LLMs' code reasoning abilities.\n",
      "        Citation: Jianbo Dai, Jianqiao Lu, Yunlong Feng, Dong Huang, Guangtao Zeng, Rongju Ruan, Ming Cheng, Haochen Tan, Zhijiang Guo (2024). MHPP: Exploring the Capabilities and Limitations of Language Models Beyond Basic Code Generation. arXiv:2405.11430. https://arxiv.org/abs/2405.11430\n",
      "        \n",
      "        Paper 8:\n",
      "        Summary: The cited paper introduces CodeMind, a framework designed to evaluate the code reasoning abilities of LLMs through tasks like Independent Execution Reasoning, Dependent Execution Reasoning, and Specification Reasoning. It highlights that while LLMs can generate code, their reasoning abilities, especially for complex code, are limited. This aligns with my research, which demonstrates that LLMs struggle with tracing execution paths and understanding advanced code structures, as evidenced by their performance on the Benchmark CoCoNUT. Both works underscore the gap between code generation and reasoning, emphasizing the need for further advancements in LLMs' code reasoning capabilities.\n",
      "        Citation: Changshu Liu, Shizhuo Dylan Zhang, Ali Reza Ibrahimzada, Reyhaneh Jabbarvand (2024). CodeMind: A Framework to Challenge Large Language Models for Code Reasoning. arXiv:2402.09664. https://arxiv.org/abs/2402.09664\n",
      "        \n",
      "        Paper 9:\n",
      "        Summary: The cited paper investigates the limitations of large language models (LLMs) in understanding code semantics, particularly dynamic semantics, despite their proficiency in syntax comprehension. This aligns with my research, which highlights the gap between LLMs' code generation capabilities and their ability to trace execution paths, especially in complex structures like recursion and object-oriented programming. Both studies underscore the need for improved interpretability and reliability of LLMs in software engineering tasks, emphasizing the challenges in ensuring accurate code reasoning and analysis.\n",
      "        Citation: Wei Ma, Shangqing Liu, Zhihao Lin, Wenhan Wang, Qiang Hu, Ye Liu, Cen Zhang, Liming Nie, Li Li, Yang Liu (2023). LMs: Understanding Code Syntax and Semantics for Code Analysis. arXiv:2305.12138. https://arxiv.org/abs/2305.12138\n",
      "        \n",
      "        Paper 10:\n",
      "        Summary: The cited paper highlights the limitations of existing benchmarks in evaluating code reasoning abilities of large language models (LLMs), similar to the concerns raised in my research. While my work introduces the Benchmark CoCoNUT to assess code execution path tracing, the cited paper proposes the REval framework to evaluate code reasoning and consistency with program execution. Both studies reveal that current LLMs struggle with code reasoning, with the cited paper reporting unsatisfactory performance on Runtime Behavior Reasoning and Incremental Consistency Evaluation. This aligns with my findings that LLMs, including Gemini 1.5 Pro, have limited ability to trace execution paths, especially for complex code structures. Both works underscore the need for improved code reasoning capabilities in LLMs.\n",
      "        Citation: Jiasheng Zheng, Boxi Cao, Zhengzhao Ma, Ruotong Pan, Hongyu Lin, Yaojie Lu, Xianpei Han, Le Sun (2024). Beyond Correctness: Benchmarking Multi-dimensional Code Generation for Large Language Models. arXiv:2407.11470. https://arxiv.org/abs/2407.11470\n",
      "        \n",
      "    \n",
      "    Instructions:\n",
      "    Using all the information given above, your goal is to write a cohesive and well-structured \"Related Work\" section. \n",
      "    Draw connections between the related papers and my research and highlight similarities and differences. \n",
      "    Please also make sure to put my work into the overall context of the provided related works in a summarizing paragraph at the end. \n",
      "    If multiple related works have a common point/theme, make sure to group them and refer to them in the same paragraph. \n",
      "    When referring to content from specific papers you must also cite the respective paper properly (i.e. cite right after your direct/indirect quotes).\n",
      "    Also, make sure the related works section consists of multiple paragraphs (6 at most) which are concise, but not too concise (e.g. avoid 2-sentence paragraphs).\n",
      "    \n"
     ]
    }
   ],
   "execution_count": 31
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-29T05:55:25.759322Z",
     "start_time": "2024-11-29T05:55:14.184618Z"
    }
   },
   "cell_type": "code",
   "source": [
    "response: str = prompting_client.get_completions(\n",
    "    generate_related_work_prompt(abstract, data),\n",
    "    os.getenv(\"AZURE_PROMPTING_MODEL_VERSION\"),\n",
    ")\n",
    "print(response)"
   ],
   "id": "88f8849d1ab1726f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In recent years, the evaluation of large language models (LLMs) in the domain of code understanding and generation has garnered significant attention. A common theme across several studies is the critique of existing benchmarks, such as HumanEval, for their inadequacy in capturing the full spectrum of LLMs' code reasoning abilities. For instance, Allamanis et al. (2024) introduce CodeJudge-Eval, a benchmark that assesses LLMs' ability to judge code correctness, highlighting the limitations of traditional benchmarks in evaluating code reasoning capabilities. This aligns with our research, which emphasizes the need for more comprehensive evaluation methods to assess LLMs' understanding of code execution paths, particularly in complex scenarios. The concept of round-trip correctness (RTC) proposed by Allamanis et al. offers an unsupervised evaluation approach that could potentially be adapted to assess execution tracing capabilities, a core focus of our study.\n",
      "\n",
      "Similarly, Liang et al. (2024) and Dou et al. (2024) underscore the limitations of current benchmarks like HumanEval in evaluating LLMs' real-world coding capabilities. Both studies advocate for more complex and context-rich tasks to better assess LLMs' code reasoning abilities. Our research complements these findings by introducing the Benchmark CoCoNUT, which specifically targets the evaluation of code control flow understanding, including advanced structural components such as recursion and object-oriented programming. The introduction of specialized subsets in our work parallels the call for more robust benchmarks, as seen in the REPOCOD benchmark introduced by Dou et al., which aims to address similar challenges.\n",
      "\n",
      "The need for multidimensional evaluation metrics is further echoed by Fu et al. (2023) and Yan et al. (2023), who highlight the importance of assessing LLMs beyond mere correctness. Fu et al. emphasize the need for evaluation metrics that consider readability, maintainability, and efficiency, which aligns with our critique of the overemphasis on correctness in existing benchmarks. Yan et al.'s CodeScope benchmark, which evaluates LLMs across multiple programming languages and tasks, complements our focus on execution path tracing by highlighting the necessity for comprehensive evaluation frameworks that capture the nuanced capabilities and limitations of LLMs in real-world programming scenarios.\n",
      "\n",
      "Moreover, the introduction of datasets like the Mostly Hard Python Problems (MHPP) by Dai et al. (2024) and frameworks like CodeMind by Liu et al. (2024) further illustrate the ongoing efforts to challenge LLMs' code reasoning abilities. These works emphasize the gap between code generation and reasoning, a theme central to our research. While MHPP questions the sufficiency of existing benchmarks in evaluating function-level code generation, CodeMind focuses on tasks like Independent Execution Reasoning, underscoring the need for advancements in LLMs' reasoning capabilities. Our work, which demonstrates LLMs' struggles with tracing execution paths, particularly in complex structures, aligns with these studies' findings.\n",
      "\n",
      "Finally, Ma et al. (2023) and Zheng et al. (2024) explore the limitations of LLMs in understanding code semantics and reasoning, respectively. Ma et al. highlight the gap between LLMs' proficiency in syntax comprehension and their understanding of dynamic semantics, which resonates with our findings on the limited ability of LLMs to trace execution paths. Zheng et al.'s REval framework, which evaluates code reasoning and consistency with program execution, parallels our introduction of CoCoNUT, both revealing the challenges LLMs face in code reasoning.\n",
      "\n",
      "In summary, the collective body of related work underscores a critical need for more comprehensive and multidimensional benchmarks to evaluate LLMs' code reasoning abilities. Our research contributes to this discourse by introducing the Benchmark CoCoNUT, which specifically targets the evaluation of code control flow understanding, including advanced structural components. By situating our work within this broader context, we aim to bridge the gap between code generation and reasoning, ultimately advancing the capabilities of LLMs in software engineering tasks.\n"
     ]
    }
   ],
   "execution_count": 32
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-29T06:04:02.411449Z",
     "start_time": "2024-11-29T06:04:02.403672Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print([obj[\"citation\"] for obj in top_relevant_pages.values()])"
   ],
   "id": "fca9c90501fb424e",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Miltiadis Allamanis, Sheena Panthaplackel, Pengcheng Yin (2024). Unsupervised Evaluation of Code LLMs with Round-Trip Correctness. arXiv:2402.08699. https://arxiv.org/abs/2402.08699', \"Shanchao Liang, Yiran Hu, Nan Jiang, Lin Tan (2024). Can Language Models Replace Programmers? REPOCOD Says 'Not Yet'. arXiv:2410.21647. https://arxiv.org/abs/2410.21647\", \"Shihan Dou, Haoxiang Jia, Shenxi Wu, Huiyuan Zheng, Weikang Zhou, Muling Wu, Mingxu Chai, Jessica Fan, Caishuang Huang, Yunbo Tao, Yan Liu, Enyu Zhou, Ming Zhang, Yuhao Zhou, Yueming Wu, Rui Zheng, Ming Wen, Rongxiang Weng, Jingang Wang, Xunliang Cai, Tao Gui, Xipeng Qiu, Qi Zhang, Xuanjing Huang (2024). What's Wrong with Your Code Generated by Large Language Models? An Extensive Study. arXiv:2407.06153. https://arxiv.org/abs/2407.06153\", 'Yuwei Zhao, Ziyang Luo, Yuchen Tian, Hongzhan Lin, Weixiang Yan, Annan Li, Jing Ma (2024). CodeJudge-Eval: Can Large Language Models be Good Judges in Code Understanding?. arXiv:2408.10718. https://arxiv.org/abs/2408.10718', 'Lingyue Fu, Huacan Chai, Shuang Luo, Kounianhua Du, Weiming Zhang, Longteng Fan, Jiayi Lei, Renting Rui, Jianghao Lin, Yuchen Fang, Yifan Liu, Jingkuan Wang, Siyuan Qi, Kangning Zhang, Weinan Zhang, Yong Yu (2023). CodeApex: A Bilingual Programming Evaluation Benchmark for Large Language Models. arXiv:2309.01940. https://arxiv.org/abs/2309.01940', 'Weixiang Yan, Haitian Liu, Yunkun Wang, Yunzhe Li, Qian Chen, Wen Wang, Tingyu Lin, Weishan Zhao, Li Zhu, Hari Sundaram, Shuiguang Deng (2023). CodeScope: An Execution-based Multilingual Multitask Multidimensional Benchmark for Evaluating LLMs on Code Understanding and Generation. arXiv:2311.08588. https://arxiv.org/abs/2311.08588', 'Jianbo Dai, Jianqiao Lu, Yunlong Feng, Dong Huang, Guangtao Zeng, Rongju Ruan, Ming Cheng, Haochen Tan, Zhijiang Guo (2024). MHPP: Exploring the Capabilities and Limitations of Language Models Beyond Basic Code Generation. arXiv:2405.11430. https://arxiv.org/abs/2405.11430', 'Changshu Liu, Shizhuo Dylan Zhang, Ali Reza Ibrahimzada, Reyhaneh Jabbarvand (2024). CodeMind: A Framework to Challenge Large Language Models for Code Reasoning. arXiv:2402.09664. https://arxiv.org/abs/2402.09664', 'Wei Ma, Shangqing Liu, Zhihao Lin, Wenhan Wang, Qiang Hu, Ye Liu, Cen Zhang, Liming Nie, Li Li, Yang Liu (2023). LMs: Understanding Code Syntax and Semantics for Code Analysis. arXiv:2305.12138. https://arxiv.org/abs/2305.12138', 'Jiasheng Zheng, Boxi Cao, Zhengzhao Ma, Ruotong Pan, Hongyu Lin, Yaojie Lu, Xianpei Han, Le Sun (2024). Beyond Correctness: Benchmarking Multi-dimensional Code Generation for Large Language Models. arXiv:2407.11470. https://arxiv.org/abs/2407.11470']\n"
     ]
    }
   ],
   "execution_count": 33
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
