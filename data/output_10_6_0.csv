longlist_length,longlist_ratings,shortlist_length,shortlist_ratings,rating_source_ours,rating_ours_gpt,related_works,citations
30,8|7|7|6|7|7|8|7|7|7|8|8|7|7|8|7|8|8|8|7|5|7|7|7|7|8|5|8|8|8,10,7|7|7|7|7|8|6|8|5|8,source,source,"## Related Work

The exploration of Large Language Models (LLMs) in the domain of code generation and understanding has been a burgeoning area of research, with numerous studies highlighting both the potential and limitations of these models. A significant body of work has focused on the challenges LLMs face in handling complex programming tasks and debugging. For instance, Jialin Song et al. (2024) propose the BESTER algorithm, which enhances LLMs' debugging capabilities through self-reflection and search, addressing the models' struggles with complex tasks and debugging. This aligns with our findings on LLMs' limited ability to trace execution paths in code, as demonstrated in the CoCoNUT benchmark. Similarly, Shihan Dou et al. (2024) conduct an extensive empirical study revealing the challenges LLMs face in handling complex problems, proposing a self-critique method to mitigate bugs, which complements our work by highlighting the need for improved reasoning abilities in LLMs.

Another critical aspect of LLM research is the development of benchmarks to evaluate their performance on code-related tasks. Terry Yue Zhuo et al. (2024) introduce BigCodeBench, a benchmark designed to assess LLMs' ability to handle complex programming tasks involving diverse function calls and compositional reasoning. This work parallels our research, which examines the limitations of LLMs in code reasoning, particularly in tracing execution paths and understanding advanced structural components. Additionally, Miltiadis Allamanis et al. (2024) propose round-trip correctness (RTC) as an unsupervised evaluation method for code LLMs, offering a broader evaluation framework that complements existing benchmarks like HumanEval. Both studies underscore the need for further advancements in LLMs to enhance their code reasoning capabilities.

The multidimensional evaluation of LLMs' code generation abilities is another area of focus. Jiasheng Zheng et al. (2024) introduce the RACE benchmark, which evaluates LLMs on multiple dimensions of code quality, including readability, maintainability, correctness, and efficiency. This approach aligns with our research, which highlights the limitations of current benchmarks in assessing LLMs' abilities to understand code control flow and structural components. Similarly, Weixiang Yan et al. (2023) present CodeScope, a comprehensive benchmark designed to evaluate LLMs across 43 programming languages and eight tasks using execution-based metrics. Both works emphasize the need for more comprehensive evaluation metrics to capture the multifaceted requirements of real-world code generation and reasoning tasks.

The integration of structured code data into LLM training is another promising avenue for improving code reasoning abilities. Aiden Grossman et al. (2023) highlight the importance of utilizing structured code data by leveraging the LLVM compiler infrastructure to create a large dataset of intermediate representations (IR) from multiple programming languages. This work complements our introduction of the CoCoNUT benchmark by underscoring the necessity of integrating structured code data to enhance the performance and applicability of machine learning models in programming tasks. Similarly, Siyuan Jiang et al. (2024) demonstrate that aiXcoder-7B's structured training objectives improve its performance in code completion, suggesting a potential avenue for enhancing code reasoning abilities in LLMs.

Finally, the evaluation of LLMs' programming comprehension and code generation abilities across multiple tasks and languages is crucial for understanding their limitations. Lingyue Fu et al. (2023) introduce CodeApex, a bilingual benchmark designed to evaluate these abilities, highlighting the need for comprehensive evaluation across multiple tasks and languages. This relates to our research as both works emphasize the limitations of current LLMs in programming tasks, with CodeApex focusing on a broader range of tasks and languages, while our work specifically addresses the models' ability to trace execution paths and understand advanced code structures. Additionally, Zhijie Wang et al. (2024) provide an empirical study on the types of code generation errors made by LLMs using the HumanEval dataset, categorizing these errors into semantic and syntactic characteristics, which complements our work by highlighting the limitations of LLMs in generating correct code. Both studies underscore the need for improved code reasoning capabilities in LLMs, with our work further extending the analysis to advanced structural components like recursion and object-oriented programming.","Shihan Dou, Haoxiang Jia, Shenxi Wu, Huiyuan Zheng, Weikang Zhou, Muling Wu, Mingxu Chai, Jessica Fan, Caishuang Huang, Yunbo Tao, Yan Liu, Enyu Zhou, Ming Zhang, Yuhao Zhou, Yueming Wu, Rui Zheng, Ming Wen, Rongxiang Weng, Jingang Wang, Xunliang Cai, Tao Gui, Xipeng Qiu, Qi Zhang, Xuanjing Huang (2024). What's Wrong with Your Code Generated by Large Language Models? An Extensive Study. arXiv:2407.06153. https://arxiv.org/abs/2407.06153
Lingyue Fu, Huacan Chai, Shuang Luo, Kounianhua Du, Weiming Zhang, Longteng Fan, Jiayi Lei, Renting Rui, Jianghao Lin, Yuchen Fang, Yifan Liu, Jingkuan Wang, Siyuan Qi, Kangning Zhang, Weinan Zhang, Yong Yu (2023). CodeApex: A Bilingual Programming Evaluation Benchmark for Large Language Models. arXiv:2309.01940. https://arxiv.org/abs/2309.01940
Terry Yue Zhuo, Minh Chien Vu, Jenny Chim, Han Hu, Wenhao Yu, Ratnadira Widyasari, Imam Nur Bani Yusuf, Haolan Zhan, Junda He, Indraneil Paul, Simon Brunner, Chen Gong, Thong Hoang, Armel Randy Zebaze, Xiaoheng Hong, Wen-Ding Li, Jean Kaddour, Ming Xu, Zhihan Zhang, Prateek Yadav, Naman Jain, Alex Gu, Zhoujun Cheng, Jiawei Liu, Qian Liu, Zijian Wang, David Lo, Binyuan Hui, Niklas Muennighoff, Daniel Fried, Xiaoning Du, Harm de Vries, Leandro Von Werra (2024). BigCodeBench: Benchmarking Code Generation with Diverse Function Calls and Complex Instructions. arXiv:2406.15877. https://arxiv.org/abs/2406.15877
Zhijie Wang, Zijie Zhou, Da Song, Yuheng Huang, Shengmai Chen, Lei Ma, Tianyi Zhang (2024). Where Do Large Language Models Fail When Generating Code?. arXiv:2406.08731. https://arxiv.org/abs/2406.08731
Jialin Song, Jonathan Raiman, Bryan Catanzaro (2024). Effective Large Language Model Debugging with Best-first Tree Search. arXiv:2407.19055. https://arxiv.org/abs/2407.19055
Aiden Grossman, Ludger Paehler, Konstantinos Parasyris, Tal Ben-Nun, Jacob Hegna, William Moses, Jose M Monsalve Diaz, Mircea Trofin, Johannes Doerfert (2023). ComPile: A Large IR Dataset from Production Sources. arXiv:2309.15432. https://arxiv.org/abs/2309.15432
Miltiadis Allamanis, Sheena Panthaplackel, Pengcheng Yin (2024). Unsupervised Evaluation of Code LLMs with Round-Trip Correctness. arXiv:2402.08699. https://arxiv.org/abs/2402.08699
Siyuan Jiang, Jia Li, He Zong, Huanyu Liu, Hao Zhu, Shukai Hu, Erlu Li, Jiazheng Ding, Yu Han, Wei Ning, Gen Wang, Yihong Dong, Kechi Zhang, Ge Li (2024). aiXcoder-7B: A Lightweight and Effective Large Language Model for Code Completion. arXiv:2410.13187. https://arxiv.org/abs/2410.13187
Jiasheng Zheng, Boxi Cao, Zhengzhao Ma, Ruotong Pan, Hongyu Lin, Yaojie Lu, Xianpei Han, Le Sun (2024). Beyond Correctness: Benchmarking Multi-dimensional Code Generation for Large Language Models. arXiv:2407.11470. https://arxiv.org/abs/2407.11470
Weixiang Yan, Haitian Liu, Yunkun Wang, Yunzhe Li, Qian Chen, Wen Wang, Tingyu Lin, Weishan Zhao, Li Zhu, Hari Sundaram, Shuiguang Deng (2023). CodeScope: An Execution-based Multilingual Multitask Multidimensional Benchmark for Evaluating LLMs on Code Understanding and Generation. arXiv:2311.08588. https://arxiv.org/abs/2311.08588"
30,8|8|8|6|5|7|3|8|7|4|7|7|6|3|5|7|7|7|7|5|3|7|6|3|6|3|7|6|3|7,10,3|8|7|8|3|5|5|7|6|7,target,source,"The increasing importance of privacy in machine learning (ML) has led to a diverse array of research efforts aimed at safeguarding sensitive information during various stages of the ML lifecycle. A significant body of work has focused on privacy-preserving techniques for data and model parameters, particularly in distributed and decentralized environments. This section reviews related works that align with our research on ensuring privacy and verifiability in distributed ML inference using zero-knowledge Succinct Non-interactive Arguments of Knowledge (zkSNARKs).

Several studies have explored the use of zero-knowledge proofs (ZKPs) to enhance privacy and trust in ML systems. For instance, Drynx is a decentralized system that employs zero-knowledge proofs, homomorphic encryption, and differential privacy to ensure data confidentiality and computation correctness in distributed datasets (Froelicher et al., 2019). While Drynx focuses on data privacy during analysis, our research targets model parameter privacy during inference, both aiming to maintain privacy and security in distributed environments. Similarly, a comprehensive survey by Xing et al. (2023) on zero-knowledge proof-based verifiable machine learning (ZKP-VML) technologies highlights the challenges of verifying computations without revealing sensitive data, supporting the feasibility of our approach in creating trust among participants in ML edge computing and LoRA fine-tuned models.

Other works have addressed privacy concerns in ML inference through different methodologies. VeriSplit, for example, is a framework designed for offloading ML inferences to locally available devices, using masking techniques and a commitment-based verification protocol to address privacy, confidentiality, and integrity concerns (Zhang et al., 2024). While our work uses zkSNARKs for verifiable inference, VeriSplit specifically targets resource-constrained environments, highlighting different scalability and implementation challenges. Additionally, Kang et al. (2022) present a pioneering approach to verifying ML model inference non-interactively using ZK-SNARKs for ImageNet-scale models, emphasizing the correctness of predictions in MLaaS settings. Our research extends this application to ensure partial privacy of model parameters during distributed inference, further exploring selective model section revelation to balance privacy and verifiability.

The privacy of training data and model updates has also been a focal point in related research. Zhang et al. (2018) introduce a data obfuscation methodology that adds random noise to training samples, effectively hiding sensitive information while maintaining model accuracy. This contrasts with our focus on model parameter privacy during inference, yet both approaches underscore the importance of privacy in different stages of the ML lifecycle. Similarly, Shatter employs virtual nodes to disseminate model chunks, preventing attackers from accessing full models and obscuring the identity of the original node (Biswas et al., 2024). While Shatter focuses on privacy in model updates, our research emphasizes parameter privacy during inference, both seeking to balance privacy with model utility.

Finally, several works have explored privacy-preserving protocols and frameworks that complement our research. TorMentor is a system for privacy-preserving multi-party machine learning in untrusted settings, emphasizing data privacy through brokered learning and differential privacy (Fung et al., 2018). This aligns with our focus on privacy in distributed ML inference, albeit through different mechanisms. ZkAudit, a protocol for trustless audits of ML models, allows model providers to keep their model weights and data secret while permitting third-party verification through ZKPs (Waiwitlikhit et al., 2024). This complements our use of zkSNARKs for partial privacy during distributed ML inference. Additionally, LAMINATOR employs Trusted Execution Environments (TEEs) to provide efficient and scalable attestations for various ML properties, addressing the need for verifiable property cards across the ML pipeline (Duddu et al., 2024). Lastly, privateMDI focuses on reducing communication overhead and ensuring privacy for both data and model parameters in hierarchical setups, aligning with our emphasis on model parameter privacy during inference (Dehkordi et al., 2024). Together, these works highlight the diverse strategies employed to enhance privacy and trust in ML systems, underscoring the relevance and potential of our approach in this evolving landscape.","Sayan Biswas, Mathieu Even, Anne-Marie Kermarrec, Laurent Massoulie, Rafael Pires, Rishi Sharma, Martijn de Vos (2024). Noiseless Privacy-Preserving Decentralized Learning. arXiv:2404.09536. https://arxiv.org/abs/2404.09536
Vasisht Duddu, Oskari Järvinen, Lachlan J Gunn, N Asokan (2024). Laminator: Verifiable ML Property Cards using Hardware-assisted Attestations. arXiv:2406.17548. https://arxiv.org/abs/2406.17548
Suppakit Waiwitlikhit, Ion Stoica, Yi Sun, Tatsunori Hashimoto, Daniel Kang (2024). Trustless Audits without Revealing Data or Models. arXiv:2404.04500. https://arxiv.org/abs/2404.04500
Daniel Kang, Tatsunori Hashimoto, Ion Stoica, Yi Sun (2022). Scaling up Trustless DNN Inference with Zero-Knowledge Proofs. arXiv:2210.08674. https://arxiv.org/abs/2210.08674
Zhibo Xing, Zijian Zhang, Jiamou Liu, Ziang Zhang, Meng Li, Liehuang Zhu, Giovanni Russello (2023). Zero-knowledge Proof Meets Machine Learning in Verifiability: A Survey. arXiv:2310.14848. https://arxiv.org/abs/2310.14848
Han Zhang, Zifan Wang, Mihir Dhamankar, Matt Fredrikson, Yuvraj Agarwal (2024). VeriSplit: Secure and Practical Offloading of Machine Learning Inferences across IoT Devices. arXiv:2406.00586. https://arxiv.org/abs/2406.00586
Tianwei Zhang, Zecheng He, Ruby B. Lee (2018). Privacy-preserving Machine Learning through Data Obfuscation. arXiv:1807.01860. https://arxiv.org/abs/1807.01860
Clement Fung, Jamie Koerner, Stewart Grant, Ivan Beschastnikh (2018). Dancing in the Dark: Private Multi-Party Machine Learning in an Untrusted Setting. arXiv:1811.09712. https://arxiv.org/abs/1811.09712
David Froelicher, Juan R. Troncoso-Pastoriza, Joao Sa Sousa, Jean-Pierre Hubaux (2019). Drynx: Decentralized, Secure, Verifiable System for Statistical Queries and Machine Learning on Distributed Datasets. arXiv:1902.03785. https://arxiv.org/abs/1902.03785"
30,9|8|8|8|8|8|8|8|8|8|7|9|8|7|5|6|6|5|8|8|6|7|8|9|5|7|6|6|7|7,10,8|8|8|8|8|8|7|5|9|8,target,source,"The exploration of spiking neural networks (SNNs) as energy-efficient alternatives to traditional artificial neural networks (ANNs) in natural language processing (NLP) has gained significant traction in recent years. This research direction is particularly relevant given the high energy demands of Transformer architectures. Several studies have contributed to this field by proposing innovative methods to adapt SNNs for language tasks, each addressing unique challenges and offering insights that inform the development of models like SpikeDecoder. For instance, SpikeLM introduces an elastic bi-spiking mechanism to enhance spike encoding, thereby improving SNN performance in language modeling tasks (Xing et al., 2024). This work is closely aligned with our research as it provides a foundation for understanding spike encoding techniques that could be adapted for Transformer architectures. Similarly, SpikeGPT modifies the Transformer architecture by replacing multi-head self-attention with a Spiking RWKV module, achieving energy-efficient language generation (Zhu et al., 2023). Both studies emphasize the potential of SNNs in reducing computational complexity and energy consumption, which are central themes in the development of SpikeDecoder.

The challenge of encoding text into spike-compatible formats is another critical area of research. Knipper et al. (2024) address this by proposing a deterministic rate-coding method for sentiment analysis, which improves upon traditional Poisson rate-coding. This work complements our research by providing insights into encoding methods that are crucial for implementing SNN-based models like SpikeDecoder in NLP tasks. Additionally, the study by Lv et al. (2024) on spiking convolutional neural networks for text classification highlights the potential of SNNs in NLP by demonstrating a ""conversion + fine-tuning"" method that achieves comparable performance to deep neural networks with reduced energy consumption. These studies collectively underscore the importance of efficient text encoding and training methods in the development of spike-based NLP models.

The adaptation of deep architectures like Transformers to SNNs is further explored in works such as SpikeBERT, which employs a two-stage knowledge distillation method to bridge the performance gap between SNNs and transformer-based models like BERT (Lv et al., 2023). This approach is relevant to our research as it demonstrates the feasibility of adapting complex architectures to SNNs while maintaining competitive performance. Similarly, SpikeZIP-TF focuses on ANN-to-SNN conversion methods to maintain high accuracy and reduce energy consumption, highlighting the potential of such methods in the development of SpikeDecoder (You et al., 2024). These studies provide valuable insights into the adaptation of Transformer components to spike-based alternatives, a key aspect of our research.

The exploration of training methods and coding strategies for SNNs is also crucial for enhancing their performance and energy efficiency. Stanojevic et al. (2023) investigate the use of time-to-first-spike (TTFS) coding to achieve performance parity with ANNs on image classification tasks, while Yao et al. (2024) present a general training framework that enhances feature learning and activation efficiency in SNNs. These studies are particularly relevant to our work as they address the training challenges associated with SNNs and offer strategies to optimize neuron states and spike activation, which are applicable to the development of SpikeDecoder.

Finally, the broader context of SNNs in large-scale language models is explored in BrainTransformers, which highlights the development of SNN-compatible Transformer components and a three-stage training approach (Tang & Zhu, 2024). This aligns with our research focus on energy-efficient SNN-based adaptations of Transformer models. Additionally, Zhou et al. (2024) provide a comprehensive review of the theories and methods for training high-performance deep SNNs, discussing the development of transformer-based SNNs and exploring various architectures, including residual connections and spiking self-attention mechanisms. These studies collectively emphasize the potential of SNNs in achieving energy-efficient computation and inform our efforts to create spike-based alternatives to traditional ANN components in Transformer architectures.","Ana Stanojevic, Stanisław Woźniak, Guillaume Bellec, Giovanni Cherubini, Angeliki Pantazi, Wulfram Gerstner (2023). High-performance deep spiking neural networks with 0.3 spikes per neuron. arXiv:2306.08744. https://arxiv.org/abs/2306.08744
Kang You, Zekai Xu, Chen Nie, Zhijie Deng, Qinghai Guo, Xiang Wang, Zhezhi He (2024). SpikeZIP-TF: Conversion is All You Need for Transformer-based SNN. arXiv:2406.03470. https://arxiv.org/abs/2406.03470
Yunpeng Yao, Man Wu, Zheng Chen, Renyuan Zhang (2024). Training a General Spiking Neural Network with Improved Efficiency and Minimum Latency. arXiv:2401.10843. https://arxiv.org/abs/2401.10843
Chenlin Zhou, Han Zhang, Liutao Yu, Yumin Ye, Zhaokun Zhou, Liwei Huang, Zhengyu Ma, Xiaopeng Fan, Huihui Zhou, Yonghong Tian (2024). Direct Training High-Performance Deep Spiking Neural Networks: A Review of Theories and Methods. arXiv:2405.04289. https://arxiv.org/abs/2405.04289
Changze Lv, Tianlong Li, Jianhan Xu, Chenxi Gu, Zixuan Ling, Cenyuan Zhang, Xiaoqing Zheng, Xuanjing Huang (2023). SpikeBERT: A Language Spikformer Learned from BERT with Knowledge Distillation. arXiv:2308.15122. https://arxiv.org/abs/2308.15122
Xingrun Xing, Zheng Zhang, Ziyi Ni, Shitao Xiao, Yiming Ju, Siqi Fan, Yequan Wang, Jiajun Zhang, Guoqi Li (2024). SpikeLM: Towards General Spike-Driven Language Modeling via Elastic Bi-Spiking Mechanisms. arXiv:2406.03287. https://arxiv.org/abs/2406.03287
Zhengzheng Tang, Eva Zhu (2024). BrainTransformers: SNN-LLM. arXiv:2410.14687. https://arxiv.org/abs/2410.14687
Rui-Jie Zhu, Qihang Zhao, Guoqi Li, Jason K. Eshraghian (2023). SpikeGPT: Generative Pre-trained Language Model with Spiking Neural Networks. arXiv:2302.13939. https://arxiv.org/abs/2302.13939
Changze Lv, Jianhan Xu, Xiaoqing Zheng (2024). Spiking Convolutional Neural Networks for Text Classification. arXiv:2406.19230. https://arxiv.org/abs/2406.19230"
30,8|8|8|8|9|8|8|8|8|8|8|8|8|8|7|7|8|8|8|8|7|8|7|8|8|8|6|8|7|4,10,8|8|8|7|8|8|8|7|7|8,source,source,"The field of Multimodal Large Language Models (MLLMs) has seen significant advancements, particularly in the realm of video understanding, as researchers strive to overcome the inherent challenges of processing complex video data. A comprehensive review by Heqing Zou et al. (2024) highlights the unique challenges posed by long-term temporal dependencies and dynamic events in long video understanding. This aligns with our research, which addresses the limitations of zero-shot inference and fine-tuning in video understanding by proposing a data augmentation method to enhance instruction diversity. Both works emphasize the importance of model design and training methodologies in improving the performance of MLLMs for complex video tasks, with Zou et al.'s focus on long video understanding complementing our findings on boosting performance without extensive long video training.

Several studies have explored the integration of video-specific information into LLM-based frameworks to enhance video understanding. Kanchana Ranasinghe et al. (2024) achieve state-of-the-art performance by leveraging object-centric modalities and natural language fusion, addressing the challenge of limited video information. This approach aligns with our research on enhancing video understanding using MLLMs, as both works emphasize the importance of incorporating diverse modalities. While Ranasinghe et al. focus on object-centric information and language fusion, our work introduces a data augmentation method, T2Vid, to improve instruction diversity and learning efficiency, highlighting complementary strategies for advancing video understanding with LLMs. Similarly, Zijia Zhao et al. (2024) introduce VideoNIAH, a synthetic video generation framework designed to enhance the evaluation of video understanding in MLLMs by decoupling video content from query-responses. This approach complements our focus on improving video understanding in MLLMs by providing a targeted assessment framework that isolates specific skills.

The challenge of long video comprehension is further addressed by Weihan Wang et al. (2024) with the introduction of LVBench, a benchmark specifically designed for long video understanding. This work aligns with our research, which identifies the challenges of limited generalization and temporal understanding in zero-shot inference for video data. Both studies emphasize the need for improved models and methodologies to enhance long video comprehension, with our study proposing a data augmentation method to enrich instruction diversity and improve training efficiency. In a similar vein, Xiangyu Zeng et al. (2024) present TimeSuite, a framework designed to enhance the long video understanding capabilities of MLLMs by incorporating temporal grounding. While our work focuses on improving learning efficiency through data augmentation and instruction diversity, TimeSuite addresses long video comprehension by introducing token compression and Temporal Adaptive Position Encoding (TAPE) to improve temporal awareness.

Innovative data handling techniques are also explored by Bo He et al. (2024) with the introduction of a Memory-Augmented Large Multimodal Model (MA-LMM) designed for efficient long-term video understanding. This approach addresses the limitations of existing LLM-based multimodal models, which struggle with context length constraints and GPU memory limits when handling long videos. The MA-LMM's method of enhancing instruction diversity through memory augmentation complements our T2Vid approach, which synthesizes video-like samples to improve training efficiency and performance in video understanding tasks. Similarly, Yang Jin et al. (2024) present Video-LaVIT, a framework that addresses the challenges of video-language pre-training by decomposing videos into keyframes and temporal motions, which are then tokenized for efficient integration with large language models. This approach aligns with our research on enhancing video understanding using MLLMs, as both works focus on improving the representation and comprehension of video data.

Finally, the importance of efficient context handling in MLLMs is highlighted by Jianing Zhou et al. (2024), who propose a hybrid transformer-MAMBA model to efficiently manage long contexts, significantly enhancing inference efficiency. This is relevant to our work as it underscores the importance of efficient context handling in MLLMs, particularly for video understanding, which aligns with our focus on improving learning efficiency and instruction diversity in video data. Additionally, Yuxuan Wang et al. (2024) introduce the Temporal Grounding Bridge (TGB), a framework that enhances MLLMs with advanced temporal grounding capabilities and extends their contextual scope, addressing challenges in interpreting long-form videos. Both our study and Wang et al.'s work highlight the importance of overcoming limitations in temporal understanding and generalization in video-based MLLMs, albeit through different methodologies.","Jianing Zhou, Han Li, Shuai Zhang, Ning Xie, Ruijie Wang, Xiaohan Nie, Sheng Liu, Lingyun Wang (2024). Multimodal Instruction Tuning with Hybrid State Space Models. arXiv:2411.08840. https://arxiv.org/abs/2411.08840
Heqing Zou, Tianze Luo, Guiyang Xie, Victor, Zhang, Fengmao Lv, Guangcong Wang, Junyang Chen, Zhuochen Wang, Hansheng Zhang, Huaijian Zhang (2024). From Seconds to Hours: Reviewing MultiModal Large Language Models on Comprehensive Long Video Understanding. arXiv:2409.18938. https://arxiv.org/abs/2409.18938
Weihan Wang, Zehai He, Wenyi Hong, Yean Cheng, Xiaohan Zhang, Ji Qi, Xiaotao Gu, Shiyu Huang, Bin Xu, Yuxiao Dong, Ming Ding, Jie Tang (2024). LVBench: An Extreme Long Video Understanding Benchmark. arXiv:2406.08035. https://arxiv.org/abs/2406.08035
Xiangyu Zeng, Kunchang Li, Chenting Wang, Xinhao Li, Tianxiang Jiang, Ziang Yan, Songze Li, Yansong Shi, Zhengrong Yue, Yi Wang, Yali Wang, Yu Qiao, Limin Wang (2024). TimeSuite: Improving MLLMs for Long Video Understanding via Grounded Tuning. arXiv:2410.19702. https://arxiv.org/abs/2410.19702
Yang Jin, Zhicheng Sun, Kun Xu, Kun Xu, Liwei Chen, Hao Jiang, Quzhe Huang, Chengru Song, Yuliang Liu, Di Zhang, Yang Song, Kun Gai, Yadong Mu (2024). Video-LaVIT: Unified Video-Language Pre-training with Decoupled Visual-Motional Tokenization. arXiv:2402.03161. https://arxiv.org/abs/2402.03161
Zijia Zhao, Haoyu Lu, Yuqi Huo, Yifan Du, Tongtian Yue, Longteng Guo, Bingning Wang, Weipeng Chen, Jing Liu (2024). Needle In A Video Haystack: A Scalable Synthetic Evaluator for Video MLLMs. arXiv:2406.09367. https://arxiv.org/abs/2406.09367
Yuxuan Wang, Yueqian Wang, Pengfei Wu, Jianxin Liang, Dongyan Zhao, Yang Liu, Zilong Zheng (2024). Efficient Temporal Extrapolation of Multimodal Large Language Models with Temporal Grounding Bridge. arXiv:2402.16050. https://arxiv.org/abs/2402.16050
Kanchana Ranasinghe, Xiang Li, Kumara Kahatapitiya, Michael S. Ryoo (2024). Understanding Long Videos with Multimodal Language Models. arXiv:2403.16998. https://arxiv.org/abs/2403.16998
Bo He, Hengduo Li, Young Kyun Jang, Menglin Jia, Xuefei Cao, Ashish Shah, Abhinav Shrivastava, Ser-Nam Lim (2024). MA-LMM: Memory-Augmented Large Multimodal Model for Long-Term Video Understanding. arXiv:2404.05726. https://arxiv.org/abs/2404.05726"
30,7|5|5|7|8|7|7|5|7|7|7|7|7|7|4|8|7|8|7|6|7|6|7|8|3|7|7|7|7|8,10,7|7|5|7|7|7|7|6|7|5,source,source,"The exploration of reasoning capabilities in language models has been a focal point of recent research, with various methodologies proposed to enhance these models' ability to process and generate logical inferences. The Reverse-Enhanced Thinking (REVTHINK) framework introduced in this paper aligns with several contemporary approaches that aim to improve reasoning by leveraging structured processes. For instance, Quiet-STaR and REFINER both emphasize the importance of structured reasoning paths. Quiet-STaR enhances reasoning by generating rationales at each token, which parallels REVTHINK's structured forward-backward reasoning approach (Zelikman et al., 2024). Similarly, REFINER employs a critic model to iteratively refine intermediate reasoning steps, underscoring the significance of structured feedback in reasoning enhancement (Paul et al., 2023).

Another significant theme in the literature is the optimization of reasoning paths to prevent errors and enhance model performance. Reasoning Paths Optimization (RPO) and ReGenesis both focus on exploring diverse reasoning paths to improve reasoning capabilities. RPO provides contrastive feedback between favorable and unfavorable branches, which complements REVTHINK's reverse thinking strategy aimed at enhancing consistency checks (Chia et al., 2024). ReGenesis, on the other hand, emphasizes self-improvement through abstract-to-concrete reasoning path generation, contrasting with REVTHINK's dataset augmentation approach (Peng et al., 2024). These methodologies highlight the diverse strategies employed to optimize reasoning paths and prevent divergence into errors.

The integration of contextual knowledge into model parameters is another approach that has been explored to enhance reasoning capabilities. RECKONING and SIKeD both focus on embedding knowledge directly into model parameters to improve reasoning robustness. RECKONING enhances reasoning by encoding contextual knowledge, which aligns with REVTHINK's goal of leveraging structured reasoning processes to improve reasoning (Chen et al., 2023). SIKeD, meanwhile, employs iterative self-guided training to teach smaller models to use multiple strategies, similar to REVTHINK's focus on diverse reasoning approaches (Adarsh et al., 2024). These methods underscore the importance of knowledge integration and strategy diversity in enhancing reasoning capabilities.

The use of diverse training data and empirical evaluations to enhance reasoning is another area of interest. ThoughtSource and the distillation framework introduced by Li et al. both emphasize the value of diverse reasoning paths, including those with errors, to enhance model specialization. ThoughtSource provides a meta-dataset to facilitate complex reasoning by verbalizing reasoning steps, which complements REVTHINK's reverse thinking approach (Ott et al., 2023). The distillation framework, on the other hand, leverages both positive and negative samples to refine reasoning skills, highlighting the potential synergy with REVTHINK's structured forward-backward reasoning (Li et al., 2023).

Finally, the exploration of creative reasoning and lateral thinking in language models is exemplified by BRAINTEASER and the self-reinforcement methodology introduced by Tong et al. BRAINTEASER challenges models to defy linear inference chains, aligning with REVTHINK's emphasis on reverse reasoning to improve consistency (Jiang et al., 2023). The self-reinforcement methodology, which uses minimal human supervision to enhance reasoning abilities, complements REVTHINK's focus on structured reasoning with limited supervision (Tong et al., 2024). These approaches contribute to advancing the understanding and development of more robust reasoning in language models, highlighting the need for innovative strategies to enhance reasoning capabilities.","Zeming Chen, Gail Weiss, Eric Mitchell, Asli Celikyilmaz, Antoine Bosselut (2023). RECKONING: Reasoning through Dynamic Knowledge Encoding. arXiv:2305.06349. https://arxiv.org/abs/2305.06349
Yifan Jiang, Filip Ilievski, Kaixin Ma, Zhivar Sourati (2023). BRAINTEASER: Lateral Thinking Puzzles for Large Language Models. arXiv:2310.05057. https://arxiv.org/abs/2310.05057
Yiwei Li, Peiwen Yuan, Shaoxiong Feng, Boyuan Pan, Bin Sun, Xinglin Wang, Heda Wang, Kan Li (2023). Turning Dust into Gold: Distilling Complex Reasoning Capabilities from LLMs by Leveraging Negative Data. arXiv:2312.12832. https://arxiv.org/abs/2312.12832
Xiangyu Peng, Congying Xia, Xinyi Yang, Caiming Xiong, Chien-Sheng Wu, Chen Xing (2024). ReGenesis: LLMs can Grow into Reasoning Generalists via Self-Improvement. arXiv:2410.02108. https://arxiv.org/abs/2410.02108
Yongqi Tong, Sizhe Wang, Dawei Li, Yifan Wang, Simeng Han, Zi Lin, Chengsong Huang, Jiaxin Huang, Jingbo Shang (2024). Optimizing Language Model's Reasoning Abilities with Weak Supervision. arXiv:2405.04086. https://arxiv.org/abs/2405.04086
Debjit Paul, Mete Ismayilzada, Maxime Peyrard, Beatriz Borges, Antoine Bosselut, Robert West, Boi Faltings (2023). REFINER: Reasoning Feedback on Intermediate Representations. arXiv:2304.01904. https://arxiv.org/abs/2304.01904
Eric Zelikman, Georges Harik, Yijia Shao, Varuna Jayasiri, Nick Haber, Noah D. Goodman (2024). Quiet-STaR: Language Models Can Teach Themselves to Think Before Speaking. arXiv:2403.09629. https://arxiv.org/abs/2403.09629
Simon Ott, Konstantin Hebenstreit, Valentin Liévin, Christoffer Egeberg Hother, Milad Moradi, Maximilian Mayrhauser, Robert Praas, Ole Winther, Matthias Samwald (2023). ThoughtSource: A central hub for large language model reasoning data. arXiv:2301.11596. https://arxiv.org/abs/2301.11596
Shivam Adarsh, Kumar Shridhar, Caglar Gulcehre, Nicholas Monath, Mrinmaya Sachan (2024). SIKeD: Self-guided Iterative Knowledge Distillation for mathematical reasoning. arXiv:2410.18574. https://arxiv.org/abs/2410.18574"
30,7|7|7|7|5|6|5|7|4|7|7|3|8|7|6|7|5|8|7|7|3|5|7|7|5|7|7|7|7|3,10,7|7|4|7|7|7|7|6|5|7,source,source,"The exploration of commitment power in communication mechanisms has been a focal point in recent research, with various studies examining different aspects of Bayesian persuasion and strategic information disclosure. A significant contribution to this field is the concept of ex-post individually rational (ex-post IR) Bayesian persuasion, which ensures that the sender is never worse off than with no disclosure, thereby maintaining credibility (Zhang et al., 2023). This framework is particularly relevant to our study, as it complements our model's application to Web 3.0 communities by offering insights into maintaining trust and credibility in mediated communication. Similarly, Su and Subramanian (2022) investigate the impact of commitment order in Bayesian persuasion games, highlighting how sequential commitments can alter equilibrium payoffs and strategies. Both studies underscore the importance of commitment power in strategic communication, akin to our use of money-burning tactics to enhance commitment.

The strategic use of information disclosure is further explored in the work by Arieli et al. (2023), which introduces a two-stage Bayesian persuasion model where a third-party platform controls the information available to the sender. This model emphasizes the platform's role in information control and reputation management, aligning with our exploration of commitment value in Web 3.0 communities. Additionally, Lin and Liu (2022) introduce a new notion of credibility in Bayesian persuasion, focusing on the sender's inability to profit from undetectable deviations. This concept highlights the limitations of commitment in persuasion when the sender's payoff is state-independent, providing a foundational contrast to our model's ability to leverage money-burning for enhanced commitment.

The challenges of limited commitment in persuasion are also addressed in the dynamic model by Che et al. (2020), where information generation and processing are costly, and neither the sender nor the receiver can commit to future actions. This contrasts with our focus on enhancing commitment power through money-burning tactics. While their study explores the collapse of persuasion in a Markov perfect equilibrium, our research demonstrates how money-burning can improve the sender's payoff in scenarios where commitment is valuable. Similarly, Gradwohl et al. (2020) examine a Bayesian persuasion model with multiple senders, highlighting the potential drawbacks of commitment power in competitive settings. Our study, however, demonstrates its benefits in a single-sender context, particularly in Web 3.0 communities.

The complexity of designing signaling mechanisms is further explored in the work by Iyer et al. (2023), which investigates dynamic Bayesian persuasion in a Markovian setting. This study complements our research by addressing the complexity of designing signaling mechanisms when agents have partial knowledge of the history, a challenge also relevant to our exploration of communication mechanisms with money-burning tactics. Aybas and Turkel (2019) also contribute to this theme by examining the impact of limited message capacity on an expert's persuasive ability, using a geometric approach to analyze the sender's payoff. Their findings on the potential benefits of coarser communication for the receiver align with our exploration of commitment value in Web 3.0 communities.

Finally, the computational complexity of the Bayesian persuasion model is examined by Dughmi and Xu (2015), focusing on the sender's optimization task in various input models. Their exploration of algorithmic solutions and approximation schemes for persuasion problems provides a computational perspective that complements our geometric and robust Bayesian persuasion analysis. Additionally, Salcedo (2019) explores a cheap-talk model where the sender can use private messages to persuade only a subset of the audience, enhancing credibility by selectively speaking truthfully. This approach aligns with our research on communication mechanisms, as both studies investigate how strategic communication can optimize sender outcomes. Together, these works provide a comprehensive backdrop against which our study of money-burning tactics for commitment power in mediated communication is situated, highlighting both the challenges and opportunities in enhancing sender commitment in Web 3.0 contexts.","Shaddin Dughmi, Haifeng Xu (2015). Algorithmic Bayesian Persuasion. arXiv:1503.05988. https://arxiv.org/abs/1503.05988
Itai Arieli, Omer Madmon, Moshe Tennenholtz (2023). Reputation-based Persuasion Platforms. arXiv:2305.16694. https://arxiv.org/abs/2305.16694
Xiao Lin, Ce Liu (2022). Credible Persuasion. arXiv:2205.03495. https://arxiv.org/abs/2205.03495
Ronen Gradwohl, Niklas Hahn, Martin Hoefer, Rann Smorodinsky (2020). Reaping the Informational Surplus in Bayesian Persuasion. arXiv:2006.02048. https://arxiv.org/abs/2006.02048
Yeon-Koo Che, Kyungmin Kim, Konrad Mierendorff (2020). Keeping the Listener Engaged: a Dynamic Model of Bayesian Persuasion. arXiv:2003.07338. https://arxiv.org/abs/2003.07338
Shih-Tang Su, Vijay G. Subramanian (2022). Order of Commitments in Bayesian Persuasion with Partial-informed Senders. arXiv:2202.06479. https://arxiv.org/abs/2202.06479
Krishnamurthy Iyer, Haifeng Xu, You Zu (2023). Markov Persuasion Processes with Endogenous Agent Beliefs. arXiv:2307.03181. https://arxiv.org/abs/2307.03181
Bruno Salcedo (2019). Persuading part of an audience. arXiv:1903.00129. https://arxiv.org/abs/1903.00129
Jiahao Zhang, Shuran Zheng, Renato Paes Leme, Zhiwei Steven Wu (2023). Ex-post Individually Rational Bayesian Persuasion. arXiv:2312.04973. https://arxiv.org/abs/2312.04973"
30,3|6|8|3|7|4|2|5|7|8|5|8|7|3|3|2|5|6|7|8|2|3|3|7|3|5|3|2|2|3,10,6|7|3|5|2|8|8|5|2|2,source,source,"The study of Condorcet extensions and their susceptibility to paradoxes has been a significant area of research in voting theory, with various scholars exploring different facets of these voting rules. A key aspect of this research is the manipulability of Condorcet-consistent voting rules, as explored by Dominik Peters (2017), who highlights the potential for manipulation through preference reversals when there are at least four alternatives. This complements our investigation into the reinforcement and no-show paradoxes by providing a broader context of strategic manipulation challenges faced by Condorcet extensions, particularly in three-candidate elections. Similarly, the work by Ross Hyman et al. (2023) on strategic behavior and Nash equilibria in candidacy games under Condorcet-consistent rules underscores the complexities and paradoxes associated with these voting systems, further emphasizing the need for robust solutions to mitigate such issues.

Another critical area of research focuses on the probability and behavior of Condorcet winners as the number of candidates increases. M. S. Krishnamoorthy and M. Raghavachari (2005) provide a statistical perspective on the limiting probability of a Condorcet winner, establishing that this probability decreases with more candidates. This aligns with our focus on three-candidate elections and the axiomatic characterizations of voting rules like maximin, which are relevant for scenarios with fewer candidates. The exploration of axioms and voting methods, such as Minimax, in their work supports our analysis of refinements of maximin that are immune to certain paradoxes. Additionally, Wesley H. Holliday and Eric Pacuit (2023) extend May's Theorem to three alternatives, highlighting the Minimax method's role in mitigating spoiler effects and avoiding the strong no-show paradox, which resonates with our investigation into Condorcet extensions and their paradox immunity.

The compatibility of Condorcet-consistent voting rules with various axioms, such as participation and expansion consistency, has also been a subject of extensive research. Wesley H. Holliday and Eric Pacuit (2020) use SAT solving to demonstrate that no Condorcet extension can satisfy the participation axiom for 12 or more voters when there are at least four alternatives. This finding is particularly relevant to our research on the no-show paradox in three-candidate elections, as it highlights the inherent limitations of Condorcet extensions. Similarly, Felix Brandt et al. (2016) explore the compatibility of these rules with the participation criterion, providing optimal bounds for the no-show paradox via SAT solving. Both studies complement our investigation into refinements of maximin that are immune to paradoxes in smaller electorates, emphasizing the challenges faced by Condorcet extensions in maintaining fairness conditions.

The tension between expansion consistency and resoluteness in voting methods is another area of interest, as explored by Wesley H. Holliday et al. (2022). Their work highlights the challenges of maintaining fairness conditions like anonymity and neutrality, which are relevant to our research on Condorcet extensions and the no-show paradox. The use of SAT solving and formal verification to prove impossibility theorems for voting methods complements our investigation into the axiomatic characterizations of voting rules like maximin, Nanson’s rule, and leximin, particularly in three-candidate elections. This connection underscores the complexities and paradoxes in voting systems and the potential refinements needed to achieve fairer outcomes.

Finally, the exploration of paradoxes in multiwinner voting rules, as discussed by Dominik Peters (2021), and the study of monotonicity axioms in approval-based multi-winner voting rules by Luis Sánchez-Fernández and Jesús A. Fisteus (2017), provide a broader context for understanding the limitations and potential refinements of voting rules. While our work focuses on pairwise majority comparisons and paradoxes like the reinforcement and no-show paradoxes, these studies examine the impact of monotonicity and proportionality on representation in multi-winner settings. Both contribute to the ongoing discourse on designing fair and robust voting systems, highlighting the broader implications of voting rule design and manipulation across different electoral contexts.","M. S. Krishnamoorthy, M. Raghavachari (2005). Condorcet Winner Probabilities - A Statistical Perspective. arXiv:math/0511140. https://arxiv.org/abs/math/0511140
Wesley H. Holliday, Eric Pacuit (2023). An extension of May's Theorem to three alternatives: axiomatizing Minimax voting. arXiv:2312.14256. https://arxiv.org/abs/2312.14256
Felix Brandt, Christian Geist, Dominik Peters (2016). Optimal Bounds for the No-Show Paradox via SAT Solving. arXiv:1602.08063. https://arxiv.org/abs/1602.08063
Ross Hyman, Deb Otis, Seamus Allen, Greg Dennis (2023). A Majority Rule Philosophy for Instant Runoff Voting. arXiv:2308.08430. https://arxiv.org/abs/2308.08430
Dominik Peters (2021). Proportionality and Strategyproofness in Multiwinner Elections. arXiv:2104.08594. https://arxiv.org/abs/2104.08594
Wesley H. Holliday, Eric Pacuit (2020). Axioms for Defeat in Democratic Elections. arXiv:2008.08451. https://arxiv.org/abs/2008.08451
Luis Sánchez-Fernández, Jesús A. Fisteus (2017). Monotonicity axioms in approval-based multi-winner voting rules. arXiv:1710.04246. https://arxiv.org/abs/1710.04246
Dominik Peters (2017). Condorcet's Principle and the Preference Reversal Paradox. arXiv:1707.08760. https://arxiv.org/abs/1707.08760
Wesley H. Holliday, Chase Norman, Eric Pacuit, Saam Zahedian (2022). Impossibility theorems involving weakenings of expansion consistency and resoluteness in voting. arXiv:2208.06907. https://arxiv.org/abs/2208.06907"
30,7|7|7|8|7|7|8|7|8|7|3|7|6|8|3|7|5|7|7|5|5|7|6|7|5|7|8|5|5|7,10,7|8|8|7|7|7|5|7|8|7,target,source,"The study of chaotic dynamics in coupled map lattices (CMLs) has been a focal point in understanding the interplay between microscopic chaos and macroscopic transport phenomena. Several works have explored this domain, providing insights that are pertinent to our investigation of coupled cat maps. Moyano et al. (2006) delve into the dynamics of globally coupled standard maps, emphasizing the scaling behavior of Lyapunov exponents and the emergence of metastable states due to long-range interactions. Their findings on weak chaos and ergodic properties offer a foundational context for analyzing chaotic transport phenomena, complementing our exploration of diffusion and chaos-induced fluctuations in coupled cat maps. Similarly, Hu and Rosenhaus (2022) examine the chaotic dynamics of one-dimensional lattices with diffusive coupling, focusing on covariant Lyapunov vectors and varying diffusion strengths. Their analysis of Lyapunov exponents and spatial structures aligns with our study of ballistic perturbation spread and chaos-induced diffusion, providing a broader understanding of spatiotemporal chaos in high-dimensional systems.

The propagation of perturbations in chaotic systems is another critical theme explored in the literature. Torcini and Lepri (1996) investigate the exponential spread of localized perturbations in chaotic coupled map lattices with long-range couplings, highlighting the role of Lyapunov exponents. This study complements our research on the ballistic spread of perturbations in coupled cat maps, where chaos leads to diffusive transport in phase space. Muruganandam and Senthilvelan (2021) further explore the dynamics of CMLs with quasiperiodically forced nonlinear maps, focusing on strange non-chaotic attractors (SNAs) and their influence on the spatial spread of out-of-time-ordered correlators. Their work provides insights into the chaotic properties of these systems, offering a broader context for understanding diffusion and chaotic behavior in lattice systems through the lens of Lyapunov exponents and perturbation spread.

The role of nonlinear dynamics and coupling in generating complex statistical behaviors and diffusive processes is also a significant area of study. Ginelli et al. (2001) reveal subdiffusive behavior in chains of coupled symplectic McMillan maps, similar to the dynamics observed in disordered Hamiltonian systems. Their findings align with our research on coupled cat maps, where local perturbations spread ballistically, leading to diffusive transport in phase space due to chaos. Barbish and Paul (2023) investigate the dynamics of CMLs, focusing on SNAs and their influence on the spatial spread of out-of-time-ordered correlators. Their study provides insights into the relationship between microscopic chaos and macroscopic diffusion, particularly through the lens of Lyapunov exponents and the ballistic spread of perturbations in phase space.

The influence of noise and chaos on transport and synchronization properties in extended dynamical systems is another area of interest. Baroni et al. (2000) explore stochastic synchronization and phase transitions in spatially extended systems, emphasizing the role of Lyapunov exponents and eigenfunctions in understanding diffusion and synchronization processes. Their work relates to our study of coupled cat maps by providing insights into how noise and chaos influence transport phenomena. Mulansky et al. (2011) investigate chaos in one-dimensional nonlinear Hamiltonian lattices with weakly coupled oscillators, revealing that the measure of chaos is proportional to the coupling strength and lattice length. Their exploration of the interplay between chaos and diffusion complements our findings on the ballistic spread of perturbations and diffusive transport.

Finally, the theoretical framework for analyzing chaotic dynamics and ergodic properties is explored by Axenides et al. (2022), who construct Arnol'd cat map lattice field theories. Their work highlights the spatiotemporal chaotic behavior of these systems, providing a theoretical basis for understanding the diffusion resulting from microscopic chaos in our model. Moges et al. (2021) investigate the diffusion and chaos properties of single and coupled standard maps, focusing on anomalous diffusion caused by accelerator modes. Their study parallels our findings on diffusive transport in phase space due to chaos, highlighting the role of microscopic chaos in determining macroscopic diffusion behavior in complex systems. Together, these works provide a comprehensive backdrop for our research, elucidating the intricate dynamics of coupled chaotic systems and their implications for transport phenomena.","Johnathon Barbish, Mark Paul (2023). Using Covariant Lyapunov Vectors to Quantify High Dimensional Chaos with a Conservation Law. arXiv:2303.13977. https://arxiv.org/abs/2303.13977
Minos Axenides, Emmanuel Floratos, Stam Nicolis (2022). Arnol'd cat map lattices. arXiv:2208.03267. https://arxiv.org/abs/2208.03267
F. Ginelli, R. Livi, A. Politi (2001). Emergence of chaotic behaviour in linearly stable systems. arXiv:nlin/0102005. https://arxiv.org/abs/nlin/0102005
Alessandro Torcini, Stefano Lepri (1996). Disturbance propagation in chaotic extended systems with long-range coupling. arXiv:chao-dyn/9609003. https://arxiv.org/abs/chao-dyn/9609003
Mario Mulansky, Karsten Ahnert, Arkady Pikovsky, Dima Shepelyansky (2011). Strong and weak chaos in weakly nonintegrable many-body Hamiltonian systems. arXiv:1103.2634. https://arxiv.org/abs/1103.2634
Xu-Yao Hu, Vladimir Rosenhaus (2022). Correlation functions in linear chaotic maps. arXiv:2204.13655. https://arxiv.org/abs/2204.13655
P. Muruganandam, M. Senthilvelan (2021). Manifestation of strange nonchaotic attractors in extended systems: A study through out-of-time-ordered correlators. arXiv:2109.07412. https://arxiv.org/abs/2109.07412
Lucia Baroni, Roberto Livi, Alessandro Torcini (2000). Transition to Stochastic Synchronization in Spatially Extended Systems. arXiv:nlin/0003010. https://arxiv.org/abs/nlin/0003010"
30,8|8|8|8|8|8|8|8|8|8|8|7|8|8|8|8|8|7|8|7|7|8|8|8|8|7|8|8|8|7,10,7|8|8|8|7|8|8|8|7|8,target,target,"In recent years, the modeling and prediction of chaotic dynamical systems have increasingly relied on data-driven approaches, particularly when high-fidelity physics-based models are infeasible. A significant body of work has explored various methodologies to enhance the accuracy and robustness of these models, often integrating machine learning with data assimilation techniques. For instance, Ribera et al. (2021) and Carlson et al. (2021) both address the challenge of parameter recovery in chaotic systems, specifically the Lorenz system, using partial observations. While Ribera et al. focus on model selection through variational annealing and sparse optimization, Carlson et al. employ a feedback control-based approach to dynamically learn system parameters. Both studies emphasize the importance of reconstructing system dynamics from incomplete data, aligning with the goals of my research, which introduces a novel data assimilation algorithm leveraging topological data analysis to optimize model predictions without relying on noise information.

The integration of machine learning and data assimilation is a common theme in the literature, as demonstrated by Pawar et al. (2021), Brajard et al. (2020), and Gottwald and Reich (2020). These works explore hybrid modeling approaches that combine neural networks with data assimilation to address unknown dynamics in chaotic systems. Pawar et al. utilize a recurrent neural network, specifically an LSTM, to model hidden physics, while Brajard et al. apply an ensemble Kalman filter alongside a neural network to create a surrogate model for the Lorenz 96 model. Similarly, Gottwald and Reich introduce RAFDA, a method that combines random feature maps with ensemble Kalman filter data assimilation. These studies highlight the potential of integrating machine learning with data assimilation to improve prediction accuracy, a goal shared by my research, which focuses on minimizing topological differences without relying on noise statistics.

Several studies have also explored the use of deep learning architectures to reconstruct and forecast chaotic dynamics from partial observations. Özalp et al. (2023) and Young and Graham (2022) both investigate the application of LSTM networks and deep artificial neural networks (ANNs) to infer hidden chaotic dynamics and predict time series dynamics. Özalp et al. focus on LSTM architectures, including physics-informed LSTMs, while Young and Graham utilize ANNs to learn delay coordinate mappings. These approaches align with my research's emphasis on innovative methodologies to handle incomplete data and complex dynamics, though my work uniquely leverages topological data analysis for model optimization.

The challenge of modeling chaotic systems with sparse and noisy data is further addressed by Frerix et al. (2021) and Schiff et al. (2024). Frerix et al. employ a learned inverse observation operator in variational data assimilation to enhance forecast quality, while Schiff et al. introduce the Dynamics Stable Learning by Invariant Measure (DySLIM) framework to improve long-term statistical accuracy and point-wise tracking. Both studies aim to refine predictions by optimizing model parameters, though they rely on different methodologies compared to my approach, which focuses on minimizing topological differences without noise information.

Finally, Chakraborty et al. (2024) present a novel training approach for Neural Ordinary Differential Equations (NODEs) to effectively learn chaotic dynamical systems. Their Multi-step Penalty NODE method addresses challenges like non-convexity and exploding gradients by splitting training data into non-overlapping time windows and penalizing trajectory discontinuities. This approach offers a complementary strategy to my research, which uses topological data analysis for data assimilation, highlighting the importance of innovative methodologies in enhancing prediction accuracy in complex dynamical systems.","Yair Schiff, Zhong Yi Wan, Jeffrey B. Parker, Stephan Hoyer, Volodymyr Kuleshov, Fei Sha, Leonardo Zepeda-Núñez (2024). DySLIM: Dynamics Stable Learning by Invariant Measure for Chaotic Systems. arXiv:2402.04467. https://arxiv.org/abs/2402.04467
Dibyajyoti Chakraborty, Seung Whan Chung, Troy Arcomano, Romit Maulik (2024). Divide And Conquer: Learning Chaotic Dynamical Systems With Multistep Penalty Neural Ordinary Differential Equations. arXiv:2407.00568. https://arxiv.org/abs/2407.00568
Julien Brajard, Alberto Carassi, Marc Bocquet, Laurent Bertino (2020). Combining data assimilation and machine learning to emulate a dynamical model from sparse and noisy observations: a case study with the Lorenz 96 model. arXiv:2001.01520. https://arxiv.org/abs/2001.01520
H. Ribera, S. Shirman, A. V. Nguyen, N. M. Mangan (2021). Model selection of chaotic systems from data with hidden variables using sparse data assimilation. arXiv:2105.10068. https://arxiv.org/abs/2105.10068
Thomas Frerix, Dmitrii Kochkov, Jamie A. Smith, Daniel Cremers, Michael P. Brenner, Stephan Hoyer (2021). Variational Data Assimilation with a Learned Inverse Observation Operator. arXiv:2102.11192. https://arxiv.org/abs/2102.11192
Suraj Pawar, Omer San, Adil Rasheed, Ionel M. Navon (2021). A nonintrusive hybrid neural-physics modeling of incomplete dynamical systems: Lorenz equations. arXiv:2104.00114. https://arxiv.org/abs/2104.00114
Elise Özalp, Georgios Margazoglou, Luca Magri (2023). Reconstruction, forecasting, and stability of chaotic dynamics from partial data. arXiv:2305.15111. https://arxiv.org/abs/2305.15111
Elizabeth Carlson, Joshua Hudson, Adam Larios, Vincent R. Martinez, Eunice Ng, Jared P. Whitehead (2021). Dynamically learning the parameters of a chaotic system using partial observations. arXiv:2108.08354. https://arxiv.org/abs/2108.08354"
30,5|7|4|2|7|3|7|6|6|2|3|3|2|7|6|6|6|7|3|2|2|5|5|7|3|7|7|3|2|3,10,2|3|3|2|7|7|3|2|7|4,target,source,"In recent years, the Age of Information (AoI) has emerged as a critical metric for evaluating the freshness of information in various systems, particularly those involving sequential processing steps. Several studies have explored the AoI in different contexts, providing valuable insights that inform our research on optimizing service rates under power constraints. Han Dong et al. (2021) investigate AoI in G/G/1/1 systems, focusing on service discipline models that either block or preempt updates. Their findings on optimal interarrival and service times are particularly relevant to our work, as they highlight the impact of service models on information freshness and energy efficiency, complementing our study of the age-power trade-off in processing systems. Similarly, Ismail Akturk and Ulya R. Karpuzcu (2017) examine the trade-off between computation and communication in power-efficient systems, modeling a tandem computation-transmission queue with power constraints. This aligns with our series server setup, where updates require sequential processing steps, and underscores the importance of balancing AoI with power consumption.

The exploration of energy efficiency in network processing and data-intensive applications is another area of significant overlap with our research. Ege Orkun Gamgam et al. (2023) and Balaji Subramaniam and Wu-chun Feng (2015) both investigate energy proportionality in scale-out workloads, focusing on power management techniques to improve energy efficiency. While our work addresses the age-power trade-off in sequential processing systems, these studies provide insights into power provisioning and resource management that could inform strategies for minimizing wasted power in our setups. Both studies highlight the importance of balancing performance and power consumption, albeit in different contexts, with potential cross-applicability of power management techniques.

The trade-off between energy savings and latency is another critical theme in the literature, as explored by Yibei Ling and Jie Mi (2010) and Balajee Vamanan et al. (2015). Ling and Mi address this trade-off in datacenters running on-line, data-intensive applications by exploiting latency slack in sub-critical replies, similar to how our work explores the age-power trade-off in processing updates. Both studies focus on optimizing system performance under constraints, with Ling and Mi leveraging Earliest Deadline First scheduling to manage latency and energy efficiency, while our research formulates an optimization problem to balance service rates and power consumption. Vamanan et al. further explore this theme by using scheduling strategies to manage energy without increasing missed deadlines, highlighting a shared interest in optimizing system performance metrics under resource constraints.

The age-energy trade-off is also a focal point in the work of Jiajie Huang and Jie Gong (2023) and Parisa Rafiee et al. (2020). Huang and Gong investigate this trade-off in a single-source single-server system, employing stochastic hybrid systems to analyze average age-of-synchronization and energy consumption under different wake-up policies. This parallels our approach of optimizing service rates to balance age and power in multi-step processing systems. Rafiee et al. focus on AoI in edge computing systems, highlighting the age-delay tradeoff, where minimizing AoI can lead to increased packet delay. Both works address the challenge of balancing AoI with other system constraints, such as power efficiency in Rafiee et al.'s study and power consumption in our research.

Finally, the exploration of energy minimization in data-intensive applications is addressed by Melih Bastopcu and Sennur Ulukus (2019) and Ajay Badita et al. (2021). Bastopcu and Ulukus model these applications as (n, k) fork-join systems, exploring the trade-off between energy consumption and response time by employing a probabilistic slowdown policy for server cores. This is similar to the age-power trade-off in our work, as both studies address optimizing system performance under constraints. Badita et al. focus on energy minimization for data-intensive applications on large server systems, exploring probabilistic speed selection for server cores to balance performance and energy consumption. While our work identifies wasted power in processing setups and optimizes service rates under power constraints, these studies provide a broader context of energy management strategies in high-performance computing, highlighting the importance of efficient power usage in maintaining system performance.","Balaji Subramaniam, Wu-chun Feng (2015). On the Energy Proportionality of Scale-Out Workloads. arXiv:1501.02729. https://arxiv.org/abs/1501.02729
Ege Orkun Gamgam, Nail Akar, Sennur Ulukus (2023). Minimizing Age of Information with Generate at Will Status Updates and Age-Agnostic Cyclic Scheduling. arXiv:2311.18791. https://arxiv.org/abs/2311.18791
Han Dong, Sanjay Arora, Yara Awad, Tommy Unger, Orran Krieger, Jonathan Appavoo (2021). Slowing Down for Performance and Energy: An OS-Centric Study in Network Driven Workloads. arXiv:2112.07010. https://arxiv.org/abs/2112.07010
Balajee Vamanan, Hamza Bin Sohail, Jahangir Hasan, T. N. Vijaykumar (2015). TimeTrader: Exploiting Latency Tail to Save Datacenter Energy for On-line Data-Intensive Applications. arXiv:1503.05338. https://arxiv.org/abs/1503.05338
Ismail Akturk, Ulya R. Karpuzcu (2017). Trading Computation for Communication: A Taxonomy. arXiv:1709.06555. https://arxiv.org/abs/1709.06555
Melih Bastopcu, Sennur Ulukus (2019). Age of Information for Updates with Distortion. arXiv:1904.10444. https://arxiv.org/abs/1904.10444
Ajay Badita, Rooji Jinan, Balajee Vamanan, Parimal Parag (2021). Modeling Performance and Energy trade-offs in Online Data-Intensive Applications. arXiv:2108.08199. https://arxiv.org/abs/2108.08199
Yibei Ling, Jie Mi (2010). An Optimal Trade-off between Content Freshness and Refresh Cost. arXiv:1008.0441. https://arxiv.org/abs/1008.0441
Jiajie Huang, Jie Gong (2023). Age-Energy Trade-off in Status Update System with Wake-up Control. arXiv:2305.07221. https://arxiv.org/abs/2305.07221
Parisa Rafiee, Peng Zou, Omur Ozel, Suresh Subramaniam (2020). Maintaining Information Freshness in Power-Efficient Status Update Systems. arXiv:2003.13577. https://arxiv.org/abs/2003.13577"
30,8|8|8|7|8|8|7|9|8|7|8|7|8|8|8|7|7|8|8|8|6|8|8|8|8|7|8|8|6|6,10,8|8|9|8|8|8|6|8|8|7,tie,source,"The study of rank-metric codes has been enriched by various approaches that explore their algebraic and geometric properties, providing a foundation for distinguishing different code families and enhancing their applications. A significant contribution to this field is the geometric approach to rank-metric codes, which simplifies the definition of generalized rank weights and classifies constant rank weight codes. This aligns with our research, which also emphasizes geometric invariants, specifically through the lens of Schur powers, to differentiate between code families (Randrianarisoa, 2019). Similarly, the work by Neri, Puchinger, and Horlemann-Trautmann (2019) focuses on algebraic invariants, using sequences of dimensions generated by field automorphisms to distinguish rank-metric codes, particularly Gabidulin codes. Our research complements this by introducing a geometric invariant inspired by the Schur product, offering a novel perspective on code differentiation.

The exploration of new algebraic structures within rank-metric codes is further advanced by Bik and Neri (2023), who extend symmetric rank-metric codes to higher-degree polynomials, introducing essential-rank metric codes. This work parallels our focus on geometric invariants, as both approaches aim to differentiate structured codes from random ones. Additionally, the decoding algorithm for essential-rank metric codes aligns with our interest in practical applications of rank-metric codes in error correction. Similarly, Ndiaye et al. (2023) explore the generalization of Subspace Subcodes in the rank-metric, providing algorithms for constructing generator and parity-check matrices. Their work underscores the importance of developing new code families to address structural vulnerabilities, which is a key aspect of our research on geometric invariants and their applications in post-quantum cryptography.

The foundational aspects of rank-metric codes are thoroughly covered by Gorla (2019), who provides a comprehensive introduction to the mathematical theory of these codes, discussing essential topics such as code equivalence and generalized weights. This theoretical backdrop is crucial for understanding the algebraic structures that our work aims to distinguish using novel invariants. The exploration of q-polymatroids and weight enumerators complements our investigation into the vanishing ideal of linear sets associated with rank-metric codes, offering insights into differentiating Gabidulin codes from random ones. Similarly, Ravagnani (2014) introduces ""Delsarte generalized weights"" for Delsarte rank-metric codes, refining the generalized rank weights for Gabidulin codes. This framework for distinguishing rank-metric codes through algebraic invariants complements our approach using geometric invariants inspired by the Schur product.

The intersection of geometric perspectives and algebraic structures is further explored by Borello and Zullo (2023), who investigate minimal codes in the sum-rank metric, highlighting the importance of geometric perspectives in distinguishing code families. Their introduction of the geometric dual and its application to minimal codes with few weights complements our work on geometric invariants for linear rank-metric codes. Additionally, the generalization of the Ashikhmin-Barg condition for ensuring minimality aligns with our focus on differentiating specific code structures from random ones. Byrne et al. (2019) also contribute to this intersection by introducing the concept of tensor rank as a significant parameter for rank-metric codes, offering a tensor-based framework to analyze and differentiate these codes. This aligns with our focus on using geometric invariants to distinguish Gabidulin codes from random ones.

Finally, the work by Neri (2018) on the characterization of the generator matrix in standard form for generalized Gabidulin codes introduces the concept of q-Cauchy matrices, providing a new criterion for identifying these codes. This exploration of structured matrices, such as Toeplitz and Hankel matrices, aligns with our focus on novel algebraic structures in rank-metric codes. Additionally, Neri, Santonastaso, and Zullo (2021) provide a geometric characterization of sum-rank metric codes, extending linearized Reed-Solomon codes to doubly-extended versions and exploring one-weight codes in the sum-rank metric. Their introduction of n-simplex codes and the study of constant rank-profile codes offer insights that enhance the understanding of algebraic structures in rank-metric codes, relevant to our exploration of new code families. Together, these works underscore the rich interplay between algebraic and geometric approaches in advancing the theory and application of rank-metric codes.","Arthur Bik, Alessandro Neri (2023). Higher-degree symmetric rank-metric codes. arXiv:2303.06745. https://arxiv.org/abs/2303.06745
Eimear Byrne, Alessandro Neri, Alberto Ravagnani, John Sheekey (2019). Tensor Representation of Rank-Metric Codes. arXiv:1904.05227. https://arxiv.org/abs/1904.05227
Alessandro Neri, Paolo Santonastaso, Ferdinando Zullo (2021). The geometry of one-weight codes in the sum-rank metric. arXiv:2112.04989. https://arxiv.org/abs/2112.04989
Alberto Ravagnani (2014). Generalized weights: an anticode approach. arXiv:1410.7207. https://arxiv.org/abs/1410.7207
Elisa Gorla (2019). Rank-metric codes. arXiv:1902.02650. https://arxiv.org/abs/1902.02650
Ousmane Ndiaye, Peter Arnaud Kidoudou, Hervé Tale Kalachi (2023). Generalized Subspace Subcodes in the Rank Metric. arXiv:2301.12523. https://arxiv.org/abs/2301.12523
Alessandro Neri (2018). Systematic encoders for generalized Gabidulin codes and the $q$-analogue of Cauchy matrices. arXiv:1805.06706. https://arxiv.org/abs/1805.06706
Martino Borello, Ferdinando Zullo (2023). Geometric dual and sum-rank minimal codes. arXiv:2303.07288. https://arxiv.org/abs/2303.07288
Alessandro Neri, Sven Puchinger, Anna-Lena Horlemann-Trautmann (2019). Equivalence and Characterizations of Linear Rank-Metric Codes Based on Invariants. arXiv:1911.13059. https://arxiv.org/abs/1911.13059"
