In recent years, the evaluation of Large Language Models (LLMs) in code-related tasks has garnered significant attention, with various benchmarks being developed to assess different aspects of code understanding and generation. A notable contribution in this domain is CodeJudge-Eval, which evaluates LLMs' ability to judge the correctness of code solutions rather than merely generating code (Zhao et al., 2024). This approach complements our research by highlighting the limitations of current LLMs in tracing code execution paths, underscoring the need for improved benchmarks that capture the nuanced understanding of code beyond generation. Similarly, the RACE benchmark evaluates LLMs on multiple dimensions of code quality, such as readability and maintainability, emphasizing the need for comprehensive evaluation metrics to capture the multifaceted requirements of real-world code development (Zheng et al., 2024).

Another critical aspect of LLM evaluation is the consideration of programming language and task biases. The CRUXEVAL-X benchmark addresses this by proposing a multi-lingual code reasoning benchmark, highlighting the limitations of existing benchmarks like HumanEval, which predominantly focus on Python and code generation tasks (Xu et al., 2024). This aligns with our research, which critiques the limitations of current benchmarks, particularly in evaluating code reasoning and structural control flow understanding. In a similar vein, CodeScope evaluates LLMs across 43 programming languages and eight coding tasks, addressing the need for robust evaluation metrics to assess LLMs' ability to trace execution paths and handle advanced code structures (Yan et al., 2023).

The limitations of existing benchmarks in reflecting real-world software development tasks are further highlighted by REPOCOD, which introduces complex, real-world coding problems to reveal the gaps in LLMs' abilities (Liang et al., 2024). This aligns with our findings that current LLMs struggle with tracing code execution paths and handling advanced structural components. Additionally, BigCodeBench evaluates LLMs' ability to perform complex tasks by invoking multiple function calls across various libraries and domains, emphasizing the challenges LLMs face in compositional reasoning and following complex instructions (Zhuo et al., 2024).

Structured code representations, such as LLVM IR, have been proposed to enhance the performance of machine learning models in code-related tasks, as demonstrated by ComPile (Grossman et al., 2023). This approach contrasts with the limitations observed in current LLMs, which struggle with tracing execution paths despite generating semantically correct code. By utilizing structured data, this work suggests a pathway to improve code reasoning abilities, aligning with our findings that current models need significant advancements in understanding code control flow. Furthermore, the introduction of round-trip correctness (RTC) as an alternative evaluation method for code LLMs offers a complementary perspective by focusing on semantic equivalence through prediction and synthesis, potentially addressing broader domain applicability (Allamanis et al., 2024).

Our research contributes to this growing body of work by introducing the Benchmark CoCoNUT, which specifically measures a model's ability to trace the execution of code upon relevant calls, including advanced structural components such as recursion, parallel processing, and object-oriented programming principles. This benchmark addresses the gaps identified in existing evaluations, emphasizing the need for LLMs to significantly improve their code reasoning abilities. By situating our work within the context of these related studies, we highlight the ongoing challenges and opportunities for enhancing LLMs' performance in complex coding tasks, ultimately aiming to bridge the gap between current capabilities and the nuanced understanding required for real-world programming.